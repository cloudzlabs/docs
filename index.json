[
    {
        "uri": "http://tech.cloudz-labs.io/posts/event-driven-microservice/",
        "title": "Event Driven Microservice 란?",
        "tags": ["event driven", "microservices", "spring cloud stream", "비동기 통신", "reative", "serverless", "rabbitmq", "kafka", "event store", "message queue"],
        "description": "우리는 Monolitic Architecture의 단점을 보완하고 새로운 이점을 얻기 위해 MicroService Architecture를 도입했습니다. 우리의 모든 문제들이 해결되었나요? 새로운 문제가 발생하지는 않았나요? 걱정하지 마세요. Event Driven MircoService를 소개합니다.",
        "content": " MicroService Architecture(MSA)는 loosely coupled를 기반으로 빠른 배포주기, 폴리글랏 프로그래밍, 관심사의 집중 등의 장점을 발휘해 Enterprise IT에서 가장 주목받고 있는 아키텍처 입니다. 또한, 분해된 서비스의 scalabililty, resiliency 등 컨테이너 기반의 플랫폼(Kubernetes 등)과 잘 어우러지는 성향으로 서로 끌어주고 밀어주며 발전하고 있습니다.\n하지만 MSA를 도입한 이후 새로운 문제점은 발생하지 않았나요 ? Database Per Service 라는 새로운 요구사항은 잘 지켜지나요 ? rest 통신(synchronized)으로 인한 제약사항은 없나요 ? 분산된 서비스 간 트랜잭션 처리 / 반정규화 된 데이터의 동기 처리는 잘 이루어지고 있나요 ?\nMSA가 적용된 시스템을 보완하는 Event Driven MircoService를 소개합니다.\nEvent Driven 이란? Event Driven 은 IT 영역에서 아주 오래된 키워드입니다. 또한, 현재도 그 영향력은 대단해서 2018년 가트너에서 선정한 유망한 기술 트렌드 중 하나로 뽑히기도 했습니다. (Top 10 Strategic Technology Trends for 2018: Event-Driven Model)\nEvent Driven 은 programming, Architecture 등과 연결되어 다양한 정의로 표현됩니다.\n 컴퓨터 회로를 구동시키기 위해 발생하는 일 - 마우스 클릭, 키보드 타이핑, 모바일 터치 등 특정 행동이 자동으로/순서에 따라 발생하는 것이 아닌 어떤 일에 대한 반응으로 동작하는 디자인 패턴 IOT 기기 등의 센서로부터 유입되는 데이터 스트리밍 기반의 동작 시스템 내 외부에 발생한 주목할 만한 상태의 변화에 기반한 동작 기타 등등  Event Driven 은 종종 분산 처리 시스템과 연결되어 언급됩니다. 분산 처리 시스템에 Event Driven을 엮어 느슨한 결합을 지원하고 유연성, 탄력성, 확장성 있는 시스템을 구현할 수 있습니다.\nEvent Driven MicroService 란? Event Driven MicroService(EDM)는 MSA가 적용된 시스템에서 이벤트 발생시 해당 이벤트 로그를 보관하고 이를 기반으로 동작하며, 비동기 통신을 통해 시스템 내 통합(integration)을 수행하는 Architecture 입니다.\n 이벤트\nIT 영역에서 이벤트는 다양한 정의를 갖지만, 이 곳에서 언급하는 이벤트는 상태의 변경. 즉, 데이터의 변경,생성,삭제(CUD)를 통해 발생하는 서비스의 의미있는 변화를 뜻합니다.\n이벤트 로그를 보관\n현재의 데이터는 상태 변경의 누적이라는 생각에서 시작합니다. 이 때 상태 변경은 이벤트를 뜻하고 이를 누적하는 행위는 이벤트 로그를 보관하는 것입니다. EDM 에서 생성된 이벤트는 반드시 보관되어야 합니다. 보관된 이벤트는 데이터의 현재 상태를 구성하는 근간이 됩니다. 또한, 보관된 이벤트를 바탕으로 장애 발생 또는 특정 요구사항에 따라 지정된 시점으로 복원을 수행합니다. 이벤트 로그를 보관하는 장소를 이벤트 스토어라 칭합니다. 비동기 통신\namqp, mqtt, jms 등 메세징 프로토콜을 통한 메세지 큐 방식이 자주 사용됩니다. 서비스에서 데이터의 생성,변경,삭제(CUD)를 통해 이벤트가 발생하면 발행 서비스는 메세지의 형태로 이벤트를 발행하고, 해당 이벤트에 관심이 있는 서비스에서 구독을 수행합니다. 메세지 큐를 사용함으로 requeue/dlq 등의 기능을 활용할 수 있습니다. 시스템 내 통합(integration)\n이상적으로 구현된 MSA는 서비스 간 데이터 참조를 위한 내부 통신이 필요없지만, 현실적으로 서비스 간 내부 통신이 전혀 없는 시스템을 구현하기란 불가능에 가깝습니다. 다양한 사유로 여러 서비스 간 통신을 통해 연동이 발생합니다.\n 이벤트를 데이터의 생성,변경,삭제로 정의했기 때문에 MSA의 데이터 관리와 밀접한 연관성을 갖습니다. 데이터는 현재의 상태를 나타내고 이는 보관된 데이터 변경,생성,삭제 기록 즉, 이벤트 로그에 기반합니다.\n 특정 서비스에서 기능을 수행합니다. 이벤트가 발생(데이터의 생성,변경,삭제)하면 해당 도메인 객체를 기반으로 이벤트를 생성합니다. 생성된 이벤트는 저장 공간에 보관되고 비동기 메세지 큐로 해당 이벤트에 관심이 있는 서비스들에게 전달됩니다. 이벤트를 구독한 서비스는 biz. logic을 수행합니다. 수행 도중 오류가 발생하면 저장된 이벤트 로그를 기반으로 retry/rollback을 수행합니다.  MSA는 나뉘어진 서비스와 서비스 별 각자의 데이터베이스 구성을 지향합니다. 이로 인해 발생하는 새로운 요구사항들이 있습니다. EDM을 적용해 새로운 요구사항들을 충족시킬 수 있습니다.\n서비스 별 각자 데이터베이스를 적용한 시스템에서 데이터의 무결성을 보장할 수 없지만 EDM을 통해 데이터의 최종적인 일관성을 유지할 수 있습니다.\n all commit or rollback → eventually consistency  이벤트 기반으로 마이크로서비스를 확장하려는 의도는 여러 공개된 정보에서 읽을 수 있습니다. Spring One 의 메인 주제 중 하나로 Event Driven 이 선정되고 있습니다. 여러 기업들이 컨퍼런스 및 기술 블로그를 통해 Event Driven 도입을 알리는 것을 확인할 수 있습니다.\n우아한 형제들 - 박용권 님, Spring Camp 2018 세션 내용 참고\n왜 Event Driven MicroService을 적용하는가 ? MicroService Architecture가 Event Driven 과 연결되어 언급되는 이유는 무엇일까요 ?\n MicroService에 Event Driven 끼얹기\n MSA를 도입하여 느슨한 결합, 관심사의 집중, 단일 책임 원칙, 빠른 배포주기, 폴리글랏, Scalability, 장애의 격리, 유연성, 확장성 등 여러 이점을 확보할 수 있습니다. 하지만 이를 위해 넘어야 하는 허들이 있습니다.\n쪼개다 MSA를 적용한 시스템은 서비스가 쪼개지고 Database가 쪼개집니다. MSA 에서 주요 원칙 중 하나는 서비스 별 자체 biz. logic 과 데이터, 그에 따른 최적의 Database를 선택하는 것입니다. 이상적으로 하나의 기능을 수행할 때 자신의 서비스 내에서 모두 해결할 수 있도록 서비스 분리를 잘한다면 시스템 내 통합(integration)을 고민하지 않아도 됩니다. 하지만 현실적으로 서비스 분리에 따른 서비스 간 데이터 참조 등 시스템 내부에 연결이 발생합니다. 또한, 서비스가 뭉쳐져 있을 때 발휘되던 장점을 활용하지 못하는 경우도 발생합니다.\nDatabase Per Service Database Per Service 는 MSA의 느슨한 결합, 관심사의 집중, 폴리글랏 프로그래밍, 독립적인 배포 주기 등을 달성하기 위한 핵심 키워드입니다. 하지만 MSA를 적용할 때 Database Per Service는 가장 어려운 부분 중 하나입니다.\nDatabase를 변경/분리시 고려해야 하는 요소들이 있습니다.\n 엔터프라이즈에서 데이터가 중요한 자산  금융 데이터, 통신 사용량 등 돈과 관련된 데이터 데이터를 기반으로 biz. insight 확보 → 돈, 매출의 중요한 기반 요소, 데이터 자체가 돈이 된다. 기업의 리더 중에서 90% 가 토지, 인재, 자본 등과 마찬가지로 데이터도 가장 중요한 리소스인 동시에 가장 기본적인 차별화 요소로 판단 데이터 기반 경제 → 의사결정의 기반  Shared 관계형 데이터베이스 장점 사용 불가  데이터를 효율적으로 보관하고 조회/삭제 등 기능의 효율을 높이는 장점 테이블 조인을 통한 통합 뷰 제공 ACID 원칙에 따른 트랜잭션 기능\n  database oriented system  모든 업무의 정의는 Database 스키마/테이블 설계부터 시작 결국 시스템의 성격은 데이터의 CRUD기능, 데이터의 흐름과 life cycle이 중요 → 디비 분리는 결국 기존 데이터의 흐름을 깨는 행위  데이터베이스 분리시 발생하는 비용 기존 DBMS에 최적화된 각종 세팅 기타 등등  위와 같은 이유로 Database Per Service를 적용하는 것은 어렵습니다. 하지만 중앙화된 Shared Database를 사용하는 것은 많은 제약사항이 있습니다.\n 중앙화된 Shared Database 데이터베이스는 시스템의 응집력을 저해하고 종속성을 높인다. 단일 트랜잭션 처리에 따라 테이블 락 등 장애 발생 가능성이 있다. 중앙화 된 데이터베이스에 장애 발생시 전체 시스템에 장애를 일으킨다. 데이터베이스 스케일링이 어렵다. 서비스의 특징에 따른 최적의 데이터베이스 선택이 어렵다.  기존의 Shared Database 구성에서 MSA의 이점을 살리기 어려운 이유는 명확합니다. 하지만, 기존의 Shared Database 구성, 그 중 관계형 Database를 사용 하는 경우 ACID 원칙에 따른 트랜잭션 처리, 데이터 무결성, 모든 테이블에 접근해 Join을 통한 통합 뷰 제공, 관계를 활용한 효율적인 데이터 저장/조회 등 DBMS 레벨에서 여러 유용한 기능을 제공했습니다. Database Per Service를 적용하면 Shared Database 구성의 DBMS 레벨에서 제공하던 기능을 Application 레벨에서 해결해야 합니다.\n 스텝에 따라 처리되는 비즈니스 흐름 수행 분산된 서비스 간 트랜잭션 처리 서비스 간 반정규화 데이터 동기 처리 기타 등등  Database Per Service 적용의 기술적으로 어려운 부분을 해결하는데 Event Driven 이 효율적으로 동작합니다.\n폴리글랏 모바일, SNS, IOT 등 다양한 시스템이 활발하게 발전하고 있습니다. 이들 시스템이 다루는 데이터는 스트림 형태의 비정형 데이터가 많습니다. 매우 빠른 읽기/쓰기 성능을 지원해야 하고 분산형 구조를 통해 데이터를 여러 대의 서버에 저장합니다. 상호 복제해 특정 서버에 장애가 발생했을 때에도 데이터 유실이나 서비스 중지가 없는 형태의 구조 등 기존 관계형 Database에 최적화 되지 않은 기능을 요구합니다. 확장성, 단순한 구조, 느슨한 정합성, 낮은 비용, 빠른 검색 등의 NoSQL Database 가 적합합니다.이런 경우 MSA의 Database Per Service를 적용해 각 서비스의 목적에 맞는 최적의 Database 를 선택할 수 있습니다.\n기존의 시스템에서 서로 다른 Database를 사용하는 경우 이기종 Database 간 트랜잭션 처리가 어려웠습니다. 하지만 MSA에서는 다른 서비스의 데이터를 참조할 때, 직접 접근하지 않고 데이터의 캡슐화를 통해 데이터 오너 서비스의 API를 통해서만 접근할 수 있게 합니다. EDM은 DBMS 레벨이 아닌 Application 레벨에서 트랜잭션 처리를 수행하는데 적합해 서비스의 목적에 최적화된 Database를 선택하는데 도움을 줍니다.\n효과 MicroService에 Event Driven을 엮으면 MSA를 도입하면서 새로 발생한 요구사항(허들)을 달성할 수 있습니다.\n 비즈니스 흐름에 따른 로직 수행 분산 트랜잭션 처리 서비스 간 반정규화 데이터 동기 처리 적절한 시스템 내 통합 최종적인 일관성  비즈니스 흐름 핫초코를 구매하는 과정을 생각해봅시다.\n 메뉴 선택 카운터에서 핫초코 주문 ← 상태 변화 계산 ← 상태 변화 핫초코 만들기 ← 상태 변화 핫초코 먹기 ← 상태 변화  핫초코 구매라는 하나의 기능이 여러 단계에 걸쳐서 수행됩니다. 각 단계마다 상태 변화를 동반합니다. 이전 단계를 완료하면 그에 반응해서 다음 단계를 수행합니다.\nMSA에서 각 단계별로 서비스를 구현했다면 menu → order → pay → make → delivery 서비스 순서대로 동작할 것 입니다. 이 연결 관계에서 비즈니스 흐름(데이터 흐름)을 파악하는 것은 기능 수행을 위해 매우 중요합니다.\n이 일련의 과정에 EDM 을 적용해 상태가 변경되면 이벤트를 발생,발행하고 이를 관심있는 서비스가 수신 후 기능을 수행하면서 비즈니스 흐름에 따라 각 서비스의 기능을 수행할 수 있습니다.\n트랜잭션 처리 오류가 발생하는 상황을 가정해봅시다.\n 오류 시나리오  메뉴 선택 카운터에서 핫초코 주문 ← 상태 변화 계산 ← 상태 변화 핫초코 재고 부족 ← 오류 환불, 주문 폐기 ← rollback   비즈니스 흐름에 따라 기능을 수행하다가 중간에 문제가 발생할 수 있습니다. 문제가 발생한 시점에 rollback 또는 retry 를 수행합니다.\n기존의 Lagacy 시스템에서 문제 발생시 일관된 commit 또는 rollback 처리나 이전에 발생한 상태 변경에 직접 접근해서 데이터 수정이 가능합니다. 하지만 MSA가 적용된 시스템에서 서로 다른 서비스에 걸쳐진 기능을 수행하는 도중 일관된 commit 또는 rollback을 수행할 수 없습니다. 이 때 EDM을 적용해 rollback 또는 retry를 처리할 수 있습니다. rollback이 필요한 경우 Failed 이벤트를 발생시키고, 이를 이전 스텝을 수행한 서비스에서 구독하여 보관되어 있던 이벤트 로그 기반으로 rollback을 수행합니다. retry가 필요한 경우 메세지 큐의 requeue 또는 dead letter queue 기능을 사용해 retry 처리를 수행할 수 있습니다.\n반정규화 데이터의 동기 처리 MSA가 적용된 시스템에서는 biz.logic 과 이를 수행하는데 필요한 데이터가 서로 다른 서비스에 나뉘어진 경우가 있을 수 있습니다. 처음에는 여러 서비스 간 REST 통신을 통해 데이터 참조를 수행할 수 있으나, 서비스가 커져가면서 복잡성/성능 등의 사유로 서비스 간 데이터 반정규화가 발생할 수 있습니다. 이 때 EDM을 적용해 서비스 간 데이터 동기 처리를 수행할 수 있습니다.\n예시\n 마이크로서비스 product, category로 구성된 백엔드\n product는 category의 api를 호출해서 기능 수행\n biz. logic이 복잡해진다던지, 성능 문제가 생긴다던지 등의 반정규화 needs가 발생\n product의 Database에 category의 데이터 categoryName 반정규화\n category 의 name 변경시 이벤트 발생,발행\n product 에서 CategoryEdited 이벤트를 구독해 categoryName 변경사항 반영\n  비동기 메세지 큐를 통한 시스템 내 통합(integration)  \u0026ldquo;smart endpoints and dumb pipes.\u0026rdquo; (https://martinfowler.com/articles/microservices.html)\n MSA에서 내부 통신은 크게 2가지가 사용됩니다. 두가지 방식 모두 강점과 약점이 있습니다. 상황에 따라 적합한 방식을 선택하면 됩니다.\n REST 통신 메세지 큐 통신  REST 통신은 실시간으로 보여줘야 하는 데이터를 모으는 등 조회 기능을 수행하기 적합합니다. 하지만 모든 내부 통신을 REST로 수행할 경우 몇가지 문제점이 있을 수 있습니다.\n 서비스 간 의존성  여러 서비스간 데이터 생성,삭제,변경이 얽혀있다면 어느 서비스에서 Client의 Request를 받아서 누구에게 전달해 처리해야 하는지 모호하다. 서비스가 추가된다면 해당 기능 수행에 연관있는 모든 어플리케이션에 수정이 필요할 수 있다. 시간이 지날수록 단일 기능의 서비스에서 서비스 오케스트레이션으로 변질될 가능성이 있다.  데이터 관리 매우 어려움  서비스 간 분산 트랜잭션 처리 및 중간에 오류가 발생했을 때 복원 시점, 복원 키 값을 찾는 등에 어려움이 있다. 서비스 간 반정규화 된 동일한 데이터를 변경할 때 요청 받은 서비스가 반정규화된 데이터가 위치한 서비스를 모두 찾아서 변경하도록 처리해야 한다. 데이터 오너 서비스가 반정규화 해간 서비스 리스트를 관리하지 않는 이상 그에 따른 데이터의 일관성을 유지하기 어렵다.  동기 통신의 비용  외부에서 API G/W를 통해 요청이 전달됐을 때, 응답을 하기 까지 연관된 모든 서비스의 자원을 홀딩한다.   비동기 메세지 큐 방식은 시스템 내 통합에 적합합니다.\n 서비스 간 결합도가 낮아진다. 서비스의 흐름이 단순해진다.  발행 서비스는 구독 서비스들을 고려할 필요 없이 데이터의 생성,변경,삭제(CUD)가 발생하면 이벤트를 발행한다. 구독 서비스는 메세지 큐의 라우팅 룰에 따라 전달되는 이벤트를 구독해 자신만의 biz. logic을 수행한다.  응답 지연시간이 낮아진다.  실시간으로 처리 및 전달해야하는 기능을 먼저 수행하고 나머지 후속동작은 메세지 큐를 통하게 적용한다면 응답 지연 시간을 낮출 수 있다.   추가적으로 고려할 사항은 메세지 전달 신뢰성 문제입니다.\n 메세지가 여러번 전달됨으로 인한 데이터 정합성 문제  메세지의 멱등 설계 또는 exactly once 전달을 고려해야 한다.  서비스의 스케일링에 대비해야 한다.  스케일링된 동일한 서비스에서 하나의 메세지를 경쟁적으로 구독할 수 있는 구조를 고민해야 한다.  장애 포인트  메세지 큐 장애가 일어나면 전체 시스템의 장애 요소가 된다. 서비스 장애 발생시 retry/rollback을 고려해야 한다. 부하가 발생할 수 있는 가능성이 생긴다.   RabbitMQ, Kafka 등 Backing Service를 사용해 queue를 통한 메세지 순서 정렬, exactly once 속성을 사용한 멱등성, 정상적으로 처리되지 않은 이벤트의 requeue/dlq 적재 기능을 통한 복원 등 장점을 활용할 수 있습니다.\nEventually Consistency 기존의 DBMS 활용한 ACID 트랜잭션에 따른 데이터 무결성 보장은 MSA 에서 서비스, Database가 나눠짐으로써 더이상 달성할 수 없습니다. 대신 EDM을 사용해 데이터의 최종적인 일관성 유지로 변경됩니다.\n all commit or rollback → eventually consistency  구현 Event Driven MicroService의 요구사항 및 달성 효과는 전혀 새로운 것이 아닙니다. DBMS 내 트랜잭션 로그를 활용한 redo/undo 로직, 이기종 Database 간 분산 트랜잭션 처리, 외부 시스템과 I/F를 통해 업무를 처리할 때 상이한 시스템 간 정합성 처리 등 기존에 다루던 문제들과 요구사항 및 구현 방식이 비슷합니다. 따라서 비슷한 문제를 해결하는 방식들을 참고해 필요한 Insight를 도출할 수 있습니다.\nInsight 시스템에 Event Driven을 적용하기 위해 기존과 달라지는 사항들이 있습니다.\n 업무에 기반한 의미있는 이벤트 설계  정상 시나리오에서 이벤트 흐름 도출 장애 발생시 retry/rollback 여부에 따른 이벤트 흐름 도출 이벤트 흐름 시각화 방법 Scaling된 Application 간 경쟁적인 event msg 구독  outer arch.\n 이벤트 스토어 설계/구현  Application + Backing Service 요구사항 별 별도의 뷰 제공 메세지 큐 기능 exactly once 보장 dlq 기능 고가용성 내결함성 throughput event message persistent retry 설정 time out 설정 이벤트 라우팅 룰/전략 확장성  유용한 Backing Service  nosql 관계형 Database RabbitMQ Kafka Geode CSP 내 여러 메세지 큐 서비스 기타 등등  Backing Service를 통해 선처리 가능한 요구사항을 먼저 해결하고 미비한 부분을 Application 으로 기능 구현합니다.  Application  mvc + pub/sub  project package 구조 정의 event 보관/발행을 위한 기능 설계/구현  event msg 설계/구현\n Application 별 event type 도출 event type 에 대응되는 fail event type도출 event 멱등 처리 고려  데이터 상태 변경 후 이벤트 생성+발행 logic 설계/구현 보상 트랜잭션\n 오류 발생시 fail 이벤트 생성+발행 logic 설계/구현 transactionId 생성,전달 방법 설계/구현  비동기 통신  비동기 통신을 통한 내부 통합에 따른 Application 기능 재설계 필요  ex) 하나의 기능을 수행하기 위해 필요한 데이터들을 젤 앞단에서 다 받아서 처리, 사용자와 interaction하면서 기능 수행 어려움    기타 등등  Architecture Example \n 서비스 간 통신 방식  조회 기능은 REST 통신을 수행합니다. 데이터 변경 이벤트 및 보상 트랜잭션 처리를 위한 이벤트는 메세지 큐를 통해 동작합니다.  event loop  서비스의 상태 변경 → 이벤트 발생 → 이벤트 스토어를 통해 전파 → 관심있는 서비스에서 구독 → 반복 \u0026hellip; 서비스 로직 수행 중 오류 발생 → fail 이벤트 발생 → 이벤트 스토어를 통해 전파 → 관심있는 서비스에서 구독 → 이벤트 로그 를 조회해서 rollback ← 반복 \u0026hellip;  이벤트 스토어의 설계/구현/연계가 기존과 가장 다른 점 입니다.  Conclusion Event Driven MicroService 는 MicroService Architecture 도입시 발생되는 새로운 요구사항(허들)을 달성하는 개념입니다. 서비스, Database의 분리, 폴리글랏 등에 대응해 효과적으로 동작합니다. 이를 통해 문제를 해결하고 MSA 도입의 이점을 얻을 수 있습니다.\nEDM에서 이벤트는 데이터의 생성,변경,삭제로 정의합니다. 이벤트가 발생하면 이벤트 스토어에 보관하고 메세지 큐를 통해 발행합니다. 해당 이벤트에 관심이 있는 서비스에서 구독 후 정해진 biz. logic 을 수행합니다. biz. logic을 수행한 결과 이벤트가 발생하면 이 또한 이벤트 스토어에 보관하고 메세지 큐를 통해 발행합니다. 이런 이벤트 흐름이 무한 루프를 형성하지 않게 설계에 신경쓰고 Validation Check를 수행해야 합니다. 보관된 이벤트 기록은 장애 또는 특정 요구사항에 따라 데이터를 복원하는데 사용됩니다. 이 때 복원 시점, 복원 키값을 지정할 수 있게 이벤트 객체 설계가 필요합니다.\nEvent Driven MicroService의 요구사항 및 달성 효과는 전혀 새로운 것이 아닙니다. 비슷한 문제를 다루는 내용으로 다양한 방법이 소개되고 있습니다. 이들 중 하나를 꼭 선택해야 하는 것은 아니고 핵심 factor를 도출해 시스템에 적합한 Architecture 수립 및 설계/개발 과정에 적용하는 것이 중요합니다. EDM의 Architecture는 RDB + RabbitMQ, Kafka, Kafka + KSQL, Reactive + NoSQL, Serverless 등 Application과 Backing Service의 조합으로 굉장히 다양하게 구성할 수 있습니다. 유용한 Backing Service를 통해 핵심 factor 를 우선 달성하고 미비한 기능은 Application으로 커버하면 변경사항을 줄일 수 있습니다. 비동기 통신을 통한 내부 통합이라는 특성으로 인해 하나의 기능을 수행할 때 세부 단계로 나눠 사용자와 인터렉션을 통해 상태를 변경하는 방식은 적합하지 않을 수 있습니다. 기능 수행의 젤 앞단에서 필요한 데이터 세팅을 마치고 백엔드 서비스로 전달을 하는 방식으로 Application flow의 재설계가 필요합니다. 이벤트 기반과 비동기 통신을 통한 시스템 내 통합(integration)이라는 핵심 개념을 바탕으로 실제 구현은 자유롭게 구성 가능합니다. 특정 방식을 따르기보다 조직/개인의 역량에 알맞는 방법을 선택하는 것이 가장 중요합니다.\nMonolitic Architecture의 단점에 질려서 혹은 시스템을 Cloud 환경에 구축할 때 얻을 수 있는 이점과 MicroService Architecture의 장점에 끌려서 MicroService Architecture의 도입을 결정했다면, 그 다음 닥칠 문제에 대비해 Event Driven을 고려해볼 필요가 있습니다.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/spinnaker-deploy-helm-chart/",
        "title": "Spinnaker를 활용한 Helm chart 배포",
        "tags": ["spinnaker", "kubernetes", "helm", "devops"],
        "description": "Spinnaker를 활용한 Helm chart 배포",
        "content": " 이 페이지는 Spinnaker에서 Helm chart를 배포하는 방법에 대해 설명합니다.\n자세한 내용은 Spinnaker 공식 가이드 문서의 Deploy Helm Charts를 참고하세요.\n사전 준비 GitHub 계정 생성 및 저장소 fork GitHub 계정을 생성하고 아래 저장소를 fork 합니다.\nhttps://github.com/YunSangJun/my-charts\nGitHub Webhook 설정 GitHub Webhooks 설정하기\nGitHub Artifact 설정 GitHub Artifact 설정하기\nApplication 생성 Spinnaker top menu \u0026gt; Applications \u0026gt; 우측 Actions dropbox \u0026gt; Create Application을 선택합니다.\n아래와 같이 내용 입력 후 Create 버튼을 선택합니다.\n Name : helm-chart-demo Owner Email : 애플리케이션 소유자의 이메일  Pipeline 생성 Application top \u0026gt; PIPELINES \u0026gt; Configure a new pipeline을 선택합니다.\n아래와 같이 내용 입력 후 Create 버튼을 선택합니다.\n Type : Pipeline Pipeline Name : Deploy helm chart  Pipeline 설정 Automated Triggers Pipeline configuration \u0026gt; Automated Triggers \u0026gt; Add Trigger를 선택합니다.\n아래와 같이 내용을 입력하고 Save Changes 버튼을 선택해 변경사항을 저장합니다.\n Type : Git Repo Type : github Organization or User : GitHub 조직명 또는 사용자 명.  예) https://github.com/YunSangJun/my-charts \u0026gt; YunSangJun이 사용자 명\n Project : my-charts Branch : master Secret : GitHub Webhooks 설정하기에서 입력한 secret  Expected Artifacts Pipeline configuration \u0026gt; Expected Artifacts \u0026gt; Add Artifact를 선택합니다.\n아래와 같이 내용을 입력하고 Save Changes 버튼을 선택해 변경사항을 저장합니다.\n Match against : GitHub File path : stable/demo-0.1.0.tgz  Bake Manifest Helm chart로 부터 Kubernetes manifest를 생성하는 stage를 구성합니다.\nStage 추가 Pipeline \u0026gt; Configuration \u0026gt; Add stage를 선택합니다.\n아래와 같이 내용을 입력하고 Save Changes 버튼을 선택해 변경사항을 저장합니다.\n Type : Bake(Manifest) Stage Name : Bake manifest for IBM  Stage 설정 아래와 같이 내용을 입력하고 Save Changes 버튼을 선택해 변경사항을 저장합니다.\n Render Engine : HELM2 Name : demo Namespace : demo Expected Artifact : stable/demo-0.1.0.tgz Artifact Account : GitHub Artifact 설정하기에서 추가한 GitHub 계정 Overrides :  Key : ingress.enabled, Value : true Key : ingress.hosts[0], Value : 애플리케이션 호스트 명 입력(예: demo.ibm.example.com)   Produces Artifacts Bake Manifest 설정을 완료하면 Produces Artifacts에 생성되는 Kubernetes Manifest에 대한 설정이 자동으로 설정됩니다.\n이 설정을 다음 단계인 Deploy Manifest에서 사용합니다.\nMulti cloud/cluster 추가로 다른 클라우드의 클러스터에 추가로 Helm chart를 배포해야 하는 경우 위와 같은 stage를 동일하게 추가하고 설정합니다.\n이 페이지에서는 같은 Helm chart이지만 ingress의 host명을 다르게 해서 배포하도록 설정해보겠습니다.\nPipeline \u0026gt; Configuration \u0026gt; Add stage를 선택합니다.\n아래와 같이 내용을 입력하고 Save Changes 버튼을 선택해 변경사항을 저장합니다.\n Type : Bake(Manifest) Stage Name : Bake manifest for GCP   Render Engine : HELM2 Name : demo Namespace : demo Expected Artifact : stable/demo-0.1.0.tgz Artifact Account : GitHub Artifact 설정하기에서 추가한 GitHub 계정 Overrides :  Key : ingress.enabled, Value : true Key : ingress.hosts[0], Value : 애플리케이션 호스트 명 입력(예: demo.gcp.example.com)   Deploy Manifest 생성한 Kubernetes manifest를 배포하는 stage를 구성합니다.\nStage 추가 Pipeline \u0026gt; Bake manifest IBM \u0026gt; Add stage를 선택합니다.\n아래와 같이 내용을 입력하고 Save Changes 버튼을 선택해 변경사항을 저장합니다.\n Type : Deploy(Manifest) Stage Name : Deploy to IBM Depends On : Bake manifest for IBM  Stage 설정 아래와 같이 내용을 입력하고 Save Changes 버튼을 선택해 변경사항을 저장합니다.\n Account : IBM Kubernetes Account Manifest Source : Artifact Expected Artifact : b64 name:demo, type:embedded/base64(Bake Manifest에서 생성한 Kubernetes manifest) Artifact Account : embedded-artifact  Multi cloud/cluster 추가로 다른 클라우드의 클러스터에 추가로 Helm chart를 배포해야 하는 경우 위와 같은 stage 동일하게 추가하고 설정합니다.\nPipeline \u0026gt; Bake manifest GCP \u0026gt; Add stage를 선택합니다.\n아래와 같이 내용을 입력하고 Save Changes 버튼을 선택해 변경사항을 저장합니다.\n Type : Deploy(Manifest) Stage Name : Deploy to GCP Depends On : Bake manifest for GCP  아래와 같이 내용을 입력하고 Save Changes 버튼을 선택해 변경사항을 저장합니다.\n Account : GCP Kubernetes Account Manifest Source : Artifact Expected Artifact : b64 name:demo, type:embedded/base64(Bake Manifest에서 생성한 Kubernetes manifest) Artifact Account : embedded-artifact  Helm chart 배포 Helm chart를 생성 또는 업데이트하여 이벤트를 발생시킵니다.\ngit clone https://github.com/Organization_Or_User/my-charts cd my-charts helm package demo mv demo-0.1.0.tgz stable helm repo index stable --url https://Organization_Or_User.github.io/my-charts/stable git add --all git push 결과 확인 GitHub 저장소의 stable/demo-0.1.0.tgz에 변경사항이 있으면 Pipeline이 실행됩니다.\nPipeline status가 성공적으로 완료되면(SUCCEEDED) 서버자원이 생성된 것을 확인할 수 있습니다.\nService와 Ingress도 생성된 것을 확인할 수 있습니다.\nMulti cloud/cluster에 배포한 경우 ingress host 주소가 다른 것을 확인 할 수 있습니다.\n$ kubectl get ing -n demo NAME HOSTS ADDRESS PORTS AGE demo demo.ibm.example.com 169.xx.xxx.xxx 80 12h $ kubectl get ing -n demo NAME HOSTS ADDRESS PORTS AGE demo demo.gcp.example.com 35.xx.xxx.xxx 80 12h"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/fluent-bit/",
        "title": "경량화 Log Processor &amp; Forwarder : Fluent Bit",
        "tags": ["logging", "fluentbit", "fluentd", "fluent", "로깅", "kubernetes"],
        "description": "Fluentd의 꼭 필요한 기능만을 분리해서 나온 FluentBit을 집중 분석",
        "content": "  Fluent Bit v0.13을 기준으로 작성된 포스팅입니다. https://fluentbit.io\n Log Collector 요즘은 오픈소스화된 다양한 플랫폼이 나오게 되면서 데이터 정보를 수집하기 위해 어려움이 발생합니다. 서로간의 정보의 출처도 다를뿐더러 다양한 데이터 포맷으로 전달해 오는 데이터를 처리해야만 하고 마지막으로 최종 목적지 또한 다를 수 있습니다.\n이러한 요구를 만족시키기 위해 2011년 Fluentd라는 프로젝트가 탄생하게 됩니다. Ruby로 작성된 Fluentd는 여러 소스의 데이터를 집계하고 형식이 다른 데이터를 JSON 객체로 통합하여 다른 출력 대상으로 라우팅 할 수 있는 원 스톱 구성 요소인 통합 로깅 레이어로 작동하도록 개발되었습니다.\n약 40MB의 메모리에서 실행이 되며, 초당 10,000 이벤트 이상을 처리할 수 있습니다. 이렇게 확산된 Fluentd가 현재는 Datadog에서 분석한 2018년 가장 많이 사용되는 Docker Image 순위에 4위로 랭크하게 됩니다. (2017년 8위 → 2018년 4위)\n 요즘은 기존의 ELK(Elastic Search + Logstash + Kibana)의 구조에서 EFK(Elastic Search + Fluentd + Kibana) 형태로도 널리 사용되는 추세입니다.\n Fluentd \u0026amp; Fluent Bit? Fluentd와 Fluent Bit 둘 모두 로그를 수집하고, 처리한 다음 전달해주는 역할을 수행합니다.\nFluentd와 비교하는 자료는 위와 같은 한 장의 그림으로 정리할 수 있습니다. Fluentd의 영역이 collector, processor, aggregator의 역할을 수행한다고 하면, Fluent Bit는 그중에 collector와 processor에 중점을 두었습니다.\n아래는 Fluentd와 Fluent Bit를 비교한 표입니다.\n    Fluentd Fluent Bit     Scope Containers / Servers Containers / Servers   Language C \u0026amp; Ruby C   Memory ~40MB ~450KB   Performance High Performance High Performance   Dependencies Built as a Ruby Gem, it requires a certain number of gems. Zero dependencies, unless some special plugin requires them.   Plugins More than 650 plugins available Around 35 plugins available   License Apache License v2.0 Apache License v2.0    Fluentd와 비교했을 때 차지하는 메모리가 1\u0026frasl;100 정도로 경량화 되어 있는 것을 볼 수 있고, 플러그인 갯수에도 차이가 있습니다.\n이 둘을 병행하여 사용하는 경우도 많이 볼 수 있습니다. 이럴 경우 주로 Fluent Bit는 로그를 전달자의 역할을 중점적으로 수행하도록 구성하고, Fluentd는 다양한 플러그인을 중심으로 aggregator의 역할을 수행하도록 구성합니다. 이렇게 하면 더 신뢰성 있는 솔루션을 제공할 수 있습니다.\nFluent Bit Work Flow    Interface Description     Input 데이터의 진입점. Input Plugin을 통해 구현된 인터페이스는 데이터를 수집하거나 수신할 수 있게 합니다. 예) 로그 파일 컨텐츠, TCP 데이터, Metric등   Parser Input 인터페이스에서 수집한 비정형 데이터를 구조화된 데이터로 변환해주는 역할을 합니다. (입력 플러그인에 따라 다를 수 있습니다)   Filter Input에서 수집한 데이터를 변경할 수 있습니다.   Buffer Input된 데이터는 output으로 전달될 때까지 메모리에서 대기하고 있습니다.   Routing 수집된 데이터는 각각 태깅을 수행하고, 태그된 규칙에 따라 라우팅을 수행합니다.   Output 데이터의 목적지를 지정합니다. 지정된 목적지는 라우팅을 통해 전달됩니다.    Input Plugins 입력 플러그인은 여러 소스에서 데이터를 받아오기 위해 제공되는 것으로 다양한 데이터에 따른 플러그인이 있습니다.\n   name title description     cpu CPU Usage 시스템의 CPU 사용량을 측정합니다.   disk Disk Usage 디스크 I/O를 측정합니다.   dummy Dummy 더미 이벤트를 생성합니다.   exec Exec 외부 프로그램을 실행하고 이벤트 로그를 수집합니다.   forward Forward Fluentd로 전달할 수 있는 프로토콜입니다.   head Head 파일의 앞부분을 읽어서 처리합니다.   health Health TCP services의 상태를 확인합니다.   kmsg Kernel Log Buffer 리눅스 커널 로그 버퍼메시지를 읽습니다.   mem Memory Usage 메모리를 측정합니다.   mqtt MQTT TCP 접속을 통해 MQTT 제어 패킷의 메시지 / 데이터를 검색 할 수 있습니다.   netif Network Traffic 트래픽을 분석합니다.   proc Process 프로세스의 상태를 체크합니다.   random Random 랜덤 샘플을 생성합니다.   serial Serial Interface 직렬 인터페이스의 데이터를 읽습니다.   stdin Standard Input 표준 input 데이터를 읽습니다.   syslog Syslog Unix 소켓에서 syslog를 읽습니다.   systemd Systemd Systemd/Journald 로그를 읽습니다.   tail Tail 파일의 뒷부분을 읽어서 처리합니다.   tcp TCP TCP를 통해서 JSON 메시지를 읽습니다.    Plugins\n몇 가지의 Input 플러그인을 살펴 보겠습니다.\n CPU\nCPU 사용량을 측정하는 Input 플러그인입니다. - Config 설정 파일에 셋팅하는 방법\n[INPUT] Name cpu Tag my_cpu [OUTPUT] Name stdout Match * - 명령줄로 실행하는 방법\nfluent-bit -i cpu -t my_cpu -o stdout -m \u0026#39;*\u0026#39; Fluent-Bit v0.13.4 Copyright (C) Treasure Data [2018/07/23 08:19:40] [ info] [engine] started (pid=49) [0] my_cpu: [1532333981.000247881, {\u0026#34;cpu_p\u0026#34;=\u0026gt;1.250000, \u0026#34;user_p\u0026#34;=\u0026gt;0.500000, \u0026#34;system_p\u0026#34;=\u0026gt;0.750000, \u0026#34;cpu0.p_cpu\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu0.p_user\u0026#34;=\u0026gt;0.000000, \u0026#34;cpu0.p_system\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu1.p_cpu\u0026#34;=\u0026gt;0.000000, \u0026#34;cpu1.p_user\u0026#34;=\u0026gt;0.000000, \u0026#34;cpu1.p_system\u0026#34;=\u0026gt;0.000000, \u0026#34;cpu2.p_cpu\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu2.p_user\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu2.p_system\u0026#34;=\u0026gt;0.000000, \u0026#34;cpu3.p_cpu\u0026#34;=\u0026gt;2.000000, \u0026#34;cpu3.p_user\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu3.p_system\u0026#34;=\u0026gt;1.000000}] [1] my_cpu: [1532333982.000199660, {\u0026#34;cpu_p\u0026#34;=\u0026gt;1.750000, \u0026#34;user_p\u0026#34;=\u0026gt;0.750000, \u0026#34;system_p\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu0.p_cpu\u0026#34;=\u0026gt;3.000000, \u0026#34;cpu0.p_user\u0026#34;=\u0026gt;2.000000, \u0026#34;cpu0.p_system\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu1.p_cpu\u0026#34;=\u0026gt;2.000000, \u0026#34;cpu1.p_user\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu1.p_system\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu2.p_cpu\u0026#34;=\u0026gt;2.000000, \u0026#34;cpu2.p_user\u0026#34;=\u0026gt;0.000000, \u0026#34;cpu2.p_system\u0026#34;=\u0026gt;2.000000, \u0026#34;cpu3.p_cpu\u0026#34;=\u0026gt;2.000000, \u0026#34;cpu3.p_user\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu3.p_system\u0026#34;=\u0026gt;1.000000}] [2] my_cpu: [1532333983.000195037, {\u0026#34;cpu_p\u0026#34;=\u0026gt;1.250000, \u0026#34;user_p\u0026#34;=\u0026gt;0.750000, \u0026#34;system_p\u0026#34;=\u0026gt;0.500000, \u0026#34;cpu0.p_cpu\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu0.p_user\u0026#34;=\u0026gt;0.000000, \u0026#34;cpu0.p_system\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu1.p_cpu\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu1.p_user\u0026#34;=\u0026gt;0.000000, \u0026#34;cpu1.p_system\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu2.p_cpu\u0026#34;=\u0026gt;0.000000, \u0026#34;cpu2.p_user\u0026#34;=\u0026gt;0.000000, \u0026#34;cpu2.p_system\u0026#34;=\u0026gt;0.000000, \u0026#34;cpu3.p_cpu\u0026#34;=\u0026gt;2.000000, \u0026#34;cpu3.p_user\u0026#34;=\u0026gt;2.000000, \u0026#34;cpu3.p_system\u0026#34;=\u0026gt;0.000000}] [3] my_cpu: [1532333984.000164192, {\u0026#34;cpu_p\u0026#34;=\u0026gt;3.000000, \u0026#34;user_p\u0026#34;=\u0026gt;1.500000, \u0026#34;system_p\u0026#34;=\u0026gt;1.500000, \u0026#34;cpu0.p_cpu\u0026#34;=\u0026gt;3.000000, \u0026#34;cpu0.p_user\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu0.p_system\u0026#34;=\u0026gt;2.000000, \u0026#34;cpu1.p_cpu\u0026#34;=\u0026gt;3.000000, \u0026#34;cpu1.p_user\u0026#34;=\u0026gt;2.000000, \u0026#34;cpu1.p_system\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu2.p_cpu\u0026#34;=\u0026gt;2.000000, \u0026#34;cpu2.p_user\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu2.p_system\u0026#34;=\u0026gt;1.000000, \u0026#34;cpu3.p_cpu\u0026#34;=\u0026gt;4.000000, \u0026#34;cpu3.p_user\u0026#34;=\u0026gt;2.000000, \u0026#34;cpu3.p_system\u0026#34;=\u0026gt;2.000000}] 위에서 나오는 결과는 아래 표에서 정의하는 내용을 출력합니다.\n   Key 설명 Node      cpu_p 전체 시스템 CPU 사용량. 사용자 + 시스템 값 cpuN.p_cpu 코어 N의 전체 CPU 사용량   user_p 사용자의 CPU 사용량 cpuN.p_user 코어 N의 사용자의 CPU 사용량   system_p 커널(시스템) 모드의 CPU 사용량 cpuN.p_system 코어 N의 커널(시스템)의 CPU 사용량     해당 정보는 1초 단위로 수집되고 5초 단위로 출력에 전달합니다.\n Tail\nTail 플러그인은 한 개 이상의 텍스트 파일을 모니터링 할 수 있습니다. 실제 리눅스 쉘에서 tail -f 명령과 비슷한 동작을 합니다. conf 파일에 설정하는 방법은 다음과 같습니다.\n# input-custom.conf [INPUT] Name tail Tag logs-from-fluentbit.* Path /var/logs/spring-boot-logging.log Parser spring-file-log DB /var/log/flb_kube.db Mem_Buf_Limit 5MB Skip_Long_Lines On Refresh_Interval 10 Parser 구조화 되지 않은 데이터를 구조화된 데이터로 변환해주는 부분입니다. Input에서 들어온 데이터를 분석하여 구문에 맞게 구조화 해줍니다. 여러개의 파서를 정의하여 선택적으로 사용할 수 있습니다.\n공식 홈페이지에서 제공하는 샘플 parser들은 다음과 같습니다.(fluent-bit-configmap.yaml)\n[PARSER] Name apache Format regex Regex ^(?\u0026lt;host\u0026gt;[^ ]*) [^ ]* (?\u0026lt;user\u0026gt;[^ ]*) \\[(?\u0026lt;time\u0026gt;[^\\]]*)\\] \u0026#34;(?\u0026lt;method\u0026gt;\\S+)(?: +(?\u0026lt;path\u0026gt;[^\\\u0026#34;]*?)(?: +\\S*)?)?\u0026#34; (?\u0026lt;code\u0026gt;[^ ]*) (?\u0026lt;size\u0026gt;[^ ]*)(?: \u0026#34;(?\u0026lt;referer\u0026gt;[^\\\u0026#34;]*)\u0026#34; \u0026#34;(?\u0026lt;agent\u0026gt;[^\\\u0026#34;]*)\u0026#34;)?$ Time_Key time Time_Format %d/%b/%Y:%H:%M:%S %z [PARSER] Name apache2 Format regex Regex ^(?\u0026lt;host\u0026gt;[^ ]*) [^ ]* (?\u0026lt;user\u0026gt;[^ ]*) \\[(?\u0026lt;time\u0026gt;[^\\]]*)\\] \u0026#34;(?\u0026lt;method\u0026gt;\\S+)(?: +(?\u0026lt;path\u0026gt;[^ ]*) +\\S*)?\u0026#34; (?\u0026lt;code\u0026gt;[^ ]*) (?\u0026lt;size\u0026gt;[^ ]*)(?: \u0026#34;(?\u0026lt;referer\u0026gt;[^\\\u0026#34;]*)\u0026#34; \u0026#34;(?\u0026lt;agent\u0026gt;[^\\\u0026#34;]*)\u0026#34;)?$ Time_Key time Time_Format %d/%b/%Y:%H:%M:%S %z [PARSER] Name apache_error Format regex Regex ^\\[[^ ]* (?\u0026lt;time\u0026gt;[^\\]]*)\\] \\[(?\u0026lt;level\u0026gt;[^\\]]*)\\](?: \\[pid (?\u0026lt;pid\u0026gt;[^\\]]*)\\])?( \\[client (?\u0026lt;client\u0026gt;[^\\]]*)\\])? (?\u0026lt;message\u0026gt;.*)$ [PARSER] Name nginx Format regex Regex ^(?\u0026lt;remote\u0026gt;[^ ]*) (?\u0026lt;host\u0026gt;[^ ]*) (?\u0026lt;user\u0026gt;[^ ]*) \\[(?\u0026lt;time\u0026gt;[^\\]]*)\\] \u0026#34;(?\u0026lt;method\u0026gt;\\S+)(?: +(?\u0026lt;path\u0026gt;[^\\\u0026#34;]*?)(?: +\\S*)?)?\u0026#34; (?\u0026lt;code\u0026gt;[^ ]*) (?\u0026lt;size\u0026gt;[^ ]*)(?: \u0026#34;(?\u0026lt;referer\u0026gt;[^\\\u0026#34;]*)\u0026#34; \u0026#34;(?\u0026lt;agent\u0026gt;[^\\\u0026#34;]*)\u0026#34;)?$ Time_Key time Time_Format %d/%b/%Y:%H:%M:%S %z [PARSER] Name json Format json Time_Key time Time_Format %d/%b/%Y:%H:%M:%S %z [PARSER] Name docker Format json Time_Key time Time_Format %Y-%m-%dT%H:%M:%S.%L Time_Keep On # Command | Decoder | Field | Optional Action # =============|==================|================= Decode_Field_As escaped log [PARSER] Name syslog Format regex Regex ^\\\u0026lt;(?\u0026lt;pri\u0026gt;[0-9]+)\\\u0026gt;(?\u0026lt;time\u0026gt;[^ ]* {1,2}[^ ]* [^ ]*) (?\u0026lt;host\u0026gt;[^ ]*) (?\u0026lt;ident\u0026gt;[a-zA-Z0-9_\\/\\.\\-]*)(?:\\[(?\u0026lt;pid\u0026gt;[0-9]+)\\])?(?:[^\\:]*\\:)? *(?\u0026lt;message\u0026gt;.*)$ Time_Key time Time_Format %b %d %H:%M:%S  설정된 Parser를 Input에 넣어줍니다.\n[INPUT] Name tail ... Parser apache #parser 선택 ... Refresh_Interval 10  Fluentd 정규식을 테스트해보고 결과를 바로 가져올 수 있는 사이트 : http://fluentular.herokuapp.com/\n Filter 필터 플러그인을 사용하면 입력 플러그인에서 생성된 수신 데이터를 변경할 수 있습니다.\n   name title description     grep Grep 패턴 매칭하여 특정 레코드만 포함하거나 제거하는 필터링을 수행합니다.   kubernetes Kubernetes Fluent Bit이 Kubernetes에 노드의 Demonset 형태로 배포되면 각 컨테이너의 정보를 가져오는 필터로 작용합니다.   parser Parser Parse 필터는 Input으로 들어오는 데이터를 특정 구문에서 필드로 구분할 수 있게 해줍니다.   record_modifier Record Modifier 특정 필드를 추가하거나 제거할 수 있습니다.   stdout Stdout 입력에서 받은 데이터를 표준 출력으로 전달할 수 있게 해줍니다.   throttle Throttle 이벤트 속도 제한을 적용해줍니다.   nest Nest 지정된 키 아래 레코드를 Nest 구조로 구성해줄 수 있습니다.   modify Modify 레코드를 수정할 수 있습니다.    Routing 라우팅은 필터를 통해 데이터를 라우팅하고 마지막으로 하나 이상의 대상으로 라우팅 할 수 있게 해주는 핵심 기능입니다 .\n아래는 OUTPUT으로 해당 라우팅을 수행하는 방법입니다.\n[INPUT] Name cpu Tag my_cpu [INPUT] Name mem Tag my_mem [OUTPUT] Name es Match my_cpu [OUTPUT] Name stdout Match my_mem 라우팅에서 중요한 것은 Tag와 Match입니다.\nTag는 Input에서 들어오는 데이터를 구분하기 위한 꼬리표이고, Match는 라우팅 규칙을 통해 Output을 전달하는 역할을 수행합니다.\n지정된 태그를 아래와 같이 wild card로 지정할 수도 있습니다.\n[INPUT] Name cpu Tag my_cpu [INPUT] Name mem Tag my_mem [OUTPUT] Name stdout Match my_* Output 출력 인터페이스를 사용하여 데이터의 대상을 정의 할 수 있습니다. 이것은 원격 서비스, 로컬 파일 시스템 또는 다른 시스템의 표준 인터페이스입니다.\nFluent Bit에서 사용할 수 있는 Output Plugin은 다음과 같습니다.\n   name title description     azure Azure Log Analytics Azure Log Analytics에 전달합니다.   counter Count Records 레코드의 갯수를 측정하는 간단한 플러그인입니다.   es Elasticsearch Elasticsearch 서버로 전달합니다.   file File 파일형태로 전달합니다.   flowcounter FlowCounter 레코드의 크기와 갯수를 셀 수 있는 플러그인입니다.   forward Forward Fluentd 로 전달합니다.   http HTTP 특정 HTTP 엔드포인트로 전달합니다. (POST)   influxdb InfluxDB 시계열 DB인 InfluxDB로 전달합니다.   Kafka Apache Kafka Apache Kafka로 전달합니다.   kafka-rest Kafka REST Proxy Kafka REST Proxy 서버로 전달합니다.   stdout Standard Output 표준 Output으로 전달합니다.   splunk Splunk Splunk Enterprise로 전달합니다.   td Treasure Data 분석을 위해 Treasure Data cloud service 로 전달합니다.   nats NATS NATS 서버로 전달합니다.   null NULL 이벤트를 전달하지 않습니다.    Fluent Bit in Kubernetes Fluent Bit을 Kubernetes에서 활용하는 예제는 다음 포스팅 글을 참고하시길 바랍니다. Logging in Kubernetes\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/monitoring-in-kubernetes/",
        "title": "Monitoring in kubernetes",
        "tags": ["monitoring", "kubernetes", "prometheus", "cncf", "grafana", "모니터링"],
        "description": "Cloud Native 환경에서 Monitoring Architecture 소개 및 실습",
        "content": " Container 환경에서 떠오르는 도전 과제 운영 환경으로 Container 환경 사용 운영 환경에서 Kubernets 사용 비율 증가. =\u0026gt; Container 기반 운영 환경 증가\nContainer 환경에서 떠오르는 도전 과제 Container 환경을 운영 환경으로 고려하기 시작하면서 운영을 위해 꼭 필요한 모니터링에 대한 관심 증가\n[출처 : CNCF https://www.cncf.io/blog/2017/12/06/cloud-native-technologies-scaling-production-applications/]\nCloud Native 환경에서 Monitoring Architecture의 변화 Legacy  고사양의 서버에 Application을 크게 운영 Monitoring Agent를 서버에 설치 Agent가 App 및 OS의 metric 수집해 Backend에 전송  Cloud Native  Application을 작게 운영하고 필요할 때 마다 확장 동적으로 확장하는 서버에 Agent 설치 불가능 Kubernetes API를 통해 동적으로 확장된 서버 endpoint를 discovery Monitoring Backend에서 discovery한 endpoint를 통해 metric 수집  Monitoring Architecture Prometheus  Service Discovery, Metric 수집 및 저장, Alert 기능을 통합해 제공하는 Monitoring 시스템 CNCF의 메인 프로젝트로 Container 기반 Monitoring 시스템의 defactor Kubernetes외의 다른 Cloud Provider에 대한 Service Discovery 기능 제공으로 동적인 Cloud를 효율적을 모니터링 자체 Alert 엔진 보유. 외부 시스템과 연계하여 Alarm을 송신 가능 Web UI와 Grafana를 통해 Data 시각화 자체 TSDB(Time Series Database) 보유. Metric data 저장 및 관리에 최적화 다양한 exporter(수집기)를 제공해 외부 시스템 통합 모니터링 가능  Service Discovery \u0026amp; Scrape  Prometheus가 Kubernetes API Server로 부터 monitoring metric 수집 대상을 discovery 각 대상으로 부터 metic scrape(pull) Pod(Application)이 동적으로 증가하면 discovery 통해 자동으로 수집 대상에 추가  Monitoring 실습 실습 목표  Demo Application 및 JMX Exporter를 Kubernetes cluster에 배포 Kubernetes로 부터 App 및 JMX Exporter target 정보를 discovery Target으로 부터 Pod 및 JVM metrics 수집 Grafana dashboard에서 Pod 및 JMX metrics 정보를 시각화   참고 : 이 실습은 Kubernetes cluster를 위한 Prometheus와 Grafana가 설치되어 있다는 전제하에 작성했습니다. Prometheus와 Grafana 설치에 대한 가이드는 추후 작성 예정입니다.  Demo Application Download 및 Build  Demo Application 다운로드\n$ git clone https://github.com/cloudz-labs/spring-boot-monitoring-demo.git $ cd spring-boot-monitoring-demo Maven build\n$ mvn clean package Dockerfile 편집\n$ vi Dockerfile FROM openjdk:8-jdk-alpine ADD target/spring-boot-monitoring-demo-0.0.1-SNAPSHOT.jar app.jar ADD jmx-exporter/jmx_prometheus_javaagent-0.3.1.jar jmx_prometheus_javaagent-0.3.1.jar ADD jmx-exporter/tomcat.yaml tomcat.yaml ENTRYPOINT [\u0026#34;java\u0026#34;,\u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34;,\u0026#34;-javaagent:/jmx_prometheus_javaagent-0.3.1.jar=8090:/tomcat.yaml\u0026#34;,\u0026#34;-jar\u0026#34;,\u0026#34;/app.jar\u0026#34;]  ADD target/spring-boot-monitoring-demo-0.0.1-SNAPSHOT.jar app.jar\nDemo application을 Docker container에 추가\n jmx-exporter/jmx_prometheus_javaagent-0.3.1.jar\nPrometheus JMX exporter agent를 Docker container에 추가\n jmx-exporter/tomcat.yaml tomcat.yaml\nJMX exporter Tomcat configuration file을 Docker container에 추가\n ENTRYPOINT ...\nDemo App 및 JMX exporter agent 실행\n  Docker build\n$ docker build -t spring-boot-monitoring-demo . $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE spring-boot-monitoring-demo latest 1444f50dcadc 9 seconds ago 117M Application 실행\n$ docker run -p 8080:8080 -p 8090:8090 spring-boot-monitoring-demo Web browser에서 localhost:8080에 접속\nHello World! 메세지가 출력됨\n Web browser에서 localhost:8090에 접속\n아래와 같이 JVM metrics 출력됨\n# HELP jvm_gc_collection_seconds Time spent in a given JVM garbage collector in seconds. # TYPE jvm_gc_collection_seconds summary jvm_gc_collection_seconds_count{gc=\u0026#34;PS Scavenge\u0026#34;,} 15.0 jvm_gc_collection_seconds_sum{gc=\u0026#34;PS Scavenge\u0026#34;,} 0.18 jvm_gc_collection_seconds_count{gc=\u0026#34;PS MarkSweep\u0026#34;,} 2.0 jvm_gc_collection_seconds_sum{gc=\u0026#34;PS MarkSweep\u0026#34;,} 0.262 ... Docker image를 Docker registry에 push\n$ docker tag spring-boot-monitoring-demo:[VERSION] [REPOSITORY_ADDRESS]/spring-boot-monitoring-demo:[VERSION] $ docker push [REPOSITORY_ADDRESS]/spring-boot-monitoring-demo:[VERSION]  Demo Application 배포  Demo Application Deployment yaml에 discovery, docker image 정보 수정(default 그대로 사용해도 무관)\n$ vi kubernetes/deployment.yaml ... spec: ... template: metadata: ... annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/port: \u0026#34;8090\u0026#34; prometheus.io/path: /metrics spec: containers: - name: spring-boot-monitoring-demo image: dtlabs/spring-boot-monitoring-demo:latest  spec.template.metadata.annotations.prometheus.io/scrape\nDiscorvery 대상 여부\n spec.template.metadata.annotations.prometheus.io/port\nDiscovery target port\n spec.template.metadata.annotations.prometheus.io/path\nDiscovery target path\n spec.template.spec.containers.image\nDocker image path\n  Demo Application 배포\n$ kubectl apply -f kubernetes/deployment.yaml $ kubectl get po NAME READY STATUS RESTARTS AGE spring-boot-monitoring-demo-5795695496-ljfkw 1/1 Running 0 1m spring-boot-monitoring-demo-5795695496-mqzrr 1/1 Running 0 1m Demo Application에 8080 Port Forwarding 설정\nWeb browser에서 localhost:8080으로 접속.\nHello World! 메세지가 출력됨.\n$ kubectl port-forward spring-boot-monitoring-demo-5795695496-ljfkw 8080:8080 Forwarding from 127.0.0.1:8080 -\u0026gt; 8080 Demo Application에 8090 Port Forwarding 설정\nWeb browser에서 localhost:8090으로 접속.\n아래와 같이 JMX metrics가 출력됨.\n# HELP jvm_gc_collection_seconds Time spent in a given JVM garbage collector in seconds. # TYPE jvm_gc_collection_seconds summary jvm_gc_collection_seconds_count{gc=\u0026#34;PS Scavenge\u0026#34;,} 15.0 jvm_gc_collection_seconds_sum{gc=\u0026#34;PS Scavenge\u0026#34;,} 0.18 jvm_gc_collection_seconds_count{gc=\u0026#34;PS MarkSweep\u0026#34;,} 2.0 jvm_gc_collection_seconds_sum{gc=\u0026#34;PS MarkSweep\u0026#34;,} 0.262 ...  Monitoring Dashboard 구성 및 활용 JMX Dashboard 구성  Grafana Dashboard 접속 \u0026gt; Left 메뉴 \u0026gt; + 버튼 선택 \u0026gt; Import 메뉴 선택\n Upload .json File 버튼 선택\n다운로드 받은 spring-boot-monitoring-demo 프로젝트 \u0026gt; jmx-exporter \u0026gt; jmx-exporter-tomcat-grafana-dashboard.json 파일 import\n Options \u0026gt; prometheus combo box \u0026gt; 원하는 data source 선택 \u0026gt; Import 버튼 선택\n 참고 : data source 설정은 prometheus로 부터 data를 검색하기 위한 용도  Heap Memory, Threads, Class Loading, Open File, GC 등에 대한 metrics을 그래프 형태로 조회\n  Pod(Application) Dashboard 활용 Kubernets cluster 용도로 Prometheus 구성 시 동적으로 생성되는 Pod를 discovery 해서 자동으로 metric을 수집합니다. 이 문서에서는 Kubernetes cluster를 위한 Prometheus와 Grafana가 설치되어 있다는 전제하에 작성했습니다. Prometheus와 Grafana 설치에 대한 가이드는 추후 작성 예정입니다.\n Dashboard List \u0026gt; Container Dashboards \u0026gt; Kubernetes: POD Overview 선택\n Pod(Application)의 cpu, memory, network metrics을 그래프 형태로 조회\n  "
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/logging-in-kubernetes/",
        "title": "Logging in kubernetes",
        "tags": ["logging", "kubernetes", "fluent", "cncf", "elasticsearch", "kibana", "로깅"],
        "description": "Cloud Native 환경에서 Logging Architecture 소개 및 실습",
        "content": " Cloud Native 환경에서 Logging Architecture의 변화 Legacy  고사양의 서버에 Application을 크게 운영 Log를 Application이 실행 중인 서버 내부에 저장 개발자/운영자는 서버 내부에 접속해 Log를 확인  Cloud Native  Application을 작게 운영하고 필요할 때 마다 확장 다중 인스턴스의 로그를 효율적으로 검색하기 위해 외부 Log 시스템에 저장 개발자/운영자는 서버에 직접 접속하지 않고 외부 Log Backend에서 로그 확인  Cloud Native Logging Architecture Overview DaemonSet Pattern  App Console Log가 각 Node의 Storage에 자동 저장 각 Node의 Agent가 Log를 Aggregator로 전달 Log data를 전/후 처리 후 Backend로 전달  Sidecar Pattern  App Log를 Pod의 Storage에 파일로 저장(Log4j 등 사용) Pod의 Agent가 Log data를 전/후 처리 후 Backend로 전달  DaemonSet Pattern 상세 Architecture  App Console Log가 각 Node의 Storage에 자동 저장 Fluentbit가 각 Node의 Log 수집해 FluentD로 전달 FluentD가 수집한 Log를 전/후 처리 후 ElasticSearch로 전달 Log raw data를 S3와 같은 저장소에 동시 전달 가능(Log Data 백업 활용) Kibana를 통해 ES의 Log data 검색/분석 및 시각화  Sidecar Pattern 상세 Architecture  App Log를 Pod의 Storage에 파일로 저장(Log4j 등 사용) Fluentbit가 저장된 Log를 전/후 처리 후 ElasticSearch로 전달. Log raw data를 S3와 같은 저장소에 동시 전달 가능(Log Data 백업 활용) Kibana를 통해 ES의 Log data 검색/분석 및 시각화  Logging 실습 Sidecar Pattern Demo Application Download 및 Build  Demo Application Download\n$ git clone https://github.com/cloudz-labs/spring-boot-logging-demo $ cd spring-boot-logging-demo Maven build\n$ mvn clean package Docker build\n$ docker build -t logging-demo . $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE logging-demo latest 1444f50dcadc 9 seconds ago 117M Application 실행 및 Web Browser에서 localhost:8080 접속.\n$ docker run -p 8080:8080 logging-demo Docker Container에 접속하여 log file 확인\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 79e28edfbb12 logging-demo \u0026#34;java -Djava.securit…\u0026#34; 4 minutes ago Up 4 minutes 0.0.0.0:8080-\u0026gt;8080/tcp gifted_ride $ docker exec -it 79e28edfbb12 sh / # tail -f logs/spring-boot-logging.log .... 2018-07-16 17:31:40.153 [http-nio-8080-exec-1] INFO o.s.web.servlet.DispatcherServlet - FrameworkServlet \u0026#39;dispatcherServlet\u0026#39;: initialization completed in 86 ms 2018-07-16 17:31:40.315 [http-nio-8080-exec-1] INFO com.example.demo.HomeRestController - logback example - info level 2018-07-16 17:31:40.316 [http-nio-8080-exec-1] WARN com.example.demo.HomeRestController - logback example - warn level 2018-07-16 17:31:40.318 [http-nio-8080-exec-1] ERROR com.example.demo.HomeRestController - logback example - error level  Deom Application 구조 Logback 설정해 /logs directory 하위에 log message를 file로 저장\n Log message 출력하는 HomeRestController\npackage com.example.demo; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.RestController; @RestController public class HomeRestController { private final Logger logger = LoggerFactory.getLogger(this.getClass()); @RequestMapping(value = \u0026#34;/\u0026#34;, method = RequestMethod.GET) public String getHome() { logger.debug(\u0026#34;logback example - debug level\u0026#34;); logger.info(\u0026#34;logback example - info level\u0026#34;); logger.warn(\u0026#34;logback example - warn level\u0026#34;); logger.error(\u0026#34;logback example - error level\u0026#34;); return \u0026#34;Hello World!\u0026#34;; } } Logback 설정\n\u0026lt;logback-spring.xml\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- Send debug messages to System.out --\u0026gt; \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;!-- By default, encoders are assigned the type ch.qos.logback.classic.encoder.PatternLayoutEncoder --\u0026gt; \u0026lt;encoder\u0026gt; \u0026lt;pattern\u0026gt;%d{HH:mm:ss.SSS} [%thread] %-5level %logger{5} - %msg%n\u0026lt;/pattern\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;!-- Send debug message to file --\u0026gt; \u0026lt;appender name=\u0026#34;FILE\u0026#34; class=\u0026#34;ch.qos.logback.core.rolling.RollingFileAppender\u0026#34;\u0026gt; \u0026lt;file\u0026gt;logs/spring-boot-logging.log\u0026lt;/file\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.classic.encoder.PatternLayoutEncoder\u0026#34;\u0026gt; \u0026lt;pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n\u0026lt;/pattern\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;rollingPolicy class=\u0026#34;ch.qos.logback.core.rolling.TimeBasedRollingPolicy\u0026#34;\u0026gt; \u0026lt;fileNamePattern\u0026gt;logs/spring-boot-logging.%d{yyyy-MM-dd}_%i.log\u0026lt;/fileNamePattern\u0026gt; \u0026lt;!-- each file should be at most 10MB, keep 30 days worth of history --\u0026gt; \u0026lt;maxHistory\u0026gt;30\u0026lt;/maxHistory\u0026gt; \u0026lt;timeBasedFileNamingAndTriggeringPolicy class=\u0026#34;ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\u0026#34;\u0026gt; \u0026lt;maxFileSize\u0026gt;10MB\u0026lt;/maxFileSize\u0026gt; \u0026lt;/timeBasedFileNamingAndTriggeringPolicy\u0026gt; \u0026lt;/rollingPolicy\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;STDOUT\u0026#34; /\u0026gt; \u0026lt;appender-ref ref=\u0026#34;FILE\u0026#34; /\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;/configuration\u0026gt;  Demo Application 및 Fluentbit 배포  Fluentbit Configmap에 Elasticsearch의 정보 설정\n$ vi configmap.yaml ... - name: fluent-bit env: - name: FLUENT_ELASTICSEARCH_HOST value: \u0026#34;elasticsearch.kube-system.svc.cluster.local\u0026#34; - name: FLUENT_ELASTICSEARCH_PORT value: \u0026#34;9200\u0026#34;  FLUENT_ELASTICSEARCH_HOST의 value : Elasticsearch의 endpoint. IP or Domain or Service FLUENT_ELASTICSEARCH_PORT의 value : Elasticsearch의 port  Fluentbit Configmap 배포\n$ kubectl apply -f configmap.yaml $ kubectl get cm NAME DATA AGE fluent-bit-config 4 1m Demo Application Deployment yaml의 docker image 정보 수정(default 그대로 사용해도 무관)\n$ vi deployment.yaml ... spec: containers: - name: logging-demo image: dtlabs/spring-boot-logging-demo:latest Demo Application 및 Fluentbit Kubernetes Deployment 배포\n$ kubectl apply -f deployment.yaml $ kubectl get deploy NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE logging-demo 1 1 1 1 1m $ kubectl get po NAME READY STATUS RESTARTS AGE logging-demo-6c659fb7cb-zndhs 2/2 Running 0 1m Demo Application에 Port Forwarding 설정 \u0026gt; Web browser에서 localhost:8080으로 접속\n$ kubectl port-forward logging-demo-6c659fb7cb-zndhs 8080:8080 Forwarding from 127.0.0.1:8080 -\u0026gt; 8080 Demo Application 및 Fluentbit Container에 접속해 Log file 확인\n$ kubectl exec -it logging-demo-6c659fb7cb-zndhs -c logging-demo sh / # tail -f /logs/spring-boot-logging.log 2018-07-17 07:47:59.751 [http-nio-8080-exec-5] INFO com.example.demo.HomeRestController - logback example - info level $ kubectl exec -it logging-demo-6c659fb7cb-zndhs -c fluent-bit sh # tail -f /var/logs/spring-boot-logging.log 2018-07-17 07:47:59.751 [http-nio-8080-exec-5] INFO com.example.demo.HomeRestController - logback example - info level  Kibana Index Pattern 설정 및 Log message 검색  Kibana 접속 \u0026gt; Left 메뉴 \u0026gt; Management 메뉴 선택 \u0026gt; Kibana Index Patterns 선택   Index name or pattern input box에 fluentbit 입력 후 Time Filter field name combo box에 @timestamp 선택   정상적으로 생성 시 아래와 같은 모습   Left 메뉴 \u0026gt; Discover 메뉴 선택 \u0026gt; Demo Application의 Log message 검색 됨  지금까지 Kubernetes 환경에서 Logging Architecture에 대해 살펴봤습니다.\nLog message 전/후처리에 대한 상세한 내용은 향후 추가로 작성예정입니다.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/hugo/hugo/",
        "title": "Hugo를 활용한 기술 블로그 구축기",
        "tags": ["static generator", "hugo", "go", "github pages"],
        "description": "",
        "content": " 현재 보고계신 블로그는 Hugo + Github Pages로 운영되고 있습니다. 이번 글에서는 Hugo를 활용한 Static Site 구축에 대해서 얘기해볼까 합니다.\nCMS에서 Static Generator까지 많은 사람들이 블로그를 구축한다고 했을 때, 가장 많이 떠올리는 것은 다음과 같이 2가지 형태일 것입니다.\n Naver Blog, Tistroy, Blogger 등등 Wordpress, Drupal, Joomla  첫번째 형태인 Naver나 Tistroy의 포털 사이트에서 제공하는 가입형 블로그 서비스는, 단순하게 가입만 해도 사용할 수 있기 때문에, 도메인이나 호스팅 등등의 것들을 신경쓰지 않고 바로 글을 작성할 수 있죠. 그러나, 운영자가 블로그를 커스터마이징할 수 있는 영역이 매우 적습니다.\n그래서 처음에 기술 블로그를 운영하려고 했을 때 선택한 형태는, CMS를 사용하는 것이었습니다. CMS는 \u0026lsquo;Contents Management System\u0026rsquo;의 약자로 Blog를 직접 설치해서 사용한다고 이해를 하면 쉬울 것 같습니다. 저희가 생각했던 초기 컨셉에 맞게 블로그를 구성하고 싶었고, 다양한 기능들을 블로그에 적용하고 싶었습니다. 최종적으로 저희는 Wordpress를 선택했었습니다.\nCMS 대장 Wordpress Wordpress는 PHP로 이루어진 CMS의 최강자입니다. 수천가지의 테마와 플러그인을 유/무료 제공하고 있으며, 커뮤니티도 활발하게 이루어져 있습니다. Linkedin, ebay, mozilla등의 기업들도 Wordpress를 이용하여 블로그를 운영하고 있습니다.\nWordpress는 오픈소스 버전과 상용 버전이 존재합니다. 오픈소스는 직접 Wordpress를 설치하여 구성해야합니다. 서버, 도메인, 호스팅까지 모든 것들을 사용자가 직접 설정하고 운영해야합니다. 반대로 상용버전은 가입만 하면 별도의 서버를 운영할 필요없습니다. 단, 요금제에 따라 제공되는 기능이 다릅니다. 그래서 저희는 상용 버전의 Wordpress를 사용했습니다. 별도의 서버 및 호스팅에 대한 부분을 Wordpress에 맡기고, 저희는 블로그를 간단히 구성하고 글만 잘 작성하여 관리하면 될 줄 알았습니다\u0026hellip;\n글쓰기가 스트레스가 되다 Wordpress를 하면서, 가장 마음에 안들었던 점은 글쓰는 것이 너무 불편하고, 나타나는 결과물이 너무 만족스럽지 못했습니다. 현재, 저희는 업무 공유 Tool로 Atlassian의 Confluence를 사용하고 있습니다. Confluence에 온갖 내용들을 Draft형식으로 남기고, 그것을 직접 공유해서 사용했습니다. 별도로 글을 이쁘게 꾸밀 필요가 거의 없이, 깔끔하게 보여졌습니다. 바로, Wordpress에서 그런 것들을 기대했는데 전혀 아니었습니다. 글을 쓴 첫느낌은\u0026hellip; Wordpad에 글을 적는 느낌을 받았고, 결국은 다시 깔끔하고 꾸미는 시간이 필요했습니다.\n두번째는 너무 많고 복잡했습니다. 기본으로 제공되는 에디터의 기능이 너무 부족해서 플러그인을 설치하고, 표를 이쁘게 삽입하기 위해서 플러그인을 설치하고, 소셜을 연동하기 위해서 플러그인을 설치하고\u0026hellip; 필요한 플러그인만 10개 이상이 넘어갔고, 주기적으로 업데이트도 해야했습니다. 그리고 제공되는 테마 중 마음에 드는 것이 없어서, 새로 적용하려고 해도 PHP를 어느정도 공부하고 Wordpress만의 구조를 이해하고 넣어야 했죠.\n이 2가지가 저희에게는 너무 큰 부담으로 다가왔습니다. 그래서 과감히 Wordpress를 버렸습니다.\nSolutions 저희는 기술 블로그를 운영함에 있어서 가장 중요하게 고려되어야 할 점들에 대해서 다시 정리하고, 한가지씩 답을 도출했습니다.\n 글을 작성하고 꾸미는 것이 쉬울 것 테마를 자유롭게 바꿀 수 있을 것 운영이 쉬울 것  3가지 항목을 모두 만족시킬 수 있는 방안에 대해서 고민한 결과, 최종적으로 도달한 결론은\nGitHub Pages + Static Generator(정적 웹사이트 생성기)\n 로 기술 블로그를 구성하는 방법이었습니다.\nGitHub Pages GitHub pages란, GitHub Reposiroty의 내용들을 Web Page로 볼 수 있게 서비스해주는 기능입니다. GitHub Pages는 Static Contents만을 서비스할 수 있기 때문에, OpenSource Project들의 공식 Documents나 소개 서비스로 많이 사용됩니다. 사용법은 매우 간단합니다. [REPOSITORY_NAME].github.io으로 Repository를 Public으로 생성한 뒤 HTML을 해당 Reposiroty로 Push하면 완료입니다. Static Contents만 지정된 Repository에 Push 된다면, GitHub 서버에 무료로 호스팅할 수 있습니다(단, URL에 github.io가 붙습니다).\nStatic Generator 그럼 블로그 글은 HTML로 작성을 해야할까요? 아닙니다. 바로 Static Contents를 생성해 주는 것이 Static Generator의 역할이며, GitHub Pages와 결합되어 사용했을 때, 빛을 발합니다!!! 우리는 그저 텍스트(마크다운) 형식으로 글을 작성하고, 디렉토리 별로 글들을 정리하면 됩니다. 그러면 Static Generator가 자동으로 정리된 글들과 HTML 템플릿을 함께 합쳐서, 완전한 정적 웹사이트로 만들어 줍니다.\n가장 대표적인 Static Generator는 Jekyll이 있습니다. Jekyll은 Ruby로 Github의 창업자인 Tom Preston-Werner가 만들었으며, 현재 GitHub Pages의 내부 엔진으로 사용되고 있습니다. Docker Documents가 Jekyll로 되어 있습니다.\nHugo Static Generator는 Jekyll외에도 Hugo, Gatsby, Hexo 등등 많이 있습니다. 그 중에서도 저희 블로그는 Go언어로 만들어진 Hugo를 사용하였습니다. Jekyll을 사용할 경우, 별도의 Build 과정 없이 Repository에 Push만으로 작성한 글들이 알아서 Publishing됩니다. 하지만, 글이 많아질 수록 Jekyll의 빌드 성능은 현저하게 저하됩니다. 하지만, Hugo는 Build 과정이 있어도 성능저하 없이, 빠르게 글을 Publishing할 수 있습니다. Go나 기타 종속성 없이, Hugo CLI를 통해서 쉽게 블로그 및 글을 생성할 수 있습니다. 그래서, Hugo로 블로그를 만들게 되었습니다.\n블로그 구축 자, 서론이 길었습니다. 지금부터는 Hugo를 사용하여 블로그를 구성해보겠습니다.\n본 내용은 Mac을 기준으로 작성하였습니다. 타 OS는 Install Hugo를 참고하여 설치하시기 바랍니다.\n Hugo 설치 Homebrew로 hugo를 설치합니다.\n$ brew install hugo ... 설치 진행 ... $ hugo version Hugo Static Site Generator v0.34 darwin/amd64 BuildDate: Static Site 생성하기 hugo new site [PATH]로 Static Site를 생성합니다.\n$ hugo new site tech-blog Congratulations! Your new Hugo site is created in /tech-blog. Just a few more steps and you\u0026#39;re ready to go: 1. Download a theme into the same-named folder. Choose a theme from https://themes.gohugo.io/, or create your own with the \u0026#34;hugo new theme \u0026lt;THEMENAME\u0026gt;\u0026#34; command. 2. Perhaps you want to add some content. You can add single files with \u0026#34;hugo new \u0026lt;SECTIONNAME\u0026gt;/\u0026lt;FILENAME\u0026gt;.\u0026lt;FORMAT\u0026gt;\u0026#34;. 3. Start the built-in live server via \u0026#34;hugo server\u0026#34;. Visit https://gohugo.io/ for quickstart guide and full documentation. Hugo로 생성된 Static Site의 구조는 다음과 같습니다.\n/tech-blog $ tree . ├── archetypes │ └── default.md ├── config.toml ├── content ├── data ├── layouts ├── static └── themes 각 항목에 대한 상세역할은 다음과 같습니다.\n   항목 내용 비고     archetypes 컨텐츠 기본 구조 정의 default.md 파일에서 마크다운 구조 설정   config.toml Hugo 블로그 전체 설정 파일    content/posts 블로그에 올라갈 마크다운 파일 위치    data 태그, 카테고리, 저자 등 기타 항목 정의    layouts, static 블로그의 템플릿 및 정적 리소스 위치 현재는 테마를 사용하기 때문에 사용안함   themes 사용할 Hugo 테마 위치    public 블로그 빌드 타켓 폴더 gh-pages 브랜치에 Push될 결과물    Theme 적용하기 갓 생성된 Static Site는 디렉토리로만 구성되어 있습니다. 따라서, Theme를 적용하여 화면이 보일 수 있도록 하겠습니다. Hugo도 Wordpress와 마찬가지로 다양한 Theme이 제공되고 있으며, 대부분 Open Source로 자유롭게 사용하실 수 있습니다. Theme에 대한 상세내용 Hugo Themes에서 확인할 수 있습니다.\n$ cd tech-blog /tech-blog $ git init; Initialized empty Git repository in /Volumes/1000jaeh/test/workspace/tech-blog/.git/ /tech-blog $ git submodule add https://github.com/budparr/gohugo-theme-ananke.git themes/ananke; Cloning into \u0026#39;/Volumes/1000jaeh/test/workspace/tech-blog/themes/ananke\u0026#39;... remote: Counting objects: 1104, done. remote: Compressing objects: 100% (17/17), done. remote: Total 1104 (delta 9), reused 9 (delta 4), pack-reused 1083 Receiving objects: 100% (1104/1104), 2.51 MiB | 1.85 MiB/s, done. Resolving deltas: 100% (584/584), done. Git으로 Pull 받은 Ananke Theme을 사용하기 위해서 config.toml 파일에 theme의 속성값을 ananke로 적용합니다.\nbaseURL = \u0026#34;http://example.org/\u0026#34; languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;My New Hugo Site\u0026#34; theme = \u0026#34;ananke\u0026#34; TOML이란 \u0026ldquo;Tom\u0026rsquo;s Obvious, Minimal Language.\u0026ldquo;의 약자로 Github의 창업자인 Tom Preston-Werner가 만든 새로운 설정 파일 형식이다.\n Posting hugo new [PATH]를 사용하여 새로 포스팅할 글을 /content/posts내에 Markdown형식으로 생성합니다.\n/tech-blog $ hugo new posts/my-first-post.md /Volumes/1000jaeh/test/workspace/tech-blog/content/posts/my-first-post.md created 생성된 Markdown은 다음과 같이 구성되어 있습니다.\n---title: \u0026#34;My First Post\u0026#34;date: 2018-06-26T13:35:54+09:00draft: true---Hello HUGO!!! 상단의 --- 영역에 작성된 부분은 생성된 Post에 대한 Metadata를 담고 있는 Front Matter부분이며, 하단 부분부터 실제 글이 작성되는 부분입니다. Front Matter는 hugo new로 새로운 Post가 생성될 때 자동으로 작성되며, Metadata에 대한 Template는 /archetypes/default.md에서 확인하고 수정할 수 있습니다.\nRun Hugo 구성된 Static Site를 hugo serve [FLAGS]로 실행시킵니다. Hugo의 Default Port는 1313입니다. -D 옵션은 draft상태의 글을 Build시킬지 확인하는 옵션입니다.\n/tech-blog $ hugo serve -D | EN +------------------+----+ Pages | 10 Paginator pages | 0 Non-page files | 0 Static files | 3 Processed images | 0 Aliases | 1 Sitemaps | 1 Cleaned | 0 Total in 49 ms Watching for changes in /Volumes/1000jaeh/test/workspace/tech-blog/{content,data,layouts,static,themes} Watching for config changes in /Volumes/1000jaeh/test/workspace/tech-blog/config.toml Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop http://localhost:1313에 접속하여, 정상적으로 Static Site가 구성되었는지, 작성한 글이 제대로 Posting 되었는지 확인합니다.\n 이번 글에서는 Static Generator인 Hugo로 간단하게 블로그를 구성해 봤습니다. 다음 글에서는 Travis-CI를 이용하여, Github에 자동으로 배포되고, 작성한 글이 Publishing되는지 확인해보겠습니다.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/service-mesh/",
        "title": "Service Mesh 란?",
        "tags": ["service mesh", "istio", "envoy", "linkerd", "spring cloud", "msa", "microservice architecture", "architecture", "runtime complexity", "서비스 메시"],
        "description": "우리는 Monolitic Architecture의 단점을 보완하고 새로운 이점을 얻기 위해 MicroService Architecture를 도입했습니다. 우리의 모든 문제들이 해결되었나요? 새로운 문제가 발생하지는 않았나요? 걱정하지 마세요. Service Mesh Architecture를 소개합니다.",
        "content": " 수년간 Enterprise IT환경은 급격하게 변하고 있습니다. 특히 Cloud로 대변되는 시스템 구축 환경의 변화에 따라 이를 잘 활용할 수 있는 다양한 Architecture들이 대두되고 있습니다.\nService Mesh Architecture는 MicroService Architecture와 더불어 최근 활발하게 언급되고 있습니다.\n이번 포스팅에서는 Service Mesh Architecture가 \u0026lsquo;무엇\u0026rsquo;인지, \u0026lsquo;왜\u0026rsquo; 활발하게 언급되고 있는지, Service Mesh Architecture \u0026lsquo;구현체\u0026rsquo; 간단 소개와 \u0026lsquo;장단점\u0026rsquo; 등을 알아보겠습니다.\nService Mesh 란? Service Mesh는 모티브와 정의, 구현체의 기능 등 다양한 관점에서 용어를 정의하고 있습니다.\n MicroService Architecture를 적용한 시스템의 내부 통신이 Mesh 네트워크의 형태를 띄는 것에 빗대어 Service Mesh로 명명되었습니다. Service Mesh 는 서비스 간 통신을 추상화하여 안전하고, 빠르고, 신뢰할 수 있게 만드는 전용 InfraStructure Layer입니다.\n추상화를 통해 복잡한 내부 네트워크를 제어하고, 추적하고, 내부 네트워크 관련 로직을 추가함으로써 안정성, 신뢰성, 탄력성, 표준화, 가시성, 보안성 등을 확보합니다. Service Mesh 는 URL 경로, 호스트 헤더, API 버전 또는 기타 응용 프로그램 수준 규칙을 기반으로하는 계층 7 네트워크 Layer 입니다.\nService Mesh 의 구현체인 경량화 Proxy를 통해 다양한 Routing Rules, circuit breaker 등 공통기능을 설정할 수 있습니다. 이는 서비스 간 통신에 연관된 기능 뿐만 아니라, 서비스의 배포 전략에도 도움을 줍니다.  왜 Service Mesh 를 적용하는가? Service Mesh 가 떠오르는 이유는 무엇일까요 ?\nMicroService Architecture는 Monolitic Architecture의 단점 극복과 Cloud 환경에서 시스템을 운영할 때의 이점을 극대화하기 위해 많이 사용되고 있습니다.\n이를 통해 많은 문제들이 해결되었지만, 또다른 문제점도 발생했습니다. 그것은 시스템의 런타임 복잡성 입니다.\n Buoyant 블로그 중\n전 세계적으로 가장 큰 마이크로 서비스 응용 프로그램 중 하나를 작업 한 결과, 마이크로 서비스는 마법의 확장이나 유연성, 보안 또는 신뢰성의 뿌리가 아니라는 점을 확신 할 수 있습니다. 모놀리식 제품보다 작동하기가 훨씬 어렵습니다.\n구성 관리, 로그 처리, strace, tcpdump 등 우리가 익숙하고 익숙한 도구는 마이크로 서비스에 적용 할 때 조잡하고 둔한 도구임을 입증합니다.\n한 번의 요청으로 수 백 개의 인스턴스가있는 수백 개의 서비스에 액세스 할 수있는 세상에서 tcpdump를 어디에서 실행할 수 있습니까? 어떤 로그를 읽습니까? 느린 경우 어떻게 그 이유를 알아낼 수 있습니까?\n내가 뭔가를 바꾸고 싶을 때, 어떻게 이러한 변화가 안전하다는 것을 확신합니까?\n MicroService Architecture를 도입시 복잡성이 증가하는 명백한 이유가 있습니다.\n서비스 \u0026amp; Instance 의 증가 시스템의 규모에 따라 다르지만 활발히 운영되는 MSA 시스템에는 수십개의 MicroService가 분리되어 있고, 운영 환경에 수백 ~ 수천개의 서비스 Instance가 동작하고 있습니다.\nPaaS, CaaS 등의 환경에서 각 서비스의 Instance는 스케일링되며 동적으로 뜨고 집니다. 이 수백 ~ 수천개의 Instance을 모니터링 하고 로깅을 처리하고 Instance를 관리해야 합니다.\n서비스 간 통신 Monolitic Architecture에서 프로세스나 쓰레드 간 메모리 공유 등 서비스 Instance 내부에서 처리하던 기능들이 서비스 간 통신을 통해 처리됩니다.\n보통 사람들은 네트워크를 레이턴시가 없고 무한대 대역폭에 항상 안정적인 idle 상태로 생각하지만, 현실은 그렇지 않습니다. 안정적이지 않은 내부 네트워크는 시스템의 신뢰성, 안정성을 보장할 수 없습니다.\n동적으로 수많은 Instance가 뜨고/지고, 서비스 간 통신이 유발하는 이런 복잡한 상황에서 내부 네트워크를 안정적으로 다루기 위해 새로운 기능(또는 요구사항, 관리 point)들이 필요합니다.\nService Mesh 기능 아래 내용은 MicroService Architecture가 적용된 시스템의 새로운 요구사항입니다. 또한, Service Mesh의 기능이기도 합니다. Service Mesh는 MicroService Architecture를 보완하고 있기 때문입니다.\n Service Discovery Load Balancing Dynamic Request Routing Circuit Breaking Retry and Timeout TLS Distributed Tracing metrics 수집 기타 등등  Service Mesh 구현 Service Mesh Architecture의 구현은 보통 서비스의 앞단에 경량화 프록시를 사이드카 패턴으로 배치하여 서비스 간 통신을 제어하는 방법으로 구현합니다.\n서비스 간 통신은 사이드카로 배치된 경량화 Proxy를 통해서 동작합니다. 이 경량화 Proxy에 Routing rules, retry, timeout 등을 설정하고 로직을 작성하여 공통 기능을 기본 어플리케이션에서 분리시킬 수 있습니다.\n 사이드카 패턴이란?\n사이드카 패턴은 클라우드 디자인 패턴의 일종입니다.\n기본 Application 외 필요한 추가 기능을 별도의 Application으로 구현하고 이를 동일한 프로세스 또는 컨테이너 내부에 배치하는 것입니다.\n동일한 프로세스 또는 컨테이너에 배치된 사이드카 Application은 저장 공간, 네트워크 등의 리소스를 공유하며 모니터링, 로깅, 프록시 등의 동작을 합니다.\n사이드카 패턴 사용은 몇가지 장점이 있습니다.\n 사이드카 Application은 기본 Application과 별도의 Application입니다.  기본 Application의 로직을 수정하지 않고도 추가 기능을 수행할 수 있습니다. 기본 Application을 polyglot 프로그래밍을 적용해 요구 사항에 최적화된 환경에서 개발을 진행할 수 있습니다.  사이드카 Application은 기본 Application과 리소스를 공유할 수 있습니다. 이를 통해 모니터링에 필요한 Metrics 수집, 프록시 동작 등을 수행할 수 있습니다.   현재 활발히 발전하고 있는 Service Mesh 구현체를 소개합니다.\n istio : Google, IBM, Lyft가 함께 기여하고 있는 오픈소스 Service Mesh 구현체입니다. kubernetes를 기본으로 지원합니다. Control Plane - Data Plane 구조로 동작합니다. Envoy를 기본 Proxy로 사용하지만 nginx나 linkerd 등으로 대체할 수 있다고 합니다. (istio 소개글) linkerd : Buoyant에서 기여하고 있는 오픈소스 Service Mesh 구현체입니다. Twitter의 Finagle을 운영하던 인물들이 주축이라고 합니다. Local, DC/OS, kubernetes, docker, AWS ECS 등 다양한 환경에 Service Mesh를 적용할 수 있습니다. Host(or Node) 당 linkerd 하나를 배포해서 동작하는 것을 기본으로 소개합니다. 이는 \u0026lsquo;전형적인 사이드카 패턴으로 배포하는 것\u0026rsquo;이 리소스를 많이 사용한다는 문제점에서 시작해 나름의 해법을 찾은 것으로 볼 수 있습니다. Buoyant에서 운영중인 blog를 참조하시면 다양한 사례를 기반으로 하는 컨텐츠를 확인할 수 있습니다. conduit : linkerd를 운영하고 있는 Buoyant에서 기여하고 있는 오픈소스 Service Mesh 구현체입니다. Control Plane - Data Plane 구조로 동작합니다. kubernetes에 최적화한다는 점에서 linkerd와 차이가 있습니다. 신규 프로젝트로 미성숙 단계로 보이지만 linkerd를 잘 운영하고 있다는 점에서 관심있게 보고 있습니다. istio 와 비교해서 확인하시면 흥미로울 것 같습니다.\n  SOA - ESB와 비교 분산 처리 환경에서 기존과는 다른 문제가 발생하고, 새로 발생한 문제를 해결 하기 위한 다양한 해법을 제시하는 것. 어디서 많이 들어본 내용입니다.\nService Oriented Architecture(SOA)는 비즈니스 로직에 집중하고 도메인 중심으로 서비스를 분화하는 등등, MicroService Architecture와 유사한 점이 많습니다. SOA는 분산처리 환경에서 발생하는 문제를 해결하기 위해 Enterprise Service Bus(ESB)를 도입했습니다. 분산 처리 환경에서의 문제를 기본 Application의 외부에서 해결하려고 한 점이 ESB와 Service Mesh의 유사한 점이라고 할 수 있습니다. 하지만 ESB는 중앙집중형으로 공통 기능의 비대화에 따른 문제를 제대로 해소하지 못했습니다. Service Mesh는 각 MicroService 앞단에 경량화 Proxy를 배치하는 방법으로 공통 기능 처리를 분산화했다는 점이 가장 큰 차이로 보입니다.\nCloud Native 문제 해결 History 지금까지 Service Mesh에 대해 간략히 알아봤습니다. 위의 내용에 Cloud Native의 문제 해결 History를 추가로 살펴봄으로써 Service Mesh의 이해도를 높여보겠습니다. (http://philcalcado.com/2017/08/03/pattern_service_mesh.html 참고)\n Application level에서 해결  Application에 내부 네트워크 관련 로직을 구현해서 해결 Application을 구현할 때마다 소스를 붙여넣어야하는 비용이 발생한다.  공통 기능을 라이브러리화\n 공통적으로 적용되는 기능을 라이브러리화해서 Application을 구현할 때마다 Import하는 방식이다. (ex. spring cloud) 라이브러리화는 Application의 언어,런타임에 종속성이 생기고, 각 Application마다 라이브러리를 Import하는 비용이 발생하는 등의 단점이 있다. 비즈니스 로직이 아닌 해당 라이브러리를 관리하고 고도화하기 위한 별도의 조직을 운영해야한다. 라이브러리를 가져다 쓰는 경우, 주도권이 우리에게 없다. 라이브러리 버전이 올라가면 Application을 수정하고 테스트 해야하는 부담을 가져야 한다. 비즈니스 로직에 집중하기 어렵게 해당 라이브러리의 기능 및 비중이 점점 비대해진다는 의미에서 fat-라이브러리라고 언급되기도 한다.  사이드카 패턴으로 경량화 Proxy와 연계\n 비즈니스 로직과 내부 네트워크 관련 로직을 분리한다. 하지만 MicroService Architecture에서 필요한 내부 네트워크 로직을 OS단의 Network Layer에 추가하는 것은 현실적으로 쉽지 않다. 해결책으로 별도의 사이드카 Application을 구현하고 기본 Application과 연계하여 서비스 간 통신을 수행한다. 이는 시스템에 공통 기능 추가를 자유롭게 한다. Service Mesh Architecture의 시작으로 data plane으로 언급되기도 한다.  분산된 기능의 관리를 중앙집중화\n kubernetes, Mesos 와 같은 정교한 런타임을 활용하면서 별도의 격리되어 동작하는 독립적인 Proxy들을 중앙에서 관리하기 시작한다. (개별 관리는 번거로움) control plane으로 언급된다. 개별 Proxy의 Routing Rules 관리 및 동적 동기화, metrics 수집 등의 기능을 합니다.   Service Mesh 장단점  장점  기능을 어플리케이션 외부에 구현하며 재사용 가능하다. MicroService Architecture를 도입하면서 발생한 런타임 복잡성 이슈를 해결한다. 어플리케이션 개발시 언어와 미들웨어 등에 종속성을 제거한다.  단점  시스템의 런타임 인스턴스 수가 크게 증가한다. (최소 2배수) 서비스 간 통신에 네트워크 레이어가 추가된다. 신기술이다. 구현체가 Release 될 때까지 시간이 필요하다.   Conclusion Service Mesh Architecture는 MicroService Architecture가 유발하는 새로운 문제점을 보완하는 개념입니다. 경량화 Proxy를 통해 서비스 간 통신에 Routing Rules 설정, Circuit Breaker 등의 공통 기능을 적용합니다. 새롭게 떠오르는 키워드지만 그 속을 자세히 들여다보면 분산 처리 환경에서 네트워크를 안정적으로 다루기 위한 기술들의 발전 양상을 따르고 있습니다. Service Mesh를 도입해 MicroService Architecture가 적용된 시스템의 안정성, 가시성, 신뢰성, 표준화 등을 달성할 수 있습니다.\n물론 만능은 아닙니다. 서비스 간 통신에 네트워크 레이어가 추가되므로 오버헤드가 발생하기도 하고, 사이드카 패턴으로 배포된 경량화 Proxy도 시스템의 리소스를 소모하는 Application입니다. 또한, 아직 오래되지 않은 기술로 Spring Cloud Netflix OSS 등의 유명하고 안정된 라이브러리와 비교해 기능이 충분치 않을 수도 있습니다. 하지만 Application의 수정없이 공통 기능을 추가할 수 있어 적용 부담이 적고, 시스템에 Polyglot 프로그래밍을 보장할 수 있는 등의 장점이 많이 있습니다.\nMonolitic Architecture의 단점에 질려서, 혹은 시스템을 Cloud 환경에 구축할 때 얻을 수 있는 이점과 MicroService Architecture의 장점에 끌려서 MicroService Architecture의 도입을 결정했다면 그 다음 닥칠 문제에 대비해 Service Mesh Architecture 및 istio, linkerd, conduit 등을 유심히 지켜볼 필요가 있습니다. 특히 kubernetes 등 Service Mesh Architecture 구현체를 적용하기 쉬운 환경을 사용중이라면 바로 테스트해보는 것을 추천드립니다.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/docker-user-defined-network/docker-user-defined-network-with-spring-cloud/",
        "title": "[Docker-User Defined Network 활용(3/3)] Docker User Defined Bridge Network with Spring Cloud",
        "tags": ["docker", "user defined network", "spring boot", "spring cloud"],
        "description": "Docker User Defined Bridge Network와 Spring Cloud를 활용해 최소 단위의 MSA가 적용된 system을 구성합니다.",
        "content": " 지난 포스팅에서 Docker의 네트워크 기능과 연관된 factors를 경우의 수에 따라 검증을 했습니다.\nDocker toolbox를 사용하는 환경(win7)에서 테스트 결과, 아래의 경우에 서비스 간 호출이 성공했습니다.\n 컨테이너의 Port를 노출하고 {docker-machine ip}:{외부 노출 Port}으로 접속한 경우 컨테이너를 user defined network에 연결하고 {컨테이너 명}:{컨테이너 내부 port}로 접근한 경우 컨테이너를 user defined network에 연결하고 {user defined network ip}:{컨테이너 내부 port}로 접근한 경우  이 중 2번. 컨테이너를 user defined network에 연결하고 {컨테이너 명}:{컨테이너 내부 port}로 접근하는 방법으로 Docker에서 최소 단위의 MSA가 적용된 system을 구성해보겠습니다.\n구성할 시스템의 명칭은 cloud-movie 입니다.\nAbout cloud-movie system cloud-movie system Diagram \n 해당 Diagram에서 User Defined Network는 frontend와 backend로 구분지어 구성했습니다. API G/W는 frontend와 backend에 동시에 network binding을 해서 frontend로 부터 API 호출을 가능하게 설정했습니다. 또한, port 노출없이 private network에서 frontend와 backend 사이 proxy, routing을 수행하도록 구성했습니다. cloud-movie-front와 cloud-movie-hystrix-dashboard는 브라우저를 통해 접속이 필요한 경우입니다. frontend용 User Defined Network와 연결된 API G/W를 통해 API 서버에 접근할 수 있도록 구성했고 외부로 port를 노출했습니다. API 서버는 API G/W를 통한 접속만을 허용하고 외부로 port 노출을 시키지 않는 private network 구성했습니다. 이러한 구성으로 User가 frontend 네트워크 영역의 Application을 통하지 않고 API 서버에 직접 접근하는 것을 차단하고 허용된 기능만 수행하는 방식으로 설계할 수 있습니다. config server는 모든 Application과 연계를 위해 stand alone의 형태로 구현했습니다. config server 또한 User Defined Network를 설정하고, 관련있는 Application을 동일한 network에 binding해서 통신을 수행하도록 구성할 수 있습니다.  cloud-movie system Details 구현한 Application은 모두 Spring Boot 1.5.x 기반입니다. 테스트 목적으로 필요한 기능을 구현했습니다.\nApplication 구동시 config server와 연계하기 위한 설정이 각 Application의 bootstrap.yml에 공통 적용되어 있습니다.\nspring: profiles: default cloud: config: uri: http://localhost:8888 name: cloud-movie-apigateway --- spring: profiles: docker cloud: config: uri: http://192.168.99.100:8888 name: cloud-movie-apigateway \n아래는 개별 Application에 대한 간략한 설명과 network 관련 설정입니다.\ncloud-movie  Spring Boot로 구현한 backend 서비스입니다. cloud-movie-db와 연계해 CRUD 기능을 수행합니다. 테스트를 위해 application 구동시 샘플 데이터 초기화를 수행합니다. cloud-movie-db와 연계 설정  cloud-movie-db의 컨테이너 명을 사용해 Host 정보를 사용했습니다.   spring: datasource: initialize: true platform: mariadb url: jdbc:mariadb://cloud-movie-db:3306/db_example username: root password: ThePassword  해당 설정에 포함된 민감정보는 실제 사용시 security 적용이 필요한 항목입니다.\n cloud-movie-db  Docker Hub에 배포된 mariadb 공식 이미지의 10.2.13 버전을 사용했습니다. docker run 수행시 기본 설정 및 인코딩 방식을 설정합니다. 해당 테스트에서 샘플 데이터는 cloud-movie 구동시 초기화하도록 설정했습니다.   실제 Docker 에서 DB 구동시 volume을 설정해 데이터의 연속성을 보장합니다.\n cloud-movie-front  Spring Boot로 구현한 frontend 서비스입니다. thymeleaf 템플릿 엔진 사용해 UI를 구성했습니다. 외부에서 접근이 필요하므로 port를 노출합니다. cloud-movie-apigateway와 연계 설정  cloud-movie-apigateway의 컨테이너 명을 사용해 Host 정보를 사용했습니다.   rest: address: http://cloud-movie-apigateway:9999/movie-service cloud-movie-apigateway  Netflix OSS의 Zuul을 사용해 apigateway 기능을 구현합니다. /movie-service/** 패턴으로 접근시 cloud-movie로 라우팅을 합니다. zuul routing 설정  cloud-movie의 컨테이너 명을 사용해 Host 정보를 사용했습니다.   zuul: ignoredService: \u0026#34;*\u0026#34; routes: movies: path: /movie-service/** url: http://cloud-movie:8080 serviceId: cloud-movie cloud-movie-config-server  Spring Cloud Config를 사용해 config server 기능을 구현했습니다. Github의 Repository에 업로드 한 Application들의 설정을 일원화해 관리합니다. 해당 테스트에서 config server는 stand alone으로 동작하는 상황을 가정하여 {ip:port}로 접근하도록 구현했습니다. 외부에서 접근이 필요하므로 port를 노출합니다.  cloud-movie-hystrix-dashboard  Netflix OSS의 Hystrix 사용해 Circuit Breaker 기능을 구현했습니다. hystrix command의 적용은 cloud-movie-front, cloud-movie 두가지 Application에 적용되어 있습니다. 해당 Application은 hystrix dashboard 기능을 별도 Application으로 구현한 것입니다. 외부에서 접근이 필요하므로 port를 노출합니다. frontend 영역의 Application 모니터링시 {컨테이너 명}:{컨테이너 port}/hystrix.stream으로 접근합니다.   backend 영역의 Application 모니터링시 cloud-movie-apigateway:9999/{backend 라우팅 패턴}/hystrix.stream으로 접근합니다.  cloud-movie system Deploy 각 어플리케이션을 Dockerfile을 통해 이미지화 합니다.\n해당 테스트에서는 docker toolbox로 설치된 docker-machine상의 local 환경에 이미지를 저장해두고 컨테이너 구동시 추가적인 image pull 없이 바로 사용합니다.\n별도의 이미지 저장소를 사용하는 경우 docker build, docker tag, docker push의 순서로 수행하시면 됩니다.\n자세한 내용은 Docker 시작하기를 참조바랍니다.\ncloud-movie system Deploy Spec    container명 images UDN port 노출     cloud-movie cloud-movie:latest cm-back 8080 x   cloud-movie-db mariadb:10.2.13 cm-back 3306 x   cloud-movie-front cloud-movie-front:latest cm-front 8081 o   cloud-movie-apigateway cloud-movie-apigateway:latest cm-front cm-back 9999 x   cloud-movie-config-server cloud-movie-config-server:latest cm-back 8888 o   cloud-movie-hystrix-dashboard cloud-movie-hystrix-dashboard:latest cm-front 7979 o     외부에서 접근을 하는 기능이 필요한 경우만 port를 노출합니다.\n 컨테이너 구동  docker network 명령어로 User Defined Network를 생성합니다. cm-front, cm-back로 명명합니다.  docker network create cm-front docker network create cm-back   docker run 명령어로 컨테이너를 구동합니다. 필요한 환경변수나 port 노출 등의 옵션을 동시에 수행합니다. 구동시 참조하는 우선순위에 따라 수행합니다.  cloud-movie-config-server docker run --name cloud-movie-config-server -p 8888:8888 -d cloud-movie-config-server:latest cloud-movie-db docker run --name cloud-movie-db --network cm-back -e MYSQL_DATABASE=db_example -e MYSQL_ROOT_PASSWORD=ThePassword -d mariadb:10.2.13 --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci cloud-movie docker run --name cloud-movie --network cm-back -e SPRING_PROFILES_ACTIVE=docker -d cloud-movie:latest cloud-movie-apigateway docker run --name cloud-movie-apigateway --network cm-back -e SPRING_PROFILES_ACTIVE=docker -d cloud-movie-apigateway:latest docker network connect cm-front cloud-movie-apigateway cloud-movie-front docker run --name cloud-movie-front -p 8081:8081 --network cm-front -e SPRING_PROFILES_ACTIVE=docker -d cloud-movie-front:latest cloud-movie-hystrix-dashboard docker run --name cloud-movie-hystrix-dashboard -p 7979:7979 --network cm-front -e SPRING_PROFILES_ACTIVE=docker -d cloud-movie-hystrix-dashboard:latest   cloud-movie system Test MSA를 적용한 최소 사양의 system이 Docker에서 동작하는 것을 확인합니다.\ncloud-movie의 기본 동작들을 확인해 Docker User Defined Network 상에서 컨테이너 명을 이용해 Host 정보를 찾아 동작하는지 확인합니다.\ncloud-movie CRUD Test  전체 목록 조회  Movie List 추가  Movie List 건별 조회  Movie List 건별 수정  Movie List 건별 삭제  cloud-movie-hystrix-dashboard 모니터링   Conclusion Spring cloud와 Docker를 사용해 최소 단위의 MSA가 적용된 system을 구동했습니다. 테스트를 진행한 cloud-movie system의 Backend를 확장해 API 기능을 추가한다면, 용도에 맞게 MicroService를 개발해서 cm-back에 binding 하는 것으로 쉽게 확장이 가능합니다.\n컨테이너의 외부 노출 여부 및 컨테이너 간 연계를 고려해 User Defined Network를 적절히 사용하면 API 서버를 private network 상에서 구동시킬 수 있습니다. UI를 통해 의도된 동작 이외에 직접적으로 API 서버나 DB에 접근하는 것을 방지할 수 있습니다.\n추가적으로 docker-compose를 사용해서 이미지화 및 컨테이너 구동 관리를 용이하게 할 수 있습니다.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/docker/docker-start/",
        "title": "Docker 시작하기",
        "tags": ["docker", "container", "caas"],
        "description": "",
        "content": " 리눅스의 컨테이너 기술은 굉장히 오래전부터 있던 기술입니다. 그런데 왜 최근에 화두로 떠오르고 있는걸까요? 저는 컨테이너 기술 활성화에 크게 기여한 것이 바로 Docker라고 생각합니다. Docker는 Docker Store(구 Docker Hub)라는 퍼블릭한 레지스트리를 통해 여러 기업들에서 참여하여 이미지를 제공해주고 있지요. 오픈소스 솔루션 중에는 없는 이미지를 찾는게 더 어려운 것 같습니다. 이처럼 Docker 생태계가 잘 되어 있는데 그 생태계를 사용자들로 하여금 굉장히 쉽게 활용할 수 있도록 편의성을 제공해주고 있기 때문에 사용자들은 원하는 솔루션에 대한 이미지를 받아서 컨테이너로 띄우기만 하면 끝입니다. 또한 컨테이너는 경량화된 독립적 형태로 운영되기 때문에 새로운 기술을 빠르게 적용할 수 있고, 컨테이너가 실행 가능한 환경이라면 어디에서든 똑같은 환경을 그대로 구축할 수 있게 됩니다.\n즉, 뭔가 빠르고 적시 배포가 이루어져야 하는 상황에 아주 최적화된 형태가 바로 컨테이너로서 운영되는 형태인 것입니다. 그래서 마이크로서비스로 아키텍처를 구성하고 계신 굉장히 많은 분들이 Docker, Kubernetes를 활용하고 있습니다.\n그럼 일반적으로 Docker를 이용하여 애플리케이션을 컨테이너로서 배포하고 관리하는 방법에 대해 살펴보도록 하겠습니다.\n애플리케이션 배포 애플리케이션을 Docker에 배포하는 방법입니다.\n앱준비  샘플 앱 다운로드\n Spring Boot 공식 가이드에서 제공하는 샘플입니다. https://github.com/spring-guides/gs-spring-boot-docker.git   샘플 앱 수정 및 빌드\n 샘플 프로젝트의 initial 폴더로 이동한 뒤 /src/main/java/hello/Application.java 파일에 아래와 같이 간단한 API를 추가합니다.\nApplication.java\npackage hello; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @SpringBootApplication @RestController public class Application { @RequestMapping(\u0026#34;/\u0026#34;) public String home() { return \u0026#34;Hello Docker World\u0026#34;; } public static void main(String[] args) { SpringApplication.run(Application.class, args); } }  Maven을 활용하여 애플리케이션을 빌드합니다.     Dockerfile 작성 Dockerfile은 Docker에서 이미지 레이어를 지정하고 컨테이너 내부 환경을 정의하는데 사용되는 간단한 파일 형식입니다.\n아래와 같이 Dockerfile을 작성해봅니다.\nDockerfile\nFROM openjdk:8-jdk-alpine VOLUME /tmp ADD target/gs-spring-boot-docker-0.1.0.jar app.jar ENV JAVA_OPTS=\u0026#34;\u0026#34; ENTRYPOINT [\u0026#34;java\u0026#34;,\u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34;,\u0026#34;-jar\u0026#34;,\u0026#34;/app.jar\u0026#34;] Dockerfile 을 작성하는데 필요한 기본 명령어 입니다. (참조 - Dockerfile 작성방법)\n FROM : 기반이 되는 이미지를 설정합니다. Dockerfile은 FROM 명령어로 시작해야 합니다. VOLUME : Docker 컨테이너에 의해 사용되는 데이터를 유지하는데 사용합니다. 해당 실습에서는 Spring Boot 어플리케이션이 기본적으로 톰캣 작업 디렉토리로 /tmp 폴더를 사용하므로 해당 폴더를 VOLUME으로 추가합니다.\n그러면 호스트의 Docker 특정 영역에 임시 파일을 만들고 컨테이너 내부 디렉토리인 /tmp 에 링크합니다.   Volumes vs. Bind mounts 컨테이너의 데이터를 유지하는 또 다른 방식인 Bind mounts 는 호스트 시스템의 파일이나 디렉토리가 컨테이너에 마운트됩니다. 이 방식은 Dockerfile에서는 사용할 수 없으며 Docker 명령어 docker run -v ...의 옵션을 통해 사용 가능합니다. Bind mounts는 성능이 뛰어나지만 호스트 시스템의 파일시스템 구조에 의존적이며 Docker CLI로 관리할 수 없어 Volume 방식에 비해 제한적입니다.\n  ADD : 파일이나 디렉토리 등을 복사하여 이미지 파일시스템에 추가합니다. 해당 실습에서는 프로젝트 JAR 파일[target/gs-spring-boot-docker-0.1.0.jar]이 컨테이너에 app.jar로 추가됩니다.  ADD vs. COPY ADD 와 COPY 명령어는 모두 \u0026lt;src\u0026gt; 경로의 파일이나 디렉토리를 복사하여 \u0026lt;dest\u0026gt; 경로의 이미지 파일시스템에 추가하지만 아래 표와 같이 약간의 차이점이 있습니다.\n    명령어 사용법 설명    ADD ADD \u0026lt;src\u0026gt; \u0026hellip; \u0026lt;dest\u0026gt;\nADD [\u0026quot;\u0026lt;src\u0026gt;\u0026quot;, \u0026hellip; \u0026quot;\u0026lt;dest\u0026gt;\u0026quot;] \u0026lt;src\u0026gt;는 \u0026quot;docker build\u0026quot; 명령어를 통해 빌드할 컨텍스트 하위의 상대경로로 지정해야 하며 \u0026lt;dest\u0026gt;는 상대경로 또는 절대경로 모두 가능합니다.\n\u0026lt;src\u0026gt;에 URL 지정도 가능하며 URL을 통해 \u0026lt;dest\u0026gt;로 파일을 다운로드합니다.\nURL형식은 적절한 파일 이름을 찾을 수 있는 형태이어야 합니다.\n예) ADD http://example.com/foobar / (o), ADD http://example.com / (x)  압축형식이 tar (tar, gzip, bzip2, xz) 인 압축 파일일 경우 해제하여 복사합니다.  COPY COPY \u0026lt;src\u0026gt; \u0026hellip; \u0026lt;dest\u0026gt;\nCOPY [\u0026quot;\u0026lt;src\u0026gt;\u0026quot;, \u0026hellip; \u0026quot;\u0026lt;dest\u0026gt;\u0026quot;] \u0026lt;src\u0026gt;는 \u0026quot;docker build\u0026quot; 명령어를 통해 빌드할 컨텍스트 하위의 상대경로로 지정해야 하며 \u0026lt;dest\u0026gt;는 상대경로 또는 절대경로 모두 가능합니다. \u0026lt;src\u0026gt;에 URL 지정이 불가합니다. 압축 파일은 압축을 해제하지 않고 그대로 복사합니다.   \n ENV : 환경변수를 설정합니다.\n RUN : 새로운 레이어에서 명령어를 실행하고 결과를 커밋합니다. 보통 이미지 위에 다른 패키지(프로그램)을 설치하여 새로운 레이어를 생성할 때 사용합니다.\n RUN 사용 예제 (ubuntu 위에 curl을 설치)  FROM ubuntu:14.04 RUN apt-get update RUN apt-get install -y curl  ENRTYPOINT : 컨테이너를 실행할 때 실행될 명령을 정의합니다.  ENTRYPOINT 작성에는 아래 두가지 방식이 있습니다. 권장되는 방식은 exec 형태입니다.\n   \n Syntax 예시    exec 방식 (권장) ENTRYPOINT [\u0026quot;executable\u0026quot;, \u0026quot;param1\u0026quot;, \u0026quot;param2\u0026quot;] ENTRYPOINT [\u0026quot;java\u0026quot;,\u0026quot;-Djava.security.egd=file:/dev/./urandom\u0026quot;,\u0026quot;-jar\u0026quot;,\u0026quot;/app.jar\u0026quot;]  shell 방식 ENTRYPOINT command param1 param2 ENTRYPOINT exec java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /app.jar exec로 실행하지 않으면 \u0026quot;docker stop\u0026quot; 명령어로 컨테이너를 종료할 때 Unix 시그널인 SIGTERM을 받을수 없어 timeout 후 SIGKILL( 강제종료)됨. shell 방식에서는 Graceful Shutdown을 위해 exec로 실행해야 함.    ENTRYPOINT vs. CMD ENTRYPOINT와 CMD 명령어는 모두 컨테이너를 실행할 때 실행될 명령을 정의합니다.CMD는 docker run 실행 시 명령어를 주지 않았을 때 사용할 기본 명령을 설정하거나, 컨테이너를 실행할 때 대체(Override)될 수 있기 때문에 ENTRYPOINT의 기본 파라미터를 설정하는데 사용합니다.Dockerfile에 둘 중 하나의 명령어는 반드시 지정되어야 합니다.\n  EXPOSE : 컨테이너가 런타임 시에 지정된 포트에서 수신 대기하고 있음을 정의합니다. 이 명령어는 어떤 포트가 publish 되어야 하는지에 대해 이미지를 만든 사람과 컨테이너를 실행할 사람 사이에 일종의 규약처럼 기능합니다.\n해당 명령어로는 실제로 포트를 publish 할 수 없으며 docker run 명령어의 -p 또는 -P 플래그를 통해 가능합니다.  Docker 이미지 생성  아래와 같이 docker build 명령어를 실행하여 Dockerfile을 통해 이미지를 생성합니다.\n-t 옵션을 사용하여 [이미지명:태그] 형식의 태그를 지정합니다.\n$ docker build -t gs-spring-boot-docker . Sending build context to Docker daemon 29.32MB Step 1/5 : FROM openjdk:8-jdk-alpine 8-jdk-alpine: Pulling from library/openjdk b56ae66c2937: Pull complete 2296e775ba08: Pull complete 6e753bb2ec67: Pull complete Digest: sha256:cee76de7d24c94a9453981cc2c95ff3b7e6de71fdb67ffc2390ab8429b886b95 Status: Downloaded newer image for openjdk:8-jdk-alpine ---\u0026gt; 3b1fdb34c52a Step 2/5 : VOLUME /tmp ---\u0026gt; Running in 84c7fed2e2d8 ---\u0026gt; 694f0b4edd9a Removing intermediate container 84c7fed2e2d8 Step 3/5 : ADD target/gs-spring-boot-docker-0.1.0.jar app.jar ---\u0026gt; 5bb45b344163 Step 4/5 : ENV JAVA_OPTS \u0026#34;\u0026#34; ---\u0026gt; Running in c4388e5d2cab ---\u0026gt; 1a387aa87055 Removing intermediate container c4388e5d2cab Step 5/5 : ENTRYPOINT exec java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /app.jar ---\u0026gt; Running in a1bcf2591634 ---\u0026gt; fd3cd7c70ff6 Removing intermediate container a1bcf2591634 Successfully built fd3cd7c70ff6 Successfully tagged gs-spring-boot-docker:latest  docker images 명령어를 통해 이미지가 제대로 생성되었는지 확인합니다.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE gs-spring-boot-docker latest fd3cd7c70ff6 2 seconds ago 116MB 애플리케이션 실행(이미지실행, 컨테이너실행) docker run 명령어를 통해 생성한 이미지를 기반으로 애플리케이션을 실행합니다. Docker에서는 애플리케이션이 하나의 컨테이너로 실행됩니다.\n-p [호스트 포트:컨테이너 포트] 옵션을 통해 컨테이너의 포트를 호스트에 publish하여 호스트에서 접속 가능하도록 실행합니다. $ docker run -p 8080:8080 gs-spring-boot-docker docker run Dockerfile 명령어 중 4개 (FROM, MAINTAINER, RUN, ADD)는 docker run 으로 오버라이드 할 수 없으며 이외의 모든 명령어는 오버라이드가 가능합니다.docker run 명령어에는 많은 옵션이 있습니다. 그 중 주요 옵션에 대한 설명입니다.\n     옵션 설명    --detach, -d 컨테이너를 실행할 때 백그라운드(\u0026quot;detached mode\u0026quot;)로 실행할 지에 대한 여부를 설정합니다. -d 옵션을 사용하면 백그라운드로 실행되며 기본 설정은 포그라운드 모드입니다. 포그라운드 모드로 실행하면 표준 입력, 출력 및 오류가 콘솔에 보여질 수 있습니다.  --env, -e 컨테이너를 실행할 때 간단한 환경 변수를 설정합니다. (배열 사용 불가) Dockerfile에 정의된 변수는 덮어씁니다.  --link 해당 컨테이너로 다른 컨테이너를 연결합니다. Docker 는 환경 변수나 호스트 파일을 통해 소스 컨테이너의 연결 정보를 해당 컨테이너로 노출합니다. 소스 컨테이너의 환경변수가 공유되기 때문에 보안에 취약하므로 해당 옵션을 통한 연결은 권장되지 않는 방식입니다.network를 정의하여 사용하기를 권장합니다.  --publish, -p 컨테이너의 포트를 호스트의 특정 포트나 특정 범위를 지정하여 매핑하여 publish합니다.  --publish-all, -P 컨테이너 내부의 모든 포트를 호스트의 임시 포트 범위 내의 임의의 포트에 매핑하여 publish합니다.  --volume, -v bind 방식으로 호스트 디렉토리에 컨테이너의 데이터를 마운트합니다.  --mount 이 옵션은 volume, bind, tmpfs 세가지 방식으로 마운트하도록 지원합니다. --volume 플래그가 지원하는 대부분의 옵션을 지원하지만 문법에 차이가 있습니다. --volume 플래그보다 --mount 플래그의 사용을 권장합니다.  -it Docker 는 컨테이너에 bash 쉘을 생성하여 컨테이너의 STDIN에 연결된 가상 TTY을 할당합니다.    애플리케이션 확인 아래와 같이 docker ps 명령어를 통해 실행중인 컨테이너 목록을 확인할 수 있습니다.\n애플리케이션이 컨테이너로 실행되었음을 확인할 수 있습니다.\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f4345141af03 gs-spring-boot-docker \u0026#34;/bin/sh -c \u0026#39;exec ...\u0026#34; 27 seconds ago Up 26 seconds 0.0.0.0:8080-\u0026gt;8080/tcp unruffled_gates http://localhost:8080 으로 접속하여 확인해봅니다.\n컨테이너 관리 컨테이너 생성 컨테이너 생성 및 확인은 위 애플리케이션 실행 및 확인을 참고합니다.\n컨테이너 중지 아래와 같이 docker stop [컨테이너ID] 명령어를 통해 실행중인 컨테이너를 중지합니다.\n$ docker stop f4345141af03 f4345141af03 docker ps 명령어에 -a 옵션을 통해 전체 컨테이너 목록을 확인해보면 STATUS가 Exited 상태임을 확인할 수 있습니다.\n$ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f4345141af03 gs-spring-boot-docker \u0026#34;/bin/sh -c \u0026#39;exec ...\u0026#34; 7 minutes ago Exited (143) 6 minutes ago unruffled_gates 컨테이너 삭제  아래와 같이 docker rm [컨테이너ID] 명령어를 통해 컨테이너를 삭제합니다.\n$ docker rm f4345141af03 f4345141af03 삭제하고 난뒤 컨테이너 목록을 보면 해당 컨테이너가 삭제되었음을 확인할 수 있습니다.\n$ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 이미지 관리 컨테이너를 운영환경에 배포하는 등 이미지의 이식성을 위해 레지스트리를 통해 이미지를 관리해야 합니다.\n레지스트리는 레파지토리의 모음이며 레지스트리에서 하나의 계정으로 여러 레파지토리를 생성할 수 있습니다.\nDocker의 공용 퍼블릭 레지스트리인 Docker Hub는 사전에 이미 구성되어 있는 무료 레지스트리이지만 Docker Trusted Registry를 사용하여 private한 레지스트리를 설정할 수 있습니다.\nDocker Hub 활용 Docker 계정이 없다면 https://hub.docker.com 에서 가입합니다.\nDocker Hub에 로그인 docker login 명령어를 통해 로컬에서 Docker Hub에 로그인 합니다.\n$ docker login Login with your Docker ID to push and pull images from Docker Hub. If you don\u0026#39;t have a Docker ID, head over to https://hub.docker.com to create one. Username: dtlabs Password: Login Succeeded docker login 시 에러가 발생하는 경우 Window PC에서 Docker Quickstart Terminal 이용하는 경우에 docker login 명령어 실행 시 Cannot perform an interactive login from a non TTY device 에러가 발생할 수 있습니다. 그런 경우에는 아래와 같이 winpty 명령어를 앞에 붙여서 실행해 보시기 바랍니다.\n $ winpty docker login 로컬 이미지 태깅 로컬에 만들어진 이미지를 Docker Hub에 업로드할 수 있도록 이미지 태깅이 필요합니다. 태깅을 하면 같은 이미지가 다른 이름으로 하나 더 만들어 집니다. docker tag image username/repository:tag 명령어를 통해 이미지를 원하는 곳에 업로드 할 수 있도록 합니다.\n로컬 이미지를 Docker Hub 레파지토리와 연관시키는 표기법은 username/repository:tag 입니다.\n태그는 선택 사항이지만 Docker 이미지의 버전을 제공하는 것이기 때문에 사용하기를 권장합니다.\n$ docker tag gs-spring-boot-docker dtlabs/gs-spring-boot-docker:1.0 새로 태그 된 이미지를 확인합니다.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE dtlabs/gs-spring-boot-docker 1.0 fd3cd7c70ff6 About an hour ago 116MB gs-spring-boot-docker latest fd3cd7c70ff6 About an hour ago 116MB Docker Hub로 이미지 업로드 docker push 명령어를 통해 레파지토리에 태그된 이미지를 업로드 합니다.\n$ docker push dtlabs/gs-spring-boot-docker:1.0 The push refers to a repository [docker.io/dtlabs/gs-spring-boot-docker] a14546d9cd41: Mounted from blingeee/gs-spring-boot-docker 9ea2e4869a53: Mounted from blingeee/gs-spring-boot-docker eef4e2bfc309: Mounted from blingeee/gs-spring-boot-docker 2aebd096e0e2: Mounted from blingeee/gs-spring-boot-docker 1.0: digest: sha256:1c892a91e700c1868242072c32121850fcd1dfd01737066fcd41818381e9cfc3 size: 1159 이제부터는 업로드한 이미지로 모든 컴퓨터에서 애플리케이션을 실행할 수 있습니다.\n이미지가 로컬에 없다면 Docker는 Docker Hub 레지스트리의 레파지토리로부터 이미지를 가져옵니다.\n$ docker run -p 8080:8080 dtlabs/gs-spring-boot-docker:1.0  Private Registry 활용  Private Registry로서 활용 가능한 Harbor를 이용해서 이미지를 보호할 수 있습니다. 오픈소스인 Harbor를 통해 Private Registry를 구축한 뒤 Docker Hub를 활용하는 방식과 동일하게 활용하면 됩니다. (Harbor 레지스트리에 로그인 -\u0026gt; 이미지 태깅 -\u0026gt; Harbor 레지스트리로 이미지 업로드)  Harbor 이외에도 다양한 솔루션을 활용해서 Private Registry를 구축해 보세요.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/kubernetes/secret/",
        "title": "[Kubernetes 활용(7/8)] Secret",
        "tags": ["kubernetes", "container", "container orchestration", "k8s", "secret"],
        "description": "",
        "content": " 지난 챕터의 ConfigMap에 이어서 이번에는 Secret Object를 보도록 하겠습니다.\nSecret 이란? Secret은 비밀번호나 OAuth 토큰값 또는 ssh key 등의 민감한 정보를 유지하기 위해 사용됩니다. 이러한 정보를 Docker 이미지나 Pod에 그대로 정의하기 보다 Secret을 활용하면 더욱 안전하고 유동적으로 사용할 수 있습니다.\nSecret 적용하기 Secret 생성 명령어를 통해 생성하기 아래와 같이 kubectl create secret 명령어를 통해 Secret을 생성합니다.\nWindow OS의 경우 아래 yaml 파일로 Secret을 직접 생성하는 방식으로 사용하세요.\n$ kubectl create secret generic db-user-pass --from-literal=user=admin --from-literal=password=1f2d1e2e67df secret \u0026#34;db-user-pass\u0026#34; created 아래와 같이 kubectl get 명령어와 kuberctl describe 명령어를 통해 생성된 Secret을 확인합니다.\n이 명령어들로는 Secret 데이터를 볼 수 없으며 이는 Secret이 노출되지 않도록 보호하기 위함입니다.\n데이터를 보려면 아래 Sercret 디코딩 을 참고하세요.\n$ kubectl get secrets NAME TYPE DATA AGE db-user-pass Opaque 2 25s $ kubectl describe secrets/db-user-pass Name: db-user-pass Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== password: 12 bytes user: 5 bytes yaml 파일을 통해 생성하기 yaml 형식의 파일을 만들고 kubectl create 명령어를 통해 Secret 을 생성합니다. 각 항목은 base64로 인코딩 되어야 합니다.\n$ echo -n \u0026#34;admin\u0026#34; | base64 YWRtaW4= $ echo -n \u0026#34;1f2d1e2e67df\u0026#34; | base64 MWYyZDFlMmU2N2Rm secret.yaml\napiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm$ kubectl apply -f secret.yaml secret \u0026#34;mysecret\u0026#34; created Secret 디코딩 Secret에 작성된 실제 데이터를 확인하려면 kubectl get secret 명령어를 통해 인코딩 된 데이터를 확인합니다.\n$ kubectl get secret mysecret -o yaml apiVersion: v1 data: password: MWYyZDFlMmU2N2Rm username: YWRtaW4= kind: Secret metadata: creationTimestamp: 2017-12-11T06:31:37Z name: mysecret namespace: default resourceVersion: \u0026#34;76497\u0026#34; selfLink: /api/v1/namespaces/default/secrets/mysecret uid: f0f1b4f5-de3c-11e7-a37c-08002780475f type: Opaque 인코딩 되어 있는 password 값을 디코드하여 실제 데이터를 확인합니다.\n$ echo \u0026#34;MWYyZDFlMmU2N2Rm\u0026#34; | base64 --decode 1f2d1e2e67df Secret 사용 Secret을 Pod에서 환경변수로 사용 아래와 같이 Secret을 사용할 Pod의 spec에 env를 개별적으로 정의합니다.\nenv[].valueFrom.secretKeyRef 에 Secret의 이름과 key를 채워 환경 변수를 정의합니다.\ngs-spring-boot-docker-deployment.yaml\napiVersion: apps/v1beta2 kind: Deployment metadata: name: gs-spring-boot-docker-deployment labels: app: gs-spring-boot-docker spec: replicas: 1 selector: matchLabels: app: gs-spring-boot-docker template: metadata: labels: app: gs-spring-boot-docker spec: containers: - name: gs-spring-boot-docker image: dtlabs/gs-spring-boot-docker:1.0 ports: - containerPort: 8080 env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: mysecret key: password restartPolicy: Always 이렇게 설정한 뒤 Deployment를 배포합니다.\nkubectl apply -f gs-spring-boot-docker-deployment.yaml Pod의 컨테이너 내부에서는 Secret key 값이 base64 디코드된 값의 정상 환경변수로 나타납니다.\n 배포된 Pod에 접속하여 환경변수가 잘 적용되었는지 확인합니다.\n아래와 같이 컨테이너 접속 후 env 명령어를 통해 Secret 정보인 환경 변수(SECRET_USERNAME, SECRET_PASSWORD)가 제대로 적용되었음을 확인합니다.\n$ kubectl get pod NAME READY STATUS RESTARTS AGE gs-spring-boot-docker-deployment-5d7db89759-xvm5t 1/1 Running 0 1m $ kubectl exec -it gs-spring-boot-docker-deployment-5d7db89759-xvm5t sh / # env KUBERNETES_PORT=tcp://10.96.0.1:443 KUBERNETES_SERVICE_PORT=443 JAVA_ALPINE_VERSION=8.131.11-r2 HOSTNAME=gs-spring-boot-docker-deployment-5d7db89759-xvm5t SHLVL=1 HOME=/root SECRET_PASSWORD=1f2d1e2e67df JAVA_VERSION=8u131 TERM=xterm KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-1.8-openjdk/jre/bin:/usr/lib/jvm/java-1.8-openjdk/bin KUBERNETES_PORT_443_TCP_PORT=443 JAVA_OPTS= KUBERNETES_PORT_443_TCP_PROTO=tcp LANG=C.UTF-8 SECRET_USERNAME=admin KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 KUBERNETES_SERVICE_PORT_HTTPS=443 PWD=/ JAVA_HOME=/usr/lib/jvm/java-1.8-openjdk KUBERNETES_SERVICE_HOST=10.96.0.1 Secret 관리 Secret 생성 kubectl create secret 명령어 또는 yaml 파일을 통해 생성합니다.\n자세한 내용은 위의 활용 부분을 참고하세요.\nSecret 수정 Secret 수정 kubectl edit secret 명령어를 통해 수정합니다.\n수정할 데이터는 base64로 인코딩 된 값이어야 하며 예제에서는 user 값을 변경해 봅시다.\nWindow OS의 경우 해당 명령어로 수정이 불가합니다. 아래 파일을 직접 수정하는 방식으로 사용하세요.\n$ echo -n \u0026#34;test\u0026#34; | base64 dGVzdA== $ kubectl edit secret db-user-pass secret \u0026#34;db-user-pass\u0026#34; edited $ kubectl get secret db-user-pass -o yaml apiVersion: v1 data: password: MWYyZDFlMmU2N2Rm user: dGVzdA== kind: Secret metadata: creationTimestamp: 2017-12-11T06:16:25Z name: db-user-pass namespace: default resourceVersion: \u0026#34;85279\u0026#34; selfLink: /api/v1/namespaces/default/secrets/db-user-pass uid: d0eb0495-de3a-11e7-a37c-08002780475f type: Opaque 또는 Secret yaml 파일을 직접 수정한 뒤 적용합니다. 예제에서는 mysecret의 username 값을 변경한 뒤 secret.yaml\napiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: dGVzdA== password: MWYyZDFlMmU2N2Rm 아래 명령어 kubectl apply를 통해 적용하고 확인합니다.\n$ kubectl apply -f secret.yaml Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply secret \u0026#34;mysecret\u0026#34; configured $ kubectl get secret mysecret -o yaml apiVersion: v1 data: password: MWYyZDFlMmU2N2Rm username: dGVzdA== kind: Secret metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;password\u0026#34;:\u0026#34;MWYyZDFlMmU2N2Rm\u0026#34;,\u0026#34;username\u0026#34;:\u0026#34;dGVzdA==\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;Secret\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;mysecret\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;type\u0026#34;:\u0026#34;Opaque\u0026#34;} creationTimestamp: 2017-12-11T06:31:37Z name: mysecret namespace: default resourceVersion: \u0026#34;86747\u0026#34; selfLink: /api/v1/namespaces/default/secrets/mysecret uid: f0f1b4f5-de3c-11e7-a37c-08002780475f type: Opaque 수정된 Secret을 Pod에 적용 Secret을 수정한 뒤에 Pod에 적용하기 위해서 Pod를 재생성해야 합니다.\nkubectl replace --force -f 명령어를 통해 Pod를 재생성합니다. -l 옵션은 스케일링된 Pod일 경우 동일한 라벨을 가지고 스케일링 되므로 해당 라벨을 가진 모든 Pod에 적용하기 위한 설정입니다.\n$ kubectl get pod -l app=gs-spring-boot-docker -o yaml | kubectl replace --force -f - pod \u0026#34;gs-spring-boot-docker-deployment-5d7db89759-xvm5t\u0026#34; deleted pod \u0026#34;gs-spring-boot-docker-deployment-5d7db89759-xvm5t\u0026#34; replaced 새로 생성된 Pod에 변경된 Secret이 제대로 적용되었는지 확인합니다. 컨테이너에 접속하여 env 명령어를 통해 환경변수(SECRET_USERNAME) 값이 test로 제대로 변경되었는지 확인합니다.\n$ kubectl get pod NAME READY STATUS RESTARTS AGE gs-spring-boot-docker-deployment-5d7db89759-hn5q8 1/1 Running 0 1m $ kubectl exec -it gs-spring-boot-docker-deployment-5d7db89759-hn5q8 sh / # env KUBERNETES_PORT=tcp://10.96.0.1:443 KUBERNETES_SERVICE_PORT=443 JAVA_ALPINE_VERSION=8.131.11-r2 HOSTNAME=gs-spring-boot-docker-deployment-5d7db89759-hn5q8 SHLVL=1 HOME=/root SECRET_PASSWORD=1f2d1e2e67df JAVA_VERSION=8u131 TERM=xterm KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-1.8-openjdk/jre/bin:/usr/lib/jvm/java-1.8-openjdk/bin KUBERNETES_PORT_443_TCP_PORT=443 JAVA_OPTS= KUBERNETES_PORT_443_TCP_PROTO=tcp LANG=C.UTF-8 SECRET_USERNAME=test KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 KUBERNETES_SERVICE_PORT_HTTPS=443 PWD=/ JAVA_HOME=/usr/lib/jvm/java-1.8-openjdk KUBERNETES_SERVICE_HOST=10.96.0.1 Secret 삭제 kubectl delete 명령어를 통해 yaml 파일을 통해 생성했던 mysecret을 삭제합니다.\n$ kubectl delete -f secret.yaml secret \u0026#34;mysecret\u0026#34; deleted kubectl get secret 명령어를 통해 secret 목록을 확인합니다. mysecret이 제대로 삭제되었는지 확인해봅니다.\n$ kubectl get secret NAME TYPE DATA AGE db-user-pass Opaque 2 4h"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/kubernetes/configmap/",
        "title": "[Kubernetes 활용(6/8)] ConfigMap",
        "tags": ["kubernetes", "container", "container orchestration", "k8s", "configmap"],
        "description": "",
        "content": " 이번에는 Kubernetes에서 제공하는 ConfigMap이라는 Object를 보도록 하겠습니다.\nConfigMap 이란? ConfigMap은 컨테이너 이미지에서 사용하는 환경변수와 같은 세부 정보를 분리하고, 그 환경변수에 대한 값을 외부로 노출 시키지 않고 내부에 존재하는 스토리지에 저장해서 사용하는 방법입니다. 혹시 마이크로서비스 아키텍처에서 사용하는 Spring Cloud Config(Config Server)를 사용한 적이 있다면 동일한 역할을 하는 것인지 하는 생각이 들 수 있는데요. Spring Cloud Config 같은 경우에는 설정 파일 자체를 분리하고 파일에 대한 내용이 변경된다면 자동으로 Refresh 해주는 기능을 가지고 있습니다. 반면에 ConfigMap 은 자동 Refresh 기능이 없고 변경 시 다시 Pod를 띄워줘야 합니다. 또한 파일로 ConfigMap을 생성한다면 설정 파일 자체를 분리할 수는 있지만 실제로 설정 파일의 값을 애플리케이션에서 사용하기 위해서는 추가적인 파싱 작업이 필요합니다. 그래서 동일한 역할을 한다고 볼 수는 없으며 Spring Cloud Config 가 좀 더 강력한 기능을 제공한다고 볼 수 있을 것 같습니다. 하지만.. Spring Cloud Config 라이브러리는 Spring Boot로 작성되기 때문에 확실히 언어적인 종속성이 있는 것은 사실입니다. 뭐 찾아보면 다른 언어로 작성된 오픈소스가 있기야 하겠지만 아무래도 실제 시스템에서 사용되기엔 많은 검증이 필요하겠지요. 그럼 ConfigMap을 일반 문자열 또는 파일로 생성하는 두 가지 방식에 대해 알아봅시다.\nConfigMap 활용하기 ConfigMap 생성 (literal values) 문자열 값으로 ConfigMap 생성 kubectl create configmap 명령어의 \u0026ndash;from–literal 옵션으로 ConfigMap에 리터럴 값을 제공합니다. 아래와 같이 special-config 를 작성해 봅시다.\nWindow OS의 경우 아래 yaml 파일로 ConfigMap을 직접 생성하는 방식으로 사용하세요.\n$ kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm 다른 방법으로는 yaml 파일로 ConfigMap을 생성할 수 있으며 파일에 데이터를 key:value 형태로 등록합니다.\n아래와 같이 env-config 를 작성해 봅시다.\nenv-config.yaml\napiVersion: v1 kind: ConfigMap metadata: name: env-config data: log_level: INFO$ kubectl apply -f env-config.yaml kubectl describe 또는 kubectl get 명령어를 통해 생성된 ConfigMap의 정보를 확인합니다.\n$ kubectl describe configmaps special-config Name: special-config Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== special.how: ---- very special.type: ---- charm Events: \u0026lt;none\u0026gt;$ kubectl get configmaps env-config -o yaml apiVersion: v1 data: log_level: INFO kind: ConfigMap metadata: creationTimestamp: 2017-12-07T09:32:59Z name: env-config namespace: default resourceVersion: \u0026#34;10304\u0026#34; selfLink: /api/v1/namespaces/default/configmaps/env-config uid: 9cfd35ce-db31-11e7-a37c-08002780475f Pod에서 ConfigMap 데이터 사용 아래와 같이 배포할 Pod spec에 ConfigMap의 데이터를 환경변수(env)로 개별적으로 정의하거나\nenvFrom을 사용하여 ConfigMap의 모든 데이터를 Pod 환경 변수로 정의할 수 있습니다. 이렇게 설정한 경우 ConfigMap의 key가 Pod 환경 변수 이름이 됩니다.\ngs-spring-boot-docker-deployment.yaml\napiVersion: apps/v1beta2 kind: Deployment metadata: name: gs-spring-boot-docker-deployment labels: app: gs-spring-boot-docker spec: replicas: 1 selector: matchLabels: app: gs-spring-boot-docker template: metadata: labels: app: gs-spring-boot-docker spec: containers: - name: gs-spring-boot-docker image: dtlabs/gs-spring-boot-docker:1.0 ports: - containerPort: 8080 env: - name : SPRING_PROFILES_ACTIVE value : k8s - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: special.how - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-config key: special.type envFrom: - configMapRef: name: env-config restartPolicy: Always$ kubectl apply -f gs-spring-boot-docker-deployment.yaml 배포된 Pod에 접속하여 환경변수가 잘 적용되었는지 확인합니다.\n$ kubectl get pod NAME READY STATUS RESTARTS AGE gs-spring-boot-docker-deployment-764579896b-zm69w 1/1 Running 0 3s $ kubectl exec -ti gs-spring-boot-docker-deployment-764579896b-zm69w sh / # env KUBERNETES_PORT=tcp://10.96.0.1:443 KUBERNETES_SERVICE_PORT=443 JAVA_ALPINE_VERSION=8.131.11-r2 HOSTNAME=gs-spring-boot-docker-deployment-764579896b-zm69w SHLVL=1 HOME=/root SPECIAL_TYPE_KEY=charm JAVA_VERSION=8u131 TERM=xterm KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-1.8-openjdk/jre/bin:/usr/lib/jvm/java-1.8-openjdk/bin KUBERNETES_PORT_443_TCP_PORT=443 JAVA_OPTS= KUBERNETES_PORT_443_TCP_PROTO=tcp LANG=C.UTF-8 SPECIAL_LEVEL_KEY=very log_level=INFO KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 PWD=/ JAVA_HOME=/usr/lib/jvm/java-1.8-openjdk KUBERNETES_SERVICE_HOST=10.96.0.1 SPRING_PROFILES_ACTIVE=k8s ConfigMap이 수정되면 Pod도 다시 배포되어야 적용됩니다.\n ConfigMap 생성 (files) 파일로 ConfigMap 생성하기 아래의 redis-config 파일로 ConfigMap을 생성합니다.\nkubectl create configmap 명령어의 \u0026ndash;from–file 옵션으로 ConfigMap에 파일 데이터를 제공합니다. redis-config\nmaxmemory 2mb maxmemory-policy allkeys-lru$ ls redis-config $ kubectl create configmap example-redis-config --from-file=redis-config configmap \u0026#34;example-redis-config\u0026#34; created ConfigMap이 제대로 생성되었는지 확인합니다.\n아래와 같이 ConfigMap의 data 섹션에 파일명은 key, 파일내용 전체가 value값이 됩니다.\n$ kubectl get configmap example-redis-config -o yaml apiVersion: v1 data: redis-config: |- maxmemory 2mb maxmemory-policy allkeys-lru kind: ConfigMap metadata: creationTimestamp: 2017-12-07T10:00:37Z name: example-redis-config namespace: default resourceVersion: \u0026#34;12206\u0026#34; selfLink: /api/v1/namespaces/default/configmaps/example-redis-config uid: 792b3fa8-db35-11e7-a37c-08002780475f Pod에서 ConfigMap 데이터 사용 (볼륨에 마운트하여 사용하는 방법) 배포할 Pod spec 섹션 에서 ConfigMap의 데이터를 볼륨의 특정경로에 추가하여 사용 가능합니다.\nvolumes 섹션에 ConfigMap 이름을 추가하고 volumeMounts.mountPath 에 지정한 디렉토리로 ConfigMap 데이터를 추가합니다.\n아래 예제의 경우 config 볼륨이 컨테이너의 /redis-master 경로에 마운트되고 path 라는 필드를 사용하여 redis.conf라는 파일에 redis-config key를 추가합니다.\n따라서 마운트된 redis config 파일의 경로는 최종적으로 /redis-master/redis.conf 가 되고 이 파일은 example-redis-config 라는 ConfigMap 데이터인 redis-config key의 value로 채워집니다.\nredis-pod.yaml\napiVersion: v1 kind: Pod metadata: name: redis spec: containers: - name: redis image: kubernetes/redis:v1 env: - name: MASTER value: \u0026#34;true\u0026#34; ports: - containerPort: 6379 resources: limits: cpu: \u0026#34;0.1\u0026#34; volumeMounts: - mountPath: /redis-master-data name: data - mountPath: /redis-master name: config volumes: - name: data emptyDir: {} - name: config configMap: name: example-redis-config items: - key: redis-config path: redis.conf  아래와 같이 Pod를 배포합니다.\n$ kubectl apply -f redis-pod.yaml  배포 후 redis Pod에 접속하여 해당 경로에 파일이 존재하는지 확인합니다.\n$ kubectl exec -it redis sh # cat /redis-master/redis.conf maxmemory 2mb maxmemory-policy allkeys-lru#  Pod에 접속하고 redis-cli를 실행하여 설정이 제대로 적용되었는지 확인합니다.\n$ kubectl exec -it redis redis-cli 127.0.0.1:6379\u0026gt; CONFIG GET maxmemory 1) \u0026#34;maxmemory\u0026#34; 2) \u0026#34;2097152\u0026#34; 127.0.0.1:6379\u0026gt; CONFIG GET maxmemory-policy 1) \u0026#34;maxmemory-policy\u0026#34; 2) \u0026#34;allkeys-lru\u0026#34; 마운트된 ConfigMap은 자동으로 업데이트됩니다. Kubelet은 마운트된 ConfigMap이 최신인지 주기적으로 확인합니다. 그래서 ConfigMap이 업데이트되고 동기화 기간 + ConfigMap 캐시의 ttl 만큼의 지연이 있을 수 있습니다.\n ConfigMap 관리 ConfigMap 생성 kubectl create configmap 명령어 또는 yaml 파일을 통해 생성합니다.\n자세한 내용은 위의 활용 부분을 참고하세요.\nConfigMap 수정  ConfigMap 수정 kubectl edit configmap 명령어를 통해 configmap을 수정합니다. 예제에서는 special-config에 special.test:test 를 추가하고 확인해봅니다.\nWindow OS의 경우 해당 명령어로 수정이 불가합니다. 아래 파일을 직접 수정하는 방식으로 사용하세요.\n$ kubectl edit configmap special-config configmap \u0026#34;special-config\u0026#34; edited $ kubectl describe cm special-config Name: special-config Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== special.how: ---- very special.test: ---- test special.type: ---- charm Events: \u0026lt;none\u0026gt; 또는 ConfigMap yaml 파일을 직접 수정한 뒤 적용합니다. 예제에서는 env-config의 log_level을 ERROR로 변경한 뒤 아래 명령어를 통해 적용합니다.\n$ kubectl apply -f env-config.yaml Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply configmap \u0026#34;env-config\u0026#34; configured $ kubectl describe cm env-config Name: env-config Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: kubectl.kubernetes.io/last-applied-configuration={\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;log_level\u0026#34;:\u0026#34;ERROR\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;env-config\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;}} Data ==== log_level: ---- ERROR Events: \u0026lt;none\u0026gt; 수정된 ConfigMap을 Pod에 적용 ConfigMap을 수정한 뒤에 Pod에 적용하기 위해서 Pod를 재생성해야 합니다.\nkubectl replace --force -f 명령어를 통해 Pod를 재생성합니다. -l 옵션은 스케일링된 Pod일 경우 동일한 라벨을 가지고 스케일링 되므로 해당 라벨을 가진 모든 Pod에 적용하기 위한 설정입니다.\n$ kubectl get pod -l app=gs-spring-boot-docker -o yaml | kubectl replace --force -f - pod \u0026#34;gs-spring-boot-docker-deployment-764579896b-zm69w\u0026#34; deleted pod \u0026#34;gs-spring-boot-docker-deployment-764579896b-zm69w\u0026#34; replaced 새로 생성된 Pod에 변경된 ConfigMap이 제대로 적용되었는지 확인합니다. $ kubectl get pod NAME READY STATUS RESTARTS AGE gs-spring-boot-docker-deployment-764579896b-dfdnq 1/1 Running 0 1m redis 1/1 Running 0 2d  아래 log_level 값이 ERROR로 변경되었음을 확인합니다.\n$ kubectl exec -it gs-spring-boot-docker-deployment-764579896b-dfdnq sh / # env KUBERNETES_PORT=tcp://10.96.0.1:443 KUBERNETES_SERVICE_PORT=443 JAVA_ALPINE_VERSION=8.131.11-r2 HOSTNAME=gs-spring-boot-docker-deployment-764579896b-dfdnq SHLVL=1 HOME=/root SPECIAL_TYPE_KEY=charm JAVA_VERSION=8u131 TERM=xterm KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-1.8-openjdk/jre/bin:/usr/lib/jvm/java-1.8-openjdk/bin KUBERNETES_PORT_443_TCP_PORT=443 JAVA_OPTS= KUBERNETES_PORT_443_TCP_PROTO=tcp LANG=C.UTF-8 SPECIAL_LEVEL_KEY=very log_level=ERROR KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 KUBERNETES_SERVICE_PORT_HTTPS=443 PWD=/ JAVA_HOME=/usr/lib/jvm/java-1.8-openjdk KUBERNETES_SERVICE_HOST=10.96.0.1 SPRING_PROFILES_ACTIVE=k8s ConfigMap 삭제 kubectl delete 명령어를 통해 ConfigMap 을 삭제합니다.\n$ kubectl delete -f env-config.yaml configmap \u0026#34;env-config\u0026#34; deleted kubectl get configmap 명령어를 통해 configmap 목록을 확인합니다. 제대로 삭제되었는지 확인해봅니다.\n$ kubectl get configmap NAME DATA AGE special-config 3 3d"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/kubernetes/ingress/",
        "title": "[Kubernetes 활용(5/8)] Ingress",
        "tags": ["kubernetes", "container", "container orchestration", "k8s", "service", "ingress", "nginx"],
        "description": "",
        "content": " Kubernetes에서는 애플리케이션을 외부로 노출하기 위해 Service object를 NodePort로 생성합니다. 그러나 노출 형태가 노드의 IP에 특정 포트(30000-32767)로 제공되기 때문에 호출이 까다롭고 사용자가 서비스로 유입되는 경로도 다양해서 관리가 어려워질 수 있는데요. 이 때, 외부 액세스를 관리하고 서비스를 묶어주는 역할을 하는게 바로 Ingress 입니다.\nIngress 란? 위 그림과 같이 Ingress는 외부 액세스를 관리하고 서비스를 묶어주는 역할을 합니다. Ingress를 만들 때 도메인을 지정할 수 있고 사용자는 그 도메인으로 접속을 하게 되며 도메인 하위의 path 설정을 통해 서비스들을 라우팅할 수 있게 됩니다. 그래서 사용자는 Ingress에 설정된 도메인에 하위 path를 url로 호출하게 되면 특정 서비스로 라우팅이 되고 그 서비스에서 로드밸런싱 정책을 통해 실제 Pod(컨테이너화된 애플리케이션)로 호출이 되는 원리입니다. 또한 Ingress는 추가적으로 가상 호스팅이나 TLS 등 여러 가지 기능을 제공합니다.\nIngress 적용 방법 애플리케이션 배포 먼저, Ingress에서 라우팅할 애플리케이션을 배포합니다.\n Kubernetes 환경에 애플리케이션을 배포하기 위해, Deployment YAML 파일을 작성합니다.\ngs-spring-boot-docker-deployment.yaml\napiVersion: apps/v1beta2 kind: Deployment metadata: name: gs-spring-boot-docker-deployment labels: app: gs-spring-boot-docker spec: replicas: 1 selector: matchLabels: app: gs-spring-boot-docker template: metadata: labels: app: gs-spring-boot-docker spec: containers: - name: gs-spring-boot-docker image: dtlabs/gs-spring-boot-docker:1.0 ports: - containerPort: 8080 imagePullPolicy: Always  Kubernetes 환경에 애플리케이션을 배포하기 위해, Service YAML 파일을 작성합니다.  gs-spring-boot-docker-service.yaml\napiVersion: v1 kind: Service metadata: name: gs-spring-boot-docker-service spec: ports: - name: http port: 8081 targetPort: 8080 selector: app: gs-spring-boot-docker type: NodePort kubectl apply 명령어로 애플리케이션을 배포합니다.\nkubectl apply\n$ kubectl apply -f gs-spring-boot-docker-deployment.yaml -f gs-spring-boot-docker-service.yaml deployment \u0026#34;gs-spring-boot-docker\u0026#34; created service \u0026#34;gs-spring-boot-docker\u0026#34; created  kubectl get 명령어를 입력해 Pod, Service, Deployment가 제대로 생성되었는지 확인합니다.  kubectl get\n$ kubectl get po,svc,deploy NAME READY STATUS RESTARTS AGE po/gs-spring-boot-docker-3520417772-sqssr 1/1 Running 0 33s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/gs-spring-boot-docker NodePort 10.0.0.213 \u0026lt;none\u0026gt; 8080:\u0026lt;SERVICE_PORT\u0026gt;/TCP 27s NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/gs-spring-boot-docker 1 1 1 1 33s  Ingress 생성 배포된 애플리케이션으로 요청이 라우팅될 수 있도록 Ingress를 생성합니다.\n Ingress를 생성하기 위해, 다음과 같이 YAML를 작성합니다. gs-spring-boot-docker-ingress.yaml\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: gs-spring-boot-docker-ingress annotations: ingress.kubernetes.io/rewrite-target: / spec: rules: - host: \u0026lt;HOST_NAME\u0026gt;.\u0026lt;DOMAIN_NAME\u0026gt; http: paths: - path: /hello-world backend: serviceName: gs-spring-boot-docker-service servicePort: 8080 line1 apiVersion API Server에서 관리되는 API 버전을 나타냅니다. 사용자가 입력한 apiVersion에 맞는 API를 사용하게 됩니다. Kubernetes API는 실험 단계의 API를 \u0026lsquo;beta\u0026rsquo; 형태로 지원하고, 지속 업데이트 하고 있습니다. 따라서 Kubernetes API 공식문서를 통해 현재 사용자의 Kubernetes 버전 별 호환 및 사용 가능한 API를 확인 후 사용해야 합니다.\nline2 kind 현재 yaml이 어떤 object를 생성하기 위함인지 kind에 설정합니다. kind: Ingress 설정을 통해 현재 yaml로 Ingress object를 생성하게 됩니다.\nline3 metadata Ingress object 자신의 고유 정보를 입력합니다.\nline4 metadata.name Ingress 대한 Unique-key를 입력합니다. 이 name 값을 통해 여러 object 중 해당 name을 갖는 object를 조회할 수 있습니다.\nline6 metadata.annotations.ingress.kubernetes.io/rewrite-target Ingress의 기본 경로를 / 로 주지 않으면 자동으로 path를 구성하므로 원하는 경로로 지정하기 위해서는 /로 구성해야 합니다.\nline7 spec Ingress 수행하는 내용에 대한 설정 입니다.\nline9 spec.rules.host Ingress에 지정할 호스트입니다. Ingress를 사용하기 위해서는, 먼저 DNS Server가 구성되어 있어야 합니다. Domain Name과 Host Name이 조합되어 DNS Server로 호스팅 됩니다. 예를 들어 Kubernetes에 구성된 도메인이 zcp.mybluemix.net 이고, Ingress에 붙일 특정 호스트명은 cloudz-labs 라고 하면 cloudz-labs.zcp.mybluemix.net 으로 호스트를 지정하면 됩니다.\nline11 spec.rules.http.paths 라우팅할 서비스를 설정하는 부분입니다.\nline12 spec.rules.http.paths.path 라우팅할 서비스에 대한 path를 설정합니다. 앞에서 지정한 host 하위로 붙는 경로입니다. 사용자는 앞에서 지정한 호스트 하위에 해당 path를 호출하면 원하는 서비스로 라우팅이 가능합니다.\nline14 spec.rules.http.paths.backend.serviceName 실제로 라우팅할 서비스의 이름을 지정하는 부분입니다.\nline15 spec.rules.http.paths.backend.servicePort 실제로 라우팅할 서비스가 사용하는 포트를 지정하는 부분입니다.\n kubectl apply 명령어를 입력해 Ingress를 생성합니다.  kubectl apply\n$ kubectl apply -f gs-spring-boot-docker-ingress.yaml ingress \u0026#34;gs-spring-boot-docker-ingress\u0026#34; created   Ingress 확인  kubectl get 명령어로 생성된 Ingress의 목록을 확인합니다.\nkubectl get\n$ kubectl get ing NAME HOSTS ADDRESS PORTS AGE gs-spring-boot-docker-ingress \u0026lt;HOST_NAME\u0026gt;.\u0026lt;DOMAIN_NAME\u0026gt; 80 5s  curl 명령어로 Ingress에 설정된 URL로 접속했을 때, 배포된 애플리케이션으로 라우팅되는지 확인합니다. 애플리케이션을 직접 호출했을 때와 결과가 같은 것을 확인할 수 있습니다.  curl\n$ curl \u0026#34;http://\u0026lt;HOST_NAME\u0026gt;.\u0026lt;DOMAIN_NAME\u0026gt;/hello-world\u0026#34; Hello Docker World  Ingress 삭제 더 이상 Ingress를 사용하지 않는다면 Ingress를 삭제합니다.\n kubectl delete 명령어로 Ingress Netrowk를 삭제합니다.\nkubectl delete\n$ kubectl delete -f gs-spring-boot-docker-ingress.yaml ingress \u0026#34;gs-spring-boot-docker-ingress\u0026#34; deleted  curl 명령어로 Ingress가 삭제되었는지 확인합니다.  curl\n$ curl \u0026#34;http://\u0026lt;HOST_NAME\u0026gt;.\u0026lt;DOMAIN_NAME\u0026gt;/hello-world\u0026#34; default backend - 404  "
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/kubernetes/backingservice/",
        "title": "[Kubernetes 활용(4/8)] Mysql DB 연동하기",
        "tags": ["kubernetes", "container", "container orchestration", "서비스 연동", "db 연동", "service discovery", "k8s"],
        "description": "",
        "content": " 지난 챕터에서는 사용한 리소스를 기반으로 애플리케이션의 수를 자동으로 조절할 수 있는 HPA 라는 기능을 적용해 보았습니다. 그렇다면 애플리케이션에 Mysql 또는 Redis 등과 같은 서비스를 연동하고 싶을 땐 어떻게 해야할까요? Kubernetes에서는 애플리케이션에서 필요한 서비스를 Docker Image를 사용하여 바로 구성할 수 있습니다. 물론 대부분이 오픈소스 솔루션에 대한 서비스겠지요. Legacy에 있는 서비스들 역시 연동이 가능하긴 하지만, 여기서는 Docker Image 를 통해 Mysql DB를 구성하고 애플리케이션에 연동해보도록 하겠습니다.\n샘플 애플리케이션에 대한 자세한 설명은 Spring의 Accessing data with MySQL 문서를 참고하시기 바랍니다.\n서비스 사용 워크플로우  Kubernetes에서 사용할 수 있는 서비스(Mysql, Redis 등등)는 Docker Store에서 공식 Image를 Pull받아 구성할 수 있습니다. 사용할 서비스의 Image를 선택한 후, 서비스를 생성합니다. 생성한 서비스를 사용하기를 원하는 애플리케이션에 서비스 연결 정보를 입력한 후, 동일한 Namespace에 애플리케이션을 배포하면 됩니다. 해당 Namespace에서 더 이상 서비스 인스턴스를 사용하지 않는다면 서비스를 삭제합니다.  아래에서 서비스 워크플로우 단계별로 자세히 설명합니다. 서비스 사용방법 서비스 생성 Kubernetes에서 서비스를 사용하기 위해 Docker Image를 통해 서비스를 생성합니다.\n Kubernetes 환경에서 사용할 Mysql DB 이미지를 검색합니다. Deployment YAML 파일 을 작성합니다.(spec.image 항목에 검색한 Mysql DB Image 명을 적용)\napiVersion: apps/v1beta2 kind: Deployment metadata: name: gs-mysql-data-deployment labels: app: gs-mysql-data spec: replicas: 1 selector: matchLabels: app: gs-mysql-data template: metadata: labels: app: gs-mysql-data spec: containers: - env: - name: MYSQL_DATABASE value: db_example - name: MYSQL_PASSWORD value: ThePassword - name: MYSQL_ROOT_PASSWORD value: root - name: MYSQL_USER value: springuser image: mysql imagePullPolicy: IfNotPresent name: gs-mysql-data-container ports: - containerPort: 3306 restartPolicy: Always status: {}  Service YAML 파일을 작성합니다.  apiVersion: v1 kind: Service metadata: name: gs-mysql-data spec: ports: - name: \u0026#34;3306\u0026#34; port: 3306 targetPort: 3306 selector: app: gs-mysql-data type: NodePort Deployment와 Service Object를 생성합니다.\n$ kubectl apply -f gs-mysql-data-deployment.yaml -f gs-mysql-data-service.yaml deployment \u0026#34;gs-mysql-data\u0026#34; created service \u0026#34;gs-mysql-data\u0026#34; created$ kubectl get pod,svc,deployment NAME READY STATUS RESTARTS AGE po/gs-mysql-data-1903746765-26mlf 1/1 Running 0 8s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/gs-mysql-data NodePort 10.0.0.19 \u0026lt;none\u0026gt; 3306:\u0026lt;DB_PORT\u0026gt;/TCP 8s NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/gs-mysql-data 1 1 1 1 8s kubectl exec 명령어를 입력해 생성된 MySQL에 springuser/ThePassword로 접속하여 DB가 정상적으로 동작하는지 확인합니다.\n$ kubectl exec -it gs-mysql-data-1903746765-26mlf bash root@gs-mysql-data-1903746765-26mlf:/# mysql -u springuser -p Enter password: Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MySQL connection id is 3 Server version: 5.7.20 MySQL Community Server (GPL) Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. mysql\u0026gt; \\u db_example Database changed mysql\u0026gt; 서비스 연결 애플리케이션에서 생성한 서비스를 사용하는 방법에 대해서 설명합니다. 생성한 서비스 명으로 애플리케이션과 연동합니다. 아래는 두가지의 서비스 디스커버리 방식에 대한 설명입니다. 해당 샘플에서는 환경변수 방식을 통해 연동해 보도록 하겠습니다.\n서비스 디스커버리 방식  환경 변수 방식 Pod 생성 시 컨테이너 내부에 모든 서비스들의 HOST, PORT 정보가 환경변수로 주입됨 컨테이너에서 환경변수를 통해 다른 서비스에 대한 접속 정보를 알수 있음 redis-master 서비스가 미리 생성되어 있는 경우 , 애플리케이션을 Pod로 띄우면 내부에 아래와 같은 네이밍 룰에 따라 환경 변수가 주입됨  DNS 서버 방식 Kubernetes에 DNS 서비스를 구성하고 서비스명으로 접근 예를 들면 호출 url이 redis-master:6379 와 같은 형태 다른 네임스페이스에 있는 서비스도 접근 가능함   애플리케이션의 설정파일에 서비스 정보 작성 아래 예제의 경우 Spring Boot 애플리케이션에서 gs-mysql-data 서비스를 사용하기 위해 application.properties 설정 파일에 작성한 정보입니다.\n  spring.jpa.hibernate.ddl-auto=create spring.datasource.url=jdbc:mysql://${GS_MYSQL_DATA_SERVICE_HOST}:${GS_MYSQL_DATA_SERVICE_PORT}/${MYSQL_DATABASE} spring.datasource.username=${MYSQL_USER} spring.datasource.password=${MYSQL_PASSWORD}   연결할 서비스의 접속정보는 [서비스명]_SERVICE_HOST , [서비스명]_SERVICE_PORT 형태로 기입합니다.  그외의 나머지 Credential 정보는 Container의 환경변수로 설정하고,애플리케이션에서는 주입받아 사용할 수 있도록 합니다.   Image 빌드(애플리케이션 컨테이너화)\n 이미지를 정의하는 Dockerfile을 작성합니다.  FROM openjdk:8-jdk-alpine ADD target/gs-mysql-data-0.1.0.jar app.jar EXPOSE 8080 ENTRYPOINT [\u0026#34;java\u0026#34;,\u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34;,\u0026#34;-jar\u0026#34;,\u0026#34;/app.jar\u0026#34;] 위치는 다음과 같습니다.\n/gs-accessing-data-mysql/complete $ tree . ├── Dockerfile ├── build.gradle ├── docker-compose.yml ├── gradle ├── gradlew ├── gradlew.bat ├── mvnw ├── mvnw.cmd ├── pom.xml ├── src └── target  docker build 명령어를 사용하여, 애플리케이션에 대한 Image를 구성합니다.  /gs-accessing-data-mysql/complete $ docker build -t dtlabs/gs-mysql-data . Sending build context to Docker daemon 28.26MB Step 1/4 : FROM openjdk:8-jdk-alpine 8-jdk-alpine: Pulling from library/openjdk Digest: sha256:388566cc682f59a0019004c2d343dd6c69b83914dc5c458be959271af2761795 Status: Downloaded newer image for openjdk:8-jdk-alpine ---\u0026gt; 3642e636096d Step 2/4 : ADD target/gs-mysql-data-0.1.0.jar app.jar ---\u0026gt; 2419541c05c4 Step 3/4 : EXPOSE 8080 ---\u0026gt; Running in 4b6ef032a843 ---\u0026gt; e866f2e8876e Removing intermediate container 4b6ef032a843 Step 4/4 : ENTRYPOINT java -Djava.security.egd=file:/dev/./urandom -jar /app.jar ---\u0026gt; Running in 824af86ada81 ---\u0026gt; 7bc04f36cf6a Removing intermediate container 824af86ada81 Successfully built 7bc04f36cf6a Successfully tagged dtlabs/gs-mysql-data:latest  Image를 Registry로 업로드 만들어진 Image를 docker push 명령어로 Registry에 Push합니다. 이전에 Push된 Image가 있을 경우, 변경된 Layer만 Push됩니다.\n  /gs-accessing-data-mysql/complete $ docker push dtlabs/gs-mysql-dataㅁ The push refers to a repository [docker.io/dtlabs/gs-mysql-data] 44630b8580cc: Pushed 69cc5717c281: Layer already exists 5b1e27e74327: Layer already exists 04a094fe844e: Layer already exists latest: digest: sha256:c664a46b2341683aebcb8da9ea31e93df3c2225552dc38f494d664cfa39582d1 size: 1159 Kubernetes 환경에 애플리케이션 배포(컨테이너)\n 임의의 폴더를 생성하고 애플리케이션의 Deployment, Service Object 배포를 위한 YAML 파일들을 작성합니다.  gs-mysql-data-app-deployment.yaml\napiVersion: apps/v1beta2 kind: Deployment metadata: name: gs-mysql-data-app-deployment labels: app: gs-mysql-data-app spec: replicas: 1 selector: matchLabels: app: gs-mysql-data-app template: metadata: labels: app: gs-mysql-data-app spec: containers: - env: - name: MYSQL_DATABASE value: db_example - name: MYSQL_PASSWORD value: ThePassword - name: MYSQL_USER value: springuser image: dtlabs/gs-mysql-data imagePullPolicy: IfNotPresent name: gs-mysql-data-app-container ports: - containerPort: 8080 restartPolicy: Always status: {} env 항목에 gs-mysql-data의 연결 정보를 설정합니다.\ngs-mysql-data-app-service.yaml\napiVersion: v1 kind: Service metadata: name: gs-mysql-data-app spec: ports: - name: \u0026#34;8080\u0026#34; port: 8080 targetPort: 8080 selector: app: gs-mysql-data-app type: NodePort 위치는 다음과 같습니다.\n/gs-accessing-data-mysql/complete $ tree . ├── Dockerfile ├── build.gradle ├── docker-compose.yml ├── gradle ├── gradlew ├── gradlew.bat ├── kubernetes │ ├── gs-mysql-data-app-deployment.yaml │ ├── gs-mysql-data-app-service.yaml ├── mvnw ├── mvnw.cmd ├── pom.xml ├── src └── target1  kubernetes 폴더로 이동하여, kubectl apply 명령어로 Kubernetes 환경에 애플리케이션을 배포합니다.  /gs-accessing-data-mysql/complete/kubernetes $ kubectl apply -f gs-mysql-data-app-deployment.yaml -f gs-mysql-data-app-service.yaml deployment \u0026#34;gs-mysql-data-app\u0026#34; created service \u0026#34;gs-mysql-data-app\u0026#34; created   kubectl get 명령어를 입력해 Pod, Service, Deployment가 제대로 생성되었는지 확인합니다.\n/gs-accessing-data-mysql/complete/kubernetes $ kubectl get po,svc,deploy NAME READY STATUS RESTARTS AGE po/gs-mysql-data-1903746765-26mlf 1/1 Running 0 22m po/gs-mysql-data-app-1364076206-0x3lp 1/1 Running 0 12m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/gs-mysql-data NodePort 10.0.0.19 \u0026lt;none\u0026gt; 3306:\u0026lt;DB_PORT\u0026gt;/TCP 22m svc/gs-mysql-data-app NodePort 10.0.0.25 \u0026lt;none\u0026gt; 8080:\u0026lt;EXTERNAL_PORT\u0026gt;/TCP 12m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/gs-mysql-data 1 1 1 1 22m deploy/gs-mysql-data-app 1 1 1 1 12m 서비스 연결 확인 컨테이너로 배포된 애플리케이션이 정상적으로 연결되는지 확인해보겠습니다.\n 먼저 애플리케이션의 Pod 이름을 확인하고, kubectl describe 명령어로 생성된 Pod의 상세정보를 확인합니다.   /gs-accessing-data-mysql/complete/kubernetes $ kubectl get pod NAME READY STATUS RESTARTS AGE gs-mysql-data-1903746765-26mlf 1/1 Running 0 25m gs-mysql-data-app-1364076206-0x3lp 1/1 Running 0 15m /gs-accessing-data-mysql/complete/kubernetes $ kubectl describe pod gs-mysql-data-app-1364076206-0x3lp Name: gs-mysql-data-app-1364076206-0x3lp Namespace: default Node: poc.k8s-worker02.cloudz.co.kr/\u0026lt;EXTERNAL_HOST\u0026gt; ... 이하 생략 ...  kubectl get 명령어로 Service 목록을 확인합니다. PORT(s) 항목에서 gs-mysql-data-app의 내부 Port는 8080, 외부 Port는 EXTERNAL_PORT로 설정되어 있는 것을 확인할 수 있습니다.  /gs-accessing-data-mysql/complete/kubernetes $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gs-mysql-data NodePort 10.0.0.19 \u0026lt;none\u0026gt; 3306:\u0026lt;DB_PORT\u0026gt;/TCP 30m gs-mysql-data-app NodePort 10.0.0.25 \u0026lt;none\u0026gt; 8080:\u0026lt;EXTERNAL_PORT\u0026gt;/TCP 20m  curl 명령어로 gs-mysql-data-app 애플리케이션이 MySQL 서비스와 연동하여 정상적으로 동작하는지 확인합니다.  /gs-accessing-data-mysql/complete/kubernetes $ curl \u0026#39;http://\u0026lt;EXTERNAL_HOST\u0026gt;:\u0026lt;EXTERNAL_PORT\u0026gt;/demo/all\u0026#39; [] /gs-accessing-data-mysql/complete/kubernetes $ curl \u0026#39;http://\u0026lt;EXTERNAL_HOST\u0026gt;:\u0026lt;EXTERNAL_PORT\u0026gt;/demo/add?name=First\u0026amp;email=someemail@someemailprovider.com\u0026#39; Saved /gs-accessing-data-mysql/complete/kubernetes $ curl \u0026#39;http://\u0026lt;EXTERNAL_HOST\u0026gt;:\u0026lt;EXTERNAL_PORT\u0026gt;/demo/all\u0026#39; [{\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;First\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;someemail@someemailprovider.com\u0026#34;}]  서비스 삭제 더 이상 서비스를 사용하지 않는다면 Deployment와 Service를 삭제합니다.\n Kubernetes 설정 파일이 있는 폴더로 이동하여, kubectl delete 명령어로 Deployment와 Service를 삭제합니다.\n$ kubectl delete -f gs-mysql-data-deployment.yaml -f gs-mysql-data-service.yaml deployment \u0026#34;gs-mysql-data\u0026#34; deleted service \u0026#34;gs-mysql-data\u0026#34; deleted  "
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/kubernetes/mariadb-utf8/",
        "title": "Kubernetes에 구성한 MariaDB(Mysql)의 한글 깨짐 현상 해결방법",
        "tags": ["kubernetes", "mariadb"],
        "description": "Kubernetes 환경에서 MariaDB(Mysql)의 컨테이너를 구성할 때 한글이 깨지는 현상을 해결해봅시다.",
        "content": " Why? Kubernetes(a.k.a. K8S)에서 Mysql 또는 Mariadb 이미지를 사용해서 컨테이너를 구성할 때 initialize된 Data에 한글이 깨지는 현상이 발생하는 경우가 있습니다.\n구글에서 \u0026ldquo;mysql 한글 깨짐\u0026rdquo;이라고 검색만 해도 같은 문제를 호소하는 분들이 많고, 이를 해결하기 위한 다양한 해결 방법을 가이드하고 있습니다.\n하지만 Docker 또는 Kubernetes 환경에서는 대부분이 이미 업로드된 official 이미지를 사용해서 컨테이너를 구성하기 때문에 이를 처리하는데 약간의 수고로움 존재합니다.\n이번 세션에서는 K8S 환경에서 mysql(mariadb) 을 구성할 때 한글 깨짐 증상을 해결하고 더 나아가 설정을 자유롭게 할 수 있는 내용을 적어보았습니다.\n한글 깨짐 현상 먼저 mariadb를 이용해서 DB를 배포해보도록 하겠습니다.\n아래 설정은 Kubernetes환경으로 배포하기 위한 yaml 파일입니다.\napiVersion: v1 kind: Service metadata: name: mariadb spec: selector: app: mariadb ports: - name: mariadb port: 3306 targetPort: 3306 --- apiVersion: apps/v1beta2 kind: Deployment\t# 1.9 version 이상에서는 stable된 StatefulSet사용을 권장 metadata: name: mariadb-deployment labels: app: mariadb spec: replicas: 1 selector: matchLabels: app: mariadb template: metadata: labels: app: mariadb spec: containers: - name: mariadb env: - name: MYSQL_ROOT_PASSWORD # root계정의 패스워드 value: root - name: MYSQL_DATABASE # 구성할 database명 value: database - name: MYSQL_USER # database에 권한이 있는 user value: user - name: MYSQL_PASSWORD # database에 권한이 있는 user의 패스워드 value: password image: mariadb imagePullPolicy: IfNotPresent # 이미지를 pull받는 규칙을 지정(Always / IfNotPresent / Never) ports: - containerPort: 3306 restartPolicy: Always status: {} 작성된 위 파일을 가지고 Kubernets 환경으로 배포해보겠습니다.\n$ kubectl apply -f mariadb.yaml service \u0026#34;mariadb\u0026#34; created deployment.apps \u0026#34;mariadb-deployment\u0026#34; created 생성된 DB에 테이블을 생성하고 데이터를 집어넣습니다.\nCREATE TABLE `contents` ( `id` int NOT NULL AUTO_INCREMENT, `category` varchar(255) DEFAULT NULL, `grade` double NOT NULL, `has_episodes` bit(1) NOT NULL, `poster` varchar(255) DEFAULT NULL, `rate` varchar(255) DEFAULT NULL, `reg_date` varchar(255) DEFAULT NULL, `runtime` int(11) NOT NULL, `stillcut` varchar(255) DEFAULT NULL, `summary` varchar(255) DEFAULT NULL, `title` varchar(255) DEFAULT NULL, `video` varchar(255) DEFAULT NULL, `view` int(11) NOT NULL, `year` int(11) NOT NULL, PRIMARY KEY (`id`) ); INSERT INTO `contents` (`id`, `category`, `grade`, `has_episodes`, `poster`, `rate`, `reg_date`, `runtime`, `stillcut`, `summary`, `title`, `video`, `view`, `year`) VALUES (\u0026#39;62\u0026#39;,\u0026#39;action\u0026#39;,2,b\u0026#39;0\u0026#39;,\u0026#39;https://art-s.nflximg.net/09934/245a98e19d095c5b662e4ef469aec5d35db09934.webp\u0026#39;,\u0026#39;12\u0026#39;,\u0026#39;2015-11-11\u0026#39;,131,\u0026#39;http://awesomewallpaper.files.wordpress.com/2013/08/pacific-rim-wallpapers-7.jpg\u0026#39;,\u0026#39;바다 외계 생명체에 맞선 세계 동맹이 실패로 끝나면서 전직 파일럿과 새내기 파일럿이 힘을 합쳐 지구를 구하려 한다.\u0026#39;,\u0026#39;퍼시픽 림\u0026#39;,\u0026#39;i0ZJ7BUkUco\u0026#39;,69,2013); 위 데이터에는 한글이 포함되어 있습니다.\n하지만 위 같은 설정으로는 한글을 사용할 수 없습니다. (한글을 세계 표준으로\u0026hellip;)\nAPI호출 결과\n위는 사용될 title, summary 컬럼에서 한글이 깨져있는 현상입니다.\n해당 데이터베이스의 현재 character를 살펴 보겠습니다.\nMariaDB [(none)]\u0026gt; show variables like \u0026#39;c%\u0026#39;; +--------------------------+----------------------------+ | Variable_name | Value | +--------------------------+----------------------------+ | character_set_client | latin1 | | character_set_connection | latin1 | | character_set_database | latin1 | | character_set_filesystem | binary | | character_set_results | latin1 | | character_set_server | latin1 | | character_set_system | utf8 | | character_sets_dir | /usr/share/mysql/charsets/ | | check_constraint_checks | ON | | collation_connection | latin1_swedish_ci | | collation_database | latin1_swedish_ci | | collation_server | latin1_swedish_ci | | completion_type | NO_CHAIN | | concurrent_insert | ALWAYS | | connect_timeout | 5 | +--------------------------+----------------------------+ 한글 깨짐 현상은 mysql(mariadb)의 공식 이미지의 디폴트 언어 값이 latin1으로 되어있기 때문에 발생하는 증상입니다.\n그. 래. 서!\nDocker Hub에 존재하는 mariadb 이미지에 들어가봤습니다. (https://hub.docker.com/_/mariadb/)\n어떤 설정을 해야된다라는 내용이 Docker 관점으로 영어로 써있습니다.\n그 중에서 아래와 같은 내용이 있습니다.\nConfiguration without a cnf file\n Many configuration options can be passed as flags to mysqld. This will give you the flexibility to customize the container without needing a cnf file.\nFor example, if you want to change the default encoding and collation for all tables to use UTF-8 (utf8mb4) just run the following:\n$ docker run --name some-mariadb -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mariadb:tag --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci If you would like to see a complete list of available options, just run:\n$ docker run -it --rm mariadb:tag --verbose --help docker run을 실행할 때 argument 값으로 UTF-8을 설정할 수 있다라는 내용입니다. 이것을 Kubernetes에서 설정을 넣어야하는데 설정이 익숙하지 않으신 분들은 어디에 넣어야할 지 모르실 수 있습니다.\n그 설정은 다음과 같습니다.\nkubernetes 환경에서 DB구성에 대한 샘플 yaml파일 apiVersion: v1 kind: Service metadata: name: mariadb spec: selector: app: mariadb ports: - name: mariadb port: 3306 targetPort: 3306 --- apiVersion: apps/v1beta2 kind: Deployment metadata: name: mariadb-deployment labels: app: mariadb spec: replicas: 1 selector: matchLabels: app: mariadb template: metadata: labels: app: mariadb spec: containers: - name: mariadb env: - name: MYSQL_ROOT_PASSWORD value: root - name: MYSQL_DATABASE value: database - name: MYSQL_USER value: user - name: MYSQL_PASSWORD value: password image: mariadb imagePullPolicy: IfNotPresent args: - \u0026#34;--character-set-server=utf8mb4\u0026#34; # default Character Set 을 지정하는 argument - \u0026#34;--collation-server=utf8mb4_unicode_ci\u0026#34; # 문자열 비교 규칙 _ci(case insensitive) / _cs(case sensitive) / _bin(binary) ports: - containerPort: 3306 restartPolicy: Always status: {} 위와 같은 설정을 넣어주면 해당되는 mysqld 명령으로 argument를 설정하는 것에 포함되는 부분입니다.\n설정이 잘 되어있는지 확인해보겠습니다.\nMariaDB [(none)]\u0026gt; show variables like \u0026#39;c%\u0026#39;; +--------------------------+----------------------------+ | Variable_name | Value | +--------------------------+----------------------------+ | character_set_client | latin1 | | character_set_connection | latin1 | | character_set_database | utf8mb4 | | character_set_filesystem | binary | | character_set_results | latin1 | | character_set_server | utf8mb4 | | character_set_system | utf8 | | character_sets_dir | /usr/share/mysql/charsets/ | | check_constraint_checks | ON | | collation_connection | latin1_swedish_ci | | collation_database | utf8mb4_unicode_ci | | collation_server | utf8mb4_unicode_ci | | completion_type | NO_CHAIN | | concurrent_insert | ALWAYS | | connect_timeout | 5 | +--------------------------+----------------------------+ 잘 되었네요!!\n실제 데이터를 넣어서 확인해본 결과는 다음과 같습니다.\n다음시간에는 mariadb 전체 설정을 바꿀 수 있는 my.cnf 파일을 작성해서 Kubernetes 환경으로 배포하는 세션을 다루어보도록 하겠습니다.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/istio/",
        "title": "Istio",
        "tags": ["istio", "msa", "service mesh", "spring cloud netflix"],
        "description": "",
        "content": " 최근 MSA(Micro Service Architecture) 관련 세미나, 자료 등을 살펴보면 Istio라는 키워드가 급부상하고 있는 것을 알 수 있습니다. Istio가 무엇인지 조사한 결과를 공유합니다.\nService Mesh Architecture Istio를 설명하기 전에 Service Mesh Architecture 가 무엇인지 알아보겠습니다.\nMSA(Micro Service Architecture)가 널리 보급되면서 service discovery, routing, failure handling 등 서비스 간 통신 문제가 발생하고 있습니다. 이러한 문제를 해결하기 위해 Spring Cloud Netflix 등 다양한 해결법이 등장하였고, Service Mesh Architecture 또한 이러한 해결 방법 중 하나입니다. Service Mesh Architecture란 MSA에서 서비스 간 통신 방법을 구체화한 아키텍처로 다음과 같은 구조를 가집니다.\n 구조  각 서비스 앞에 Network Layer를 Sidecar로 위치시켜 서비스의 인/아웃 트래픽을 컨트롤한다. 모든 Network Layer를 컨트롤하는 Service Mesh Control Plane을 사용한다.  동작 방식  Control Plane에 서비스의 정보, 라우팅 정보 등을 등록한다. Control Plane은 등록된 정보를 Network Layer(sidecar)에 전달한다. Network Layer(sidecar)은 전달받은 정보를 이용하여 서비스 간 통신을 제어한다.  통신 흐름: MicroService1 -\u0026gt; Sidecar1 -\u0026gt; Sidecar2 -\u0026gt; MicroService2    이해를 돕기 위해 Docker에의 Service Mesh Architecture 이미지를 첨부합니다.\nIstio란? Service Mesh Architecture는 말 그대로 개발자가 직접 구성해야 할 아키텍처입니다. Istio란 Google, IBM이 Service Mesh Architecture 구현해 놓은 플랫폼으로 이를 활용하여 쉽게 Service Mesh Architecture를 적용할 수 있습니다. Istio는 Service Mesh Architecture의 구현 체기 때문에 구조, 특징, 동작 방식이 매우 유사합니다.\n 구조  Envoy: Service 앞단의 Network Layer로 서비스의 인/아웃 트래픽을 컨트롤한다. Control Plane: 모든 Network Layer를 컨트롤한다.  Pliot: Routing, Service Discovery 등 서비스 간 통신 기능을 담당 Mixer: Metric, Monitoring 등의 기능을 담당 Istio-Auth: 서비스 간 인증을 담당(TLS, Auth 등)   동작 방식  Control Plane에 서비스 정보, 라우팅 정보 등을 등록한다. Control Plane은 등록된 정보를 Envoy에 전달한다. Envoy 전달받은 정보를 이용하여 서비스 간 통신을 제어한다.  통신 흐름: ServiceA -\u0026gt; EnvoyA-\u0026gt; EnvoyB -\u0026gt; ServiceB   주요기능  Service Discovery Fault Injection Circuit Breaking Load Balancing  인스턴스 별로 되는 트래픽양 조절가능(Canary, Bluegreen에 활용가능)  MSA통합된 Metric, Log 수집가능 MSA간 인증, TLS등   마치며 Istio는 Spring Cloud Netflix의 기능을 총집합 해둔 것으로 이해하면 됩니다. 실제로도 많은 사람들이 Istio와 Spring Cloud Netflix를 비교합니다. Spring Cloud Netflix는 애플리케이션 소스를 작성하여 사용하기 때문에 개발 언어, 유지 보수등에 많은 문제가 있지만 Istio는 플랫폼 영역에서 동작하기 때문에 개발 언어에 무관하게 동작이 가능하며 유지 보수도 쉽다는 장점을 갖고 있습니다.\nIstio와 Spring Cloud Netflix를 직접 사용해본 결과 확실히 Istio가 개발/운영이 편했습니다. 다만 아직은 인큐베이팅 정도의 기술이며(최신버전 0.8), 레퍼런스가 부족하다는 점 등을 고려해야 할 것으로 보입니다.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/digital-transformation-journey/",
        "title": "Digital Transformation Journey",
        "tags": ["digital transformation", "docker", "kubernetes", "istio", "citizen developer", "masa", "event driven", "microservices", "hybrid application platform"],
        "description": "",
        "content": " Software is \u0026ldquo;still\u0026rdquo; eating the world  \u0026ldquo;소프트웨어가 세상을 먹어치우고 있다\u0026hellip;여전히!\u0026rdquo;\n \u0026ldquo;Software is eating the world\u0026rdquo; 는 Marc Andreessen이 아주 오래전에 Wall Street Journal에 올렸던 기고입니다. 그 이후 세상은 호텔을 하나도 소유하지 않은 에어비엔비가 숙박업을, 영화관을 하나도 소유하지 않은 넷플릭스가 미디어 산업을, 오프라인 상점을 하나도 소유하지 않았던(지금은 있지만) 아마존이 리테일 산업을 장악하는 세상으로 바뀌어 버렸습니다.\n중요한 것은 \u0026ldquo;아직도\u0026rdquo; 먹어치우고 있다는 것입니다.\n사실 몇 년 전만 해도 인터넷 서비스, 미디어, 통신, 리테일 외의 전통 산업에서 Digital Disruption은 올 듯 올 듯 하면서 동인이 잘 생기지 않았습니다. 잘 되는 기존 산업을 카니발라이즈하면서 Disruptive한 비즈니스를 굳이 만들 필요가 없을 테니까요. 즉, 모든 산업 도메인이 크리스텐슨 교수의 \u0026ldquo;파괴적 혁신\u0026rdquo; 범주에 들어갈 동인이 없었습니다.\n그러나 우리는 Nvidia의 사례에서 무엇인가 힌트를 얻을 수 있습니다. GPU를 만드는 하드웨어 제조업체가 딥러닝 SDK와 같은 소프트웨어를 만들기 시작한 것입니다. 이는 기존 산업을 Disruption하는 것이 아니라 오히려 기존 산업을 더 활성화할 수 있는 것이지요. 반면에 Nvidia의 GPU Cloud 서비스는 기존 비즈니스의 파괴가 될 수 있습니다. 이는 그래픽 카드를 판매하는 Division에서 그리 달갑지는 않을 것입니다.\n경쟁의 압박과 Commodity화 기술의 발전 속도가 빨라짐에 따라 경쟁의 압박은 더욱 심해집니다. 줄다리기 중 한 쪽이 사람이고 한 쪽이 소프트웨어/AI라면 어느 한 쪽은 더 이상 경쟁력을 잃게 됩니다. 이제는 사람의 힘을 넘어서 소프트웨어 기반, AI 기반으로 경쟁하는 구조로 이미 많이 변화하고 있습니다.\n 누구는 맥주를 나만의 차별화된 것이 필요하다고 생각하고 누구는 그저 생필품이라고 생각한다.(BUD 비하는 아님)\n  Source: https://cloudrumblings.io/a-pioneer-a-settler-and-a-town-planner-walk-into-a-bar-9889d7c8a19e#.jrt2rzvsk [DREW FIRMENT] \n이는 소프트웨어 중심으로 기술 Disruption이 일어나고 있고 이 영역에 핵심 역량이 집중되며, 기존의 인프라 자산은 급격히 Commodity화되고 있다는 것을 의미합니다. 아래 쪽 인프라가 생필품화되지 않으면 그 위에 영역에서 혁신을 가져가기 힘든 이유이기 때문이지요. 마치 매슬로우의 욕구단계설이 생각나기도 합니다. 기본의 위생/안전 욕구가 충족되지 않으면 궁극적인 자아실현 욕구에 도달할 수 없는 것이 그것입니다. Wardly의 Value Chain Mapping에서도 알 수 있듯이 무엇을 Commodity(Utility)로 보고, 무엇을 빌리며 어디에 집중할까라는 전략맵은 점점 더 중요해지고 있습니다.\n비즈니스의 변화 한 때 AI Singularity에 관한 미래학자 커즈와일의 관련 글을 보고 공포심이 생겼을 때가 있었습니다. 물론 수십년 후에는 AI가 인간의 두뇌 능력을 상상할 수도 없이 초월해 세상을 지배하게 될 지도 모르지만, 여기서는 현재와 3~5년 정도의 Tipping Point를 생각해서 이미 발생하고 있는 산업 내의 현상에 집중하고자 합니다.\nNetflix에서 판매되는 VoD의 80% 이상이 Recommendation 엔진이 추천해준 컨텐츠라고 합니다. Netflix의 개발자들은 개인 성향을 분석할 수 있는 정보를 수집하는 프로그램과 이를 중앙의 인텔리전스에 전송하는 프로그램, 그리고 그 결과를 다시 사용자에게 전송하는 프로그램에 집중하고 있을 것입니다. 상상이지만 이전에는 VoD 판매 현황 통계를 보여주는 프로그램, 그것을 사람이 관리할 수 있도록 도와주는 프로그램을 만들고 있었을 것입니다.\n 주사위는 (기계에 의해) 던져졌다.\n 제조나 기타 산업도 마찬가지입니다. 제조의 불량률 관리나, 물류의 최적화 등에서도 기존의 IT시스템의 라이프사이클은 변화될 것입니다. 기존의 IT는 Control로서의 IT, 즉 \u0026ldquo;정보수집-사람이 판단할 수 있는 정보제공-사람이 판단-시스템에 적용\u0026rdquo;하는 라이프사이클입니다.그러나 Enabler로서의 IT는 라이프사이클은 아래와 같을 것입니다.\n\u0026ldquo;정보를 한 곳에 모음 - 인텔리전스가 학습 - 추론을 서비스 형태로 사용\u0026rdquo;\n아키텍처의 변화 애플리케이션 아키텍처의 변화 관련하여 EDA(Event Driven Architecture)가 각광받고 있습니다. 성공적인 비즈니스 성과가 나오게 하기 위해 각 이벤트(System, Customer, IoT, AI)를 모두 비즈니스 기회라고 생각하고 이것을 실시간으로 의사결정 내릴 수 있는 이벤트 기반 아키텍처입니다. 다만 이벤트 기반이라 해서 모두 동일한 패턴은 아닐 수 있고 좀 더 엄밀히 구분하여 목적에 맞는 아키텍처를 선택해야 할 것입니다. 마틴 파울러 - What do you mean by “Event-Driven”?\n \u0026ldquo;난 그저 매달려 기다리고 있을테니, 내가 넓게 펴질 수록 바람이 세게 분다고 생각해.\u0026rdquo;\n 그것과 다른 아키텍처는 요청(Request) 기반 아키텍처입니다. 요청 기반 아키텍처에서는 데이터가 중심에 있고 데이터에 대한 접근을 REST등의 접근 경로를 통해 접근하는 것이 기본 골격입니다.\nAI 스피커로부터 받는 음성, IoT 장비의 실시간 이벤트, 보안 이벤트, 공장의 실시간 운영 데이터, 항공 트래픽 제어 이벤트 등 이벤트 기반 비즈니스들은 요청 기반 아키텍처보다는 메시지 큐 기반의 Pub/Sub 아키텍처가 적합합니다.\n이벤트를 처리하는 방법은 전형적인 Pub/sub 미들웨어(Kafka, RabbitMQ등의 큐)로 처리하거나 서버리스의 FaaS(Function as a Service), Bus 등 다양한 방법이 있을 수 있고, 데이터의 스트리밍 분석 처리를 위한 Spark와 같은 형태도 이벤트의 처리 방식 중 하나가 될 수 있습니다.\n하지만 모든 애플리케이션이 EDA가 되어야 하는 것은 아닙니다. 이벤트의 끝자락에는 결국 요청 기반 애플리케이션이 존재할 수 있고, 또한 이벤트와 무관한 presisitent한 시스템은 요청 기반 아키텍처로 존재하는 것이 합리적일 것입니다.\nMASA(Mesh App and Service Architecture)이라고 하는 용어는, 수 많은 앱과 서비스가 상호 간에 연결된 환경을 표현하기 적절한 용어입니다. 마이크로서비스(microservice) 아키텍처로 구성된 작은 단위의 앱들이 구동되는 시스템을 서비스 매쉬 레이어에서 오케스트레이션 하는 구조입니다.\n다만 모든 것이 마이크로서비스화되는 것은 아니고, 서비스 목적에 맞는 granularity에 따라,\n Macroservice Miniservice Microservice Nanoservice  등으로 분화될 것입니다.\n 피라미드를 모래로 짓는 것은 어리석을 수 있고, 해변가를 큰 돌로 채울 수는 없다.\n 각 앱들은 PolyGlot(다양한 프로그래밍 언어와 기술 스택)으로 구현될 수 있습니다. Java/Spring Boot, Python/Tensorflow, nodejs, C# 등 비즈니스 애플리케이션, 데이터 분석, 웹 애플리케이션, 게임 등 각 워크로드 성격에 맞는 기술 스택 구조를 선택하고 이를 외적인 아키텍처(outer architecture)가 오케스트레이션하는 방식이 적절합니다. Istio와 같이 애플리케이션 내에 영향을 주지 않고 Sidecar 방식으로 작동하는 기제가 적합한 구조가 될 것입니다.\n인프라스트럭처의 변화 이제 애플리케이션은 준비 되었습니다. 남는 것은 배포에 대한 의사결정입니다. 서비스를 즉시 개선하면 얻을 수 있는 효과는 인텔리전스가 판단할 수 있고, 이를 1000대의 서버에 즉시 배포했을 때 장애에 대한 Risk와 비교하여 이를 실행할 지 의사결정이 남게 됩니다.\n인텔리전스에서의 학습과 성장의 속도는 빠를 것입니다. 기존의 IT 교체 사이클 주기로는 이것을 따라가기 어려울 수 있습니다. 최적화에 필요한 소프트웨어를 적시적소에 제공하기 위해서는 유연한 인프라와 언제든 버릴 수 있고(disposable) 변경 가능한 애플리케이션 형태로 진화되어야 합니다. 또한 인프라도 수동으로 구성하는 대신 스크립트 등을 사용하여 마치 소프트웨어를 프로그래밍하는 것처럼 처리해야 합니다.(Infrastructure as Code). Infrastructure as Code는 DevOps의 Key가 되는 속성 중 하나입니다.\n이렇게 되면 애플리케이션의 수정과 인프라스트럭쳐의 작업 사이의 경계가 모호해지고, 애플리케이션 개선 주기는 빨라지게 됩니다. 조작 가능한(Composable) Infrastructure가 필요한 것이고, 이에 가장 보편적인 기술은 Docker입니다. 애플리케이션과 인프라를 컨테이너화해서 마치 인프라를 Software처럼 다루게 됩니다.\n 컨테이너 오케스트레이터계를 평정한 Kubernetes\n Docker의 Orchestrator로서 Docker Swarm, Rancher, Kubernetes 등 여러 오픈소스들이 각축적을 벌이다가 작년 하반기부로 Kubernetes로 Consolidation된 것은 자명한 사실입니다. 2017년 말에 CNCF는 Certified Kuberenetes Product들을 발표하였고 여기에는 Cloud Foundry와 같이 마치 Kuberenetes의 경쟁으로 보였던 플랫폼들도 놀랍게도 그곳에 라인업되어 있었습니다. Cloud Foundry는 원래 있던 PaaS 기능을 CFAR(CF Application Runtime)으로 재그룹핑하였고 Kuberenetes를 포함시키는 별도 라인업을 CFCR(Cloud Foundry Container Runtime)으로 명명하였습니다.\nPeople의 변화: 시민개발자의 부상 인프라스트럭처가 Code화 되고, 주요 비즈니스 판단은 인텔리전스가 처리하게 된다면 많은 LoB(Line of Business) 인원들은 기존의 수작업 대신 자신이 할 일을 개발(develop)하게 될 것입니다. 물론 Control Plane에 가까워질 수록 기존의 전문 개발자들이 필요하겠지만, Business Plain에 가까워질 수록 Low-code 개발자(Function, 스크립트 개발자)와 No-Code 개발자(매크로, BPM, RPA, DL모델러 등)가 많아질 것입니다.\n시민 개발자(Citizen Developer)가 많아질 수록, 기존에 고개를 숙였던 4GL이나 GUI 개발도구, 모델러 등이 다시 필요하게 됩니다. High-Control 플랫폼은 Control Plain에서, High-Productivity 플랫폼은 Business Plain에서 요구될 것입니다.\n기존의 Analyst들은 Data Scientist로, 기존의 Process Manager는 BPM/RPA(Robotics Process Automation) Developer로 바뀌어야 할 수 있습니다. 기존의 정보시스템과 IT전담조직을 매개체로 시스템과 의사소통하던 LoB들이 Business Enabler로서 역할을 가져가야 할 경우에는, 이제는 더 이상 정보시스템이 아닌 직접 시스템과 소통을 해야 할 필요가 있기 때문입니다.\n 누구에게는 전문가용 펜이 필요하고 누구에게는 연필이, 누구에게는 크레파스가 필요하다.\n 시스템의 변화: 다양성 개발자의 유형도 다양하듯이 시스템 측면에서도 모든 워크로드들과 비즈니스들이 동일한 패턴으로, 동일한 공간에서 동작하지는 않을 것입니다. 보안, 성능, 비즈니스 제약사항, 지역성, 서비스 레벨, 비용과 같은 Constraint와 이벤트 기반, 대용량 배치 처리, 스트리밍 등 워크로드의 성격에 따라 각 시스템들은 그 시스템이 있어야 할 최적의 위치와 형태에서 동작해야만 합니다. 예를 들면 배포 모델(Private/Public), 호스트 방식(Host, VM, Container, Function), 구현 스택, 아키텍처(EDA vs MSA) 등에 따라 많은 종류의 워크로드들이 많은 공간에 존재할 것입니다.\n물론 논리적으로는 가상화와 API통합을 통해 추상화하는 것이 필요합니다. 단, 각 개별 시스템의 구조는 시스템에 맞게 최적화하는 것이 바람직합니다. 이를 HAP(Hybrid Application Platform)이라고 부를 수 있는데(특정 제품은 아닙니다),\n 구성원: 전문개발자, 시민개발자, UX, 4GL 개발자, 데이터 사이언티스트 등 레이어: System of Record, System of Differenciation, System of Innovation 애플리케이션 유형: 모바일, 데이터 중심, IoT, 트랜잭션 기반, AI 기반 배포 모델: 온프리미스, 클라우드, 하이브리드, 임베디드 등  을 고려한 엔터프라이즈 아키텍처를 고려해야 할 것입니다.\n여정의 수립과 Small start 모든 것이 첫 술에 배부를 수는 없듯이 전체 시스템을 Transformation하는 데는 장기적인 로드맵이 필요합니다. 또한 Journey Map을 세워 각 시스템에 특성에 맞게 여정표를 준비해야 할 것입니다. 시스템별로 아래 예시의 질문들을 거쳐 자신이 도달할 곳으로 가게 되는데 언제 도달할지는 비즈니스 상황을 고려하여 얻을 것(Gain)과 소요되는 비용(Pain)을 따져 보아야 할 것입니다.\n 애플리케이션이 Stateful한가, Stateless한가? 애플리케이션을 완전히 리아키텍처링할 의지가 있는가? 이벤트 드리븐인가? 퍼블릭 클라우드를 사용하기에 문제가 없는가? 컨테이너의 제어권을 어느 정도 가져오고 싶은가?  등 핵심 질문(Key Question)과 여정의 순서도(Flow Chart)에 따라 장기적인 로드맵이 수립될 것입니다.\n하지만, 모든 것을 빅뱅 방식으로 완벽한 계획을 세워 실행에 옮기기보다는 작은 단위로 Small Start로 실행에 옮기는 것을 권장합니다. 빨리 착수하여 만들어진 Demonstration system은 가설 검증 뿐 아니라 학습과 성장, 그리고 장기적인 로드맵에 이정표가 되어 줄 것입니다.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/docker-user-defined-network/docker-user-defined-network-test/",
        "title": "[Docker-User Defined Network 활용(2/3)] Docker User Defined Bridge Network 테스트",
        "tags": ["docker", "user defined network", "spring boot"],
        "description": "Docker User Defined Bridge Network를 Spring Boot Application으로 테스트합니다.",
        "content": " 지난 포스팅에서 docker user defined network의 간단한 설명과 동작을 확인했습니다.\n이번에는 docker user defined network를 활용한 컨테이너 간 통신에 연관된 factors를 확인하고 경우의 수에 따라 테스트를 진행하겠습니다.\nDocker Network 기능 Service Discovery Docker는 각 Network 내 컨테이너의 ip주소, 컨테이너 명, Hostname 등을 관리합니다. 각 네트워크는 Subnet mask와 Gateway를 가집니다. 기본적으로 컨테이너의 ip주소는 Network에서 관리하는 Pool 중 할당이 됩니다. 할당하고 싶은 ip주소, Hostname이나 network 정보가 있으면 option을 적용해 지정 가능합니다. 각 Network 별 Host정보를 관리하는 기능을 사용해서 컨테이너 명으로 컨테이너 간 통신을 하는 Service Discovery 기능을 수행할 수 있습니다.\nPort Expose 대부분의 경우 Docker는 subnet을 사용해서 컨테이너가 외부에 노출되지 않게 생성합니다. 외부에서 컨테이너에 접근하기 위해서 컨테이너 내부 Port를 컨테이너 Host의 Port에 매핑하는 것이 필요합니다. 이는 Docker 컨테이너를 실행시 \u0026ndash;publish 옵션으로 간단히 수행할 수 있습니다. 외부에서 노출된 Port로 접근시 컨테이너 내부 Port로 request가 forwarding 됩니다.\ndocker run -d -p {외부 노출 Port}:{컨테이너 내부 Port} {image 명:tag}\nuser defined network을 사용한 Private Network 구성 컨테이너의 Port를 노출하지 않은 상태에서 user defined network에 연결하여 외부에서 컨테이너에 접근하지 못하지만 컨테이너 간 내부 통신은 가능한 private network를 구성할 수 있습니다. docker run시 \u0026ndash;network option을 지정하면 기본 bridge network인 docker0에 binding되지 않고, 지정한 user defined network에 연결됩니다. 이 상태에서 별도의 Port 노출을 하지 않게 설정하여 private network를 구성할 수 있습니다.\n테스트 Docker의 네트워크 기능과 연관된 3가지 factors를 검증하는 방식으로 테스트를 진행합니다. Application 2개를 준비하고 Application1에서 Application2의 \u0026lsquo;/jisang\u0026rsquo; endpoint를 호출해 결과를 얻어올 수 있는지 확인합니다.\nfactors  호출 Host 정보\n 컨테이너 명 IP주소(docker-machine ip주소)    테스트 환경은 Docker for Windows입니다.\nVirtual Box에 Docker Machine을 구성해서 사용하는 경우, 기본 IP주소는 localhost가 아닌 Docker Machine의 IP주소입니다.\n$ docker-machine url tcp://192.168.99.100:2376  IP주소(user defined network ip주소)   user defined network의 ip주소는 docker network inspect {network 명} 으로 확인 가능합니다.\n$ docker network inspect js-network [ { \u0026#34;Name\u0026#34;: \u0026#34;js-network\u0026#34;, #중략... \u0026#34;Containers\u0026#34;: { \u0026#34;0a6cca528e793a8be14b68c3b104de74473482f94ffacc9fd573a65453b81cdb\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;jisang-ms2\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.18.0.2/16\u0026#34;, #중략... } } \u0026#34;Options\u0026#34;: {}, \u0026#34;Labels\u0026#34;: {} } ] 호출 Port expose\n 노출 (컨테이너 외부 Port) 격리 (컨테이너 내부 Port)  user defined network\n 연결 해제   준비물  Application1  명 : jisang-ms1 컨테이너 port: 80 외부 노출 port: 8080 RestController로 Endpoint 호출시 jisang-ms2로 통신해 String을 반환합니다.  Application2  명 : jisang-ms2 컨테이너 port: 80 외부 노출 port: 8081 RestController로 Endpoint 호출시 \u0026ldquo;This is jisang\u0026rdquo;을 반환합니다.  user defined network\n docker network create js-network   Test Case 1    jisang-ms2 Host jisang-ms2 Port user defined network Status     IP주소(docker-machine ip) 컨테이너 외부 Port disconnect    docker run  jisang-ms1  docker run -d -e des=http://192.168.99.100:8081 -e SPRING_PROFILES_ACTIVE=docker -p 8080:80 --name jisang-ms1 jisang-ms1:latest  jisang-ms2  docker run -d -e SPRING_PROFILES_ACTIVE=docker -p 8081:80 --name jisang-ms2 jisang-ms2:latest   수행결과 호출 성공! jisang-ms1 컨테이너에서 docker-machine의 Host와 Port 정보로 jisang-ms2 호출에 성공했습니다.\nTest Case 2    jisang-ms2 Host jisang-ms2 Port Network Status     IP주소(docker-machine ip) 컨테이너 내부 Port disconnect    docker run  jisang-ms1  docker run -d -e des=http://192.168.99.100:80 -e SPRING_PROFILES_ACTIVE=docker -p 8080:80 --name jisang-ms1 jisang-ms1:latest  jisang-ms2  docker run -d -e SPRING_PROFILES_ACTIVE=docker --name jisang-ms2 jisang-ms2:latest   수행결과 호출 실패! Connection refused가 발생했습니다. jisang-ms2의 외부 port 노출이 없으니 docker-machine의 Host와 Port 정보로 호출이 안되는 것이 당연하게 보입니다.\nTest Case 3    jisang-ms2 Host jisang-ms2 Port Network Status     컨테이너 명 컨테이너 외부 Port disconnect    docker run  jisang-ms1  docker run -d -e des=http://jisang-ms2:8081 -e SPRING_PROFILES_ACTIVE=docker -p 8080:80 --name jisang-ms1 jisang-ms1:latest  jisang-ms2  docker run -d -e SPRING_PROFILES_ACTIVE=docker -p 8081:80 --name jisang-ms2 jisang-ms2:latest   수행결과 호출 실패! UnknownHosException이 발생했습니다. 기본적인 Port 노출만으로는 컨테이너 명으로 Host 정보를 얻어오는 것이 불가능으로 보입니다.\nTest Case 4    jisang-ms2 Host jisang-ms2 Port Network Status     컨테이너 명 컨테이너 내부 Port disconnect    docker run  jisang-ms1  docker run -d -e des=http://jisang-ms2:80 -e SPRING_PROFILES_ACTIVE=docker -p 8080:80 --name jisang-ms1 jisang-ms1:latest  jisang-ms2  docker run -d -e SPRING_PROFILES_ACTIVE=docker --name jisang-ms2 jisang-ms2:latest   수행결과 호출 실패! UnknownHosException이 발생했습니다. Test Case 3과 동일한 결과입니다. 컨테이너명으로 Host정보에 접근을 못하는 것은 Port 노출과는 연관이 없는 것 같습니다.\nTest Case 5    jisang-ms2 Host jisang-ms2 Port Network Status     IP주소(docker-machine ip) 컨테이너 외부 Port connect    docker run  jisang-ms1  docker run -d --network=js-network -e des=http://192.168.99.100:8081 -e SPRING_PROFILES_ACTIVE=docker -p 8080:80 --name jisang-ms1 jisang-ms1:latest  jisang-ms2  docker run -d --network=js-network -e SPRING_PROFILES_ACTIVE=docker -p 8081:80 --name jisang-ms2 jisang-ms2:latest   수행결과 호출 성공! Test Case 1과 동일하게 jisang-ms2의 Port를 노출한 경우 docker-machine의 Host와 Port 정보로 접근이 가능합니다.\nTest Case 6    jisang-ms2 Host jisang-ms2 Port Network Status     IP주소(docker-machine ip) 컨테이너 내부 Port connect    docker run  jisang-ms1  docker run -d --network=js-network -e des=http://192.168.99.100:80 -e SPRING_PROFILES_ACTIVE=docker -p 8080:80 --name jisang-ms1 jisang-ms1:latest  jisang-ms2  docker run -d --network=js-network -e SPRING_PROFILES_ACTIVE=docker --name jisang-ms2 jisang-ms2:latest   수행결과 호출 실패! Connection refused가 발생했습니다. docker-machine의 80 Port에 접근한다는 의미니까 접근이 안되는게 당연한 것 같습니다.\nTest Case 7    jisang-ms2 Host jisang-ms2 Port Network Status     컨테이너 명 컨테이너 외부 Port connect    docker run  jisang-ms1  docker run -d --network=js-network -e des=http://jisang-ms2:8081 -e SPRING_PROFILES_ACTIVE=docker -p 8080:80 --name jisang-ms1 jisang-ms1:latest  jisang-ms2  docker run -d --network=js-network -e SPRING_PROFILES_ACTIVE=docker -p 8081:80 --name jisang-ms2 jisang-ms2:latest   수행결과 호출 실패! jisang-ms1의 destination을 jisang-ms2의 컨테이너명으로 지정했습니다. UnknownHosException이 아닌 Connection refused가 발생한 것으로 보아 jisang-ms2의 Host 정보에 접근했지만 8081 Port에 띄워진 서버가 없는 것으로 보입니다.\nTest Case 8    jisang-ms2 Host jisang-ms2 Port Network Status     컨테이너 명 컨테이너 내부 Port connect    docker run  jisang-ms1  docker run -d --network=js-network -e des=http://jisang-ms2:80 -e SPRING_PROFILES_ACTIVE=docker -p 8080:80 --name jisang-ms1 jisang-ms1:latest  jisang-ms2  docker run -d --network=js-network -e SPRING_PROFILES_ACTIVE=docker --name jisang-ms2 jisang-ms2:latest   수행결과 호출 성공! user defined network의 서비스 명과 컨테이너 내부 Port로 jisang-ms1에서 \u0026ldquo;this is jisang\u0026rdquo;을 반환받았습니다.\nTest Case 9    jisang-ms2 Host jisang-ms2 Port Network Status     IP주소(user defined network ip) 컨테이너 외부 Port connect    docker run  jisang-ms1  docker run -d --network=js-network -e des=http://172.18.0.2:8081 -e SPRING_PROFILES_ACTIVE=docker -p 8080:80 --name jisang-ms1 jisang-ms1:latest  jisang-ms2  docker run -d --network=js-network -e SPRING_PROFILES_ACTIVE=docker -p 8081:80 --name jisang-ms2 jisang-ms2:latest   수행결과 호출 실패! 이제는 익숙한 Connection refused 입니다. user defined network 의 ip주소와 외부 노출 Port가 매칭이 되지 않는다는 의미 같습니다.\nTest Case 10    jisang-ms2 Host jisang-ms2 Port Network Status     IP주소(user defined network ip) 컨테이너 외부 Port disconnect    docker run  jisang-ms1  docker run -d -e des=http://172.18.0.2:8081 -e SPRING_PROFILES_ACTIVE=docker -p 8080:80 --name jisang-ms1 jisang-ms1:latest  jisang-ms2  docker run -d -e SPRING_PROFILES_ACTIVE=docker -p 8081:80 --name jisang-ms2 jisang-ms2:latest   수행결과 호출 실패! user defined network에 연결하지 않은 상태로 user defined network의 ip주소를 사용했으니 호출 실패가 당연합니다.\nTest Case 11    jisang-ms2 Host jisang-ms2 Port Network Status     IP주소(user defined network ip) 컨테이너 내부 Port connect    docker run  jisang-ms1  docker run -d --network=js-network -e des=http://172.18.0.2:80 -e SPRING_PROFILES_ACTIVE=docker -p 8080:80 --name jisang-ms1 jisang-ms1:latest  jisang-ms2  docker run -d --network=js-network -e SPRING_PROFILES_ACTIVE=docker --name jisang-ms2 jisang-ms2:latest   수행결과 호출 성공! user defined network의 ip주소와 컨테이너 내부 Port를 사용해서 접근에 성공했습니다.\nTest Case 12    jisang-ms2 Host jisang-ms2 Port Network Status     IP주소(user defined network ip) 컨테이너 내부 Port disconnect    docker run  jisang-ms1  docker run -d -e des=http://172.18.0.2:80 -e SPRING_PROFILES_ACTIVE=docker -p 8080:80 --name jisang-ms1 jisang-ms1:latest  jisang-ms2  docker run -d -e SPRING_PROFILES_ACTIVE=docker --name jisang-ms2 jisang-ms2:latest   수행결과 호출 실패! Time Out이 발생했습니다.\nConclusion Docker의 네트워크 기능과 연관된 3가지 factors(Host, Port, user defined network 연결/해제)를 경우의 수에 따라 검증하는 방식으로 테스트를 진행했습니다. (12 Test Case)\n호출에 성공한 경우를 보면 아래와 같습니다.\n   Test Case jisang-ms2 Host jisang-ms2 Port Network Status     Test Case 1 IP주소(docker-machine ip) 컨테이너 외부 Port disconnect   Test Case 5 IP주소(docker-machine ip) 컨테이너 외부 Port connect   Test Case 8 컨테이너 명 컨테이너 내부 Port connect   Test Case 11 IP주소(user defined network ip) 컨테이너 내부 Port connect     컨테이너의 Port를 노출하고 {docker-machine ip}:{외부 노출 Port}으로 접속한 경우 컨테이너를 user defined network에 연결하고 {컨테이너 명}:{컨테이너 내부 port}로 접근한 경우 컨테이너를 user defined network에 연결하고 {user defined network ip}:{컨테이너 내부 port}로 접근한 경우  Docker에서 컨테이너의 접근은 컨테이너의 Port를 노출하고 해당 컨테이너 Host의 ip 정보로 접근이 가능합니다.\nuser defined network에 컨테이너를 연결한 경우 서비스명으로 ip주소를 찾을 수 있는 Service Discovery 기능을 제공합니다. 또한, 컨테이너의 Port는 내부 Port를 사용해서 접근이 가능합니다.\nuser defined network에 두 Application을 연결하고 jisang-ms1의 Port를 노출하지 않은 경우, 두 Application 간 내부 통신만 가능한 Private Network를 구성할 수 있습니다. 아래와 같이 구성하면 jisang-ms2를 외부에 노출하지 않고 시스템 구성이 가능합니다.\n다음 포스팅에서는 Docker user defined network와 Spring Cloud를 사용해서 API 서버 구성을 해보겠습니다.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/docker-user-defined-network/what-is-docker-user-defined-network/",
        "title": "[Docker-User Defined Network 활용(1/3)] Docker User Defined Network 란?",
        "tags": ["docker", "user defined network", "spring boot"],
        "description": "Docker User Defined Bridge Network를 간단 동작을 확인합니다.",
        "content": " Docker는 몇가지 네트워크 드라이버를 기본 제공하여 강력한 네트워크 기능을 활용할 수 있게 합니다.\n bridge: 기본 네트워크 드라이버. docker0이라는 이름의 bridge 네트워크를 생성됩니다. 설정없이 컨테이너를 생성하게되면 docker0 bridge에 컨테이너를 binding 해서 네트워크 기능을 수행합니다. host: 컨테이너 네트워크가 독립/격리되지 않고, Host의 네트워크를 직접적으로 사용합니다. overlay: 서로 다른 Docker Host에서 실행되는 컨테이너 간 통신이 필요하거나, Docker Swarm 상에서 여러 컨테이너를 동시에 운영할 때 유용합니다. Macvlan: Mac주소를 컨테이너에 할당합니다. Docker 데몬은 트래픽을 컨테이너의 MAC 주소로 라우팅합니다.  상세 내용은 Docker Docs를 참고하시기 바랍니다. ( Docker Docs - Network )\nWhat  Docker Network의 사용법을 익힙니다. User Defined Bridge Network를 활용한 다양한 컨테이너 간 통신을 테스트합니다. User Defined Bridge Network로 private network상의 API 서버를 구현합니다.  Why link option의 legacy화 기존에 컨테이너 간 통신에 사용하던 docker run \u0026ndash;link 옵션은 legacy 기능이 되었습니다. 새로운 컨테이너 간 통신 방법을 확인할 필요가 있습니다. ( Docker Docs - link )\n쉽고 가벼운 네트워크 사용 기본적으로 컨테이너는 IP주소로만 접근할 수 있습니다. 반면에, user defined bridge network 적용시 컨테이너의 명으로 접근해 편리하게 네트워크 기능을 수행할 수 있습니다.\n또한, docker network 명령어로 네트워크에 컨테이너 추가/해제가 컨테이너의 생성/삭제 없이 쉽고 가볍게 이루어집니다. 이를 활용해 시스템 요구사항에 맞는 다양한 활용법을 적용할 수 있습니다. (배포전략 등)\nPrivate Network 구성 동일한 user defined bridge network 에 연결된 컨테이너는 모든 Port를 서로에게 노출시키고 외부에는 Port를 노출하지 않습니다. 이를 통해 private 네트워크를 구성해 시스템의 보안 level을 높일 수 있습니다.\nHow Docker Network의 기본동작을 확인하고, 다양한 경우의 수를 대비한 컨테이너 간 통신을 테스트합니다. 마지막으로 private network 상의 API 서버를 구현해 실제 적용할 수 있는 최소단위의 구성을 해보겠습니다.\nDocker Network 기본 동작 docker network 명령어의 기본 동작을 확인해보겠습니다. 상세한 옵션 등은 Docker Docs나 Docker cli 상의 help option을 확인하시면 됩니다.\n ls\n docker network 의 목록을 조회합니다. docker network ls  $ docker network ls NETWORK ID NAME DRIVER SCOPE 251862fde7a0 bridge bridge local aa6913be5189 host host local c455da940d9b none null local  create user defined network를 생성합니다. 기본 driver는 bridge로 생성됩니다. docker network create {network 명}   $ docker network create js-network b89583ee19716b63c30d59a5d2d21a39f3cd30ecbcced537793e9fe6a4407195 $ docker network ls NETWORK ID NAME DRIVER SCOPE 251862fde7a0 bridge bridge local aa6913be5189 host host local b89583ee1971 js-network bridge local c455da940d9b none null local inspect\n docker network의 상세정보를 조회합니다. docker network inspect {network 명}   $ docker network inspect js-network [ { \u0026#34;Name\u0026#34;: \u0026#34;js-network\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;b89583ee19716b63c30d59a5d2d21a39f3cd30ecbcced537793e9fe6a4407195\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2018-03-21T07:14:28.207599689Z\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: false, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;172.18.0.0/16\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;172.18.0.1\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Ingress\u0026#34;: false, \u0026#34;ConfigFrom\u0026#34;: { \u0026#34;Network\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;ConfigOnly\u0026#34;: false, \u0026#34;Containers\u0026#34;: {}, \u0026#34;Options\u0026#34;: {}, \u0026#34;Labels\u0026#34;: {} } ]  connect docker network에 컨테이너를 연결합니다. docker network connect {network 명} {container 명}   $ docker network connect js-network jisang-ms1 $ docker network inspect js-network [ { \u0026#34;Name\u0026#34;: \u0026#34;js-network\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;b89583ee19716b63c30d59a5d2d21a39f3cd30ecbcced537793e9fe6a4407195\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2018-03-21T07:14:28.207599689Z\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: false, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;172.18.0.0/16\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;172.18.0.1\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Ingress\u0026#34;: false, \u0026#34;ConfigFrom\u0026#34;: { \u0026#34;Network\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;ConfigOnly\u0026#34;: false, \u0026#34;Containers\u0026#34;: { \u0026#34;fe50dfe7788f46b4df3e25a128bec292f4263167dafa116ef4e68a47aeaccdcb\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;jisang-ms1\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;c7c9a46ad6715cc65250a502bbd28c1bc389514d1f1db6136b90f12e483681e0\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:ac:12:00:02\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.18.0.2/16\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;\u0026#34; } }, \u0026#34;Options\u0026#34;: {}, \u0026#34;Labels\u0026#34;: {} } ] disconnect\n docker network에서 컨테이너 연결을 해제합니다. docker network disconnect {network 명} {container 명}  $ docker network disconnect js-network jisang-ms1 $ docker network inspect js-network [ { \u0026#34;Name\u0026#34;: \u0026#34;js-network\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;b89583ee19716b63c30d59a5d2d21a39f3cd30ecbcced537793e9fe6a4407195\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2018-03-21T07:14:28.207599689Z\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: false, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;172.18.0.0/16\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;172.18.0.1\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Ingress\u0026#34;: false, \u0026#34;ConfigFrom\u0026#34;: { \u0026#34;Network\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;ConfigOnly\u0026#34;: false, \u0026#34;Containers\u0026#34;: {}, \u0026#34;Options\u0026#34;: {}, \u0026#34;Labels\u0026#34;: {} } ]  rm user defined network를 삭제합니다. docker network rm {network 명}   $ docker network rm js-network js-network $ docker network ls NETWORK ID NAME DRIVER SCOPE 251862fde7a0 bridge bridge local aa6913be5189 host host local c455da940d9b none null local docker run\n 컨테이너 수행시 user defined netowrk에 연결합니다. docker run --network={network 명} {컨테이너 image}   $ docker run -d --network=js-network -e SPRING_PROFILES_ACTIVE=docker --name jisang-ms1 jisang-ms1:latest 4e1c75a6f44763a4ac0266271698448491905388dd69397aaf84eda641464058 $ docker network inspect js-network [ { \u0026#34;Name\u0026#34;: \u0026#34;js-network\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;3e7b219d5c06c4a817ebf499316a4758f016d747b6be8a0d5b83ba108620e42c\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2018-03-21T07:38:35.244071424Z\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: false, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;172.18.0.0/16\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;172.18.0.1\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Ingress\u0026#34;: false, \u0026#34;ConfigFrom\u0026#34;: { \u0026#34;Network\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;ConfigOnly\u0026#34;: false, \u0026#34;Containers\u0026#34;: { \u0026#34;4e1c75a6f44763a4ac0266271698448491905388dd69397aaf84eda641464058\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;jisang-ms1\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;4c3b9ea660520f6ed6463575e6ec44f7a2652a43c075ee5f640367f00ec6a2ec\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:ac:12:00:02\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.18.0.2/16\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;\u0026#34; } }, \u0026#34;Options\u0026#34;: {}, \u0026#34;Labels\u0026#34;: {} } ] Conclusion 지금까지 Docker의 user defined network에 대해 알아보고 docker network 명령어의 간단 동작을 수행해보았습니다.\n다음 포스팅에서는 컨테이너 간 통신에 연관된 factors를 확인하고 경우의 수를 찾아 테스트를 진행해보겠습니다.\n  "
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/using_docker_in_cloudfoundry/",
        "title": "CF에 Docker Image 배포",
        "tags": ["cloud founcry", "docker"],
        "description": "",
        "content": " Cloud에서 애플리케이션은 Cloud Fondry(CF), Docker 등 다양한 환경에 배포할 수 있습니다. CF는 웹 애플리케이션 개발에 특화되어 있어 Docker에 비해 웹 애플리케이션 개발에 필요한 다양한 기능을 제공하고 있습니다. 반면에 Docker는 다양한 형태의 애플리케이션 개발이 가능하며 자유도가 높은 장점을 갖고 있습니다. 이 둘의 장점을 모두 사용하기 위해 Docker Image를 CF에 배포하는 리서치를 진행하여 이를 공유합니다.\n사용법 CF 설정 CF에 Docker image를 배포하기 위해서는 diego_docker플래그가 설정되어야 합니다. 아래 코드로 설정할 수 있습니다.\ncf enable-feature-flag diego_docker CF Push Docker Hub 사용 아래 명령어를 이용하여 Docker Hub에 배포된 이미지를 CF에 배포할 수 있습니다.\ncf push APP-NAME --docker-image REPO-NAME/IMAGE-NAME  APP-NAME: 애플리케이션 명 REPO-NAME: Docker Hub 레파지토리 명 IMAGE-NAME: Docker Hub에 배포된 Docker image명  Private Registry 사용 아래 명령어를 이용하여 Private Registry에 배포된 이미지를 CF에 배포할 수 있습니다.\ncf push APP-NAME --docker-image MY-PRIVATE-REGISTRY.DOMAIN:PORT/REPO-NAME/IMAGE-NAME  APP-NAME: 애플리케이션 명 MY-PRIVATE-REGISTRY.DOMAIN: Private Registry 도메인 REPO-NAME: Docker Hub 레파지토리 명 IMAGE-NAME: Docker Hub에 배포된 Docker image명  Example Docker Hub CloudFoundry에서 제공하는 lattice-app image을 cf에 배포\n$ cf push image-test-app --docker-image cloudfoundry/lattice-app Pushing app image-test-app to org *** / space *** as ***@***.com... Getting app info... Creating app with these attributes... + 이름: image-test-app + docker image: cloudfoundry/lattice-app routes: + image-test-app.***.***.*** Creating app image-test-app... Mapping routes... Staging app and tracing logs... Creating container Successfully created container Staging... Staging process started ... Staging process finished Exit status 0 Staging Complete Destroying container Successfully destroyed container Waiting for app to start... 이름: image-test-app 요청된 상태: started 인스턴스: 1/1 사용법: 256M x 1 instances routes: image-test-app.***.***.*** 마지막으로 업로드함: Mon 12 Mar 16:27:35 KST 2018 스택: cflinuxfs2 docker image: cloudfoundry/lattice-app start command: /lattice-app 상태 이후 CPU 메모리 디스크 세부사항 #0 실행 중 2018-03-12T07:33:06Z 0.0% 0 of 256M 0 of 1G "
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/docker/docker-network/",
        "title": "[Docker 기본(8/8)] Docker의 Network",
        "tags": ["docker", "service discovery", "docker network", "overlay", "ingress"],
        "description": "",
        "content": " Docker Swarm은 두 가지 종류의 Traffic을 생성합니다.\n 제어 및 관리 영역 Traffic: Docker Swarm에 대한 참가 및 탈퇴 요청과 같은 Docker Swarm의 관리 Message가 포함됩니다. 해당 Traffic은 항상 암호화됩니다. Application Data 영역 Traffic: Container 및 외부 Client와의 Traffic이 포함됩니다.  이 중에서 해당 Post에서는, Application Data 영역의 Traffic에 대해서 확인해보고자 합니다.\nDocker의 Networking에 대한 자세한 내용은 Docker Networking Reference Architecture문서를 참고하시기 바랍니다.\n Docker의 Network 먼저, Docker는 Overlay, Ingress, docker\\_gwbridge의 세 가지 Network이 존재합니다. 각각에 대한 개념과 역할에 대해서 알아보겠습니다.\nOverlay Network  Overlay Network는 Docker Swarm에 참여하는 Docker Daemon간의 통신을 관리합니다. 독립실행형 Container의 Network를 생성하는 방법과 동일한 방식으로 Overlay Network를 생성할 수 있습니다. 기존에 생성된 Overlay Network에 Service를 연결시켜 Service간 통신을 활성화할 수 있습니다. Overlay Network는 Overlay Network Driver를 사용합니다.  Ingress Network  Ingress Network는 Service의 Node들간에 Load Balancing을 하는 Overlay Network입니다. Docker Swarm의 모든 Node가 노출된 Port로 요청을 받게되면, 해당 요청을 IPVS라는 모듈로 전달합니다. IPVS는 해당 Service에 참여하는 모든 IP 주소를 추적하고 그 중 하나를 선택한 뒤, 요청을 해당 경로로 Routing합니다. Ingress Network는 Docker Swarm을 Init하거나 Join할 때 자동으로 생성됩니다.  docker_gwbridge  docker_gwbridge는 Overlay Network(Ingress Network 포함)를 개별 Docker Daemon의 물리적 Network에 연결하는 Bridge Network입니다. 기본적으로, Service가 실행 중인 각각의 Container는 로컬 Docker Daemon Host의 docker_gwbridge Network에 연결됩니다. docker_gwbridge Network는 Docker Swarm을 Init하거나 Join할 때 자동으로 생성됩니다.  Docker는 사용자가 정의한 Bridge, Overlay 및 MACVLAN Network들에게 Host 내의 모든 Container의 위치를 제공하는 내부 DNS Server를 갖고 있습니다. 각 Docker Container(또는 Docker Swarm의 Task)에 존재하는 DNS Resolver가, DNS 쿼리를 DNS Server 역할을 하는 Docker Engine으로 전달합니다. 그런 다음 Docker Engine은 DNS 쿼리가 요청한 Container가 Network 내에 포함되어있는지 확인합니다. Docker Engine은 key-value 저장소에서 Container, Task 또는 Service 이름과 일치하는 IP주소를 조회하고, 해당 IP 또는 Service Virtual IP(VIP)를 요청자에게 반환합니다. 이렇게 Docker는 내장 DNS를 사용하여, Single Docker Engine에서 실행되는 Container 및 Docker Swarm에서 실행되는 Task에 대한 Service Discovery기능을 제공합니다.\nService Discovery Service Discovery는 Network 범위 내에서 동작합니다. 동일한 Network에 있는 Contrainer나 Task만 내장 DNS 기능을 사용할 수 있음을 의미합니다. 따라서, 동일한 Network에 있지 않은 Container는 서로의 주소를 확인할 수 없습니다. 또한, 특정 Network에 Container 또는 Task가 있는 Node만 해당 Network의 DNS 항목들을 저장합니다. 이러한 특징들이 Docker의 보안 및 성능을 향상시켜 줍니다. 만약 대상 Container 또는 Service가 원본 Container와 동일한 Network에 속하지 않는다면, Doker Engine은 구성된 기본 DNS Server로 DNS 쿼리를 전달합니다.\nDocker Swarm에 내장된 Service Discovery의 경우, Token 기반으로 동작하며, 추가 설정없이 즉시 사용가능합니다. 하지만, 모든 Host들이 Docker Hub에 접근 가능해야 하기 때문에, 전체 Service가 한 곳의 이슈로 중지되는 상황(a single point of failure)이 발생할 수 있습니다. 따라서, 해당 Service Discovery는 개발 및 Test 환경에서만 사용하고, Production환경에서는 Consul, etcd, ZooKeeper 등의 Key-Value Store를 구성하여 사용하는 것을 권장합니다. Service Discovery에 대한 자세한 사항은 Docker Swarm Discovery문서를 참고하시기 바랍니다.\n Service를 위한 Network 구성하기 Overlay Network 생성 Docker Swarm의 Manager Node에 접속합니다.\n$ docker-machine ssh default ## . ## ## ## == ## ## ## ## ## === /\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\\___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\_______/ _ _ ____ _ _ | |__ ___ ___ | |_|___ \\ __| | ___ ___| | _____ _ __ | \u0026#39;_ \\ / _ \\ / _ \\| __| __) / _` |/ _ \\ / __| |/ / _ \\ \u0026#39;__| | |_) | (_) | (_) | |_ / __/ (_| | (_) | (__| \u0026lt; __/ | |_.__/ \\___/ \\___/ \\__|_____\\__,_|\\___/ \\___|_|\\_\\___|_| Boot2Docker version 17.10.0-ce, build HEAD : 34fe485 - Wed Oct 18 17:16:34 UTC 2017 Docker version 17.10.0-ce, build f4ffd25 docker@default:~$ docker network create [OPTIONS] NETWORK로 새로운 Overlay Network를 생성합니다.\ndocker@default:~$ docker network create -d overlay overnet fvyh0us5pkb50dd0qbk4z8537 생성된 Overlay Network를 docker network ls [OPTIONS]로 확인합니다.\ndocker@default:~$ docker network ls NETWORK ID NAME DRIVER SCOPE af3308efb8e3 bridge bridge local 01a00e855484 docker_gwbridge bridge local b40e73017c03 host host local 3fbffkmpwe24 ingress overlay swarm 4a7e4e260bd5 none null local fvyh0us5pkb5 overnet overlay swarm docker network inspect [OPTIONS] NETWORK [NETWORK...]로 생성된 Overlay Network의 상세정보를 확인합니다.\ndocker@default:~$ docker network inspect overnet [ { \u0026#34;Name\u0026#34;: \u0026#34;overnet\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;fvyh0us5pkb50dd0qbk4z8537\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;0001-01-01T00:00:00Z\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;swarm\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;overlay\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: false, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: null, \u0026#34;Config\u0026#34;: [] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Ingress\u0026#34;: false, \u0026#34;ConfigFrom\u0026#34;: { \u0026#34;Network\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;ConfigOnly\u0026#34;: false, \u0026#34;Containers\u0026#34;: null, \u0026#34;Options\u0026#34;: { \u0026#34;com.docker.network.driver.overlay.vxlanid_list\u0026#34;: \u0026#34;4098\u0026#34; }, \u0026#34;Labels\u0026#34;: null } ] Worker Node에 접속합니다.\n$ docker-machine ssh default-2 ## . ## ## ## == ## ## ## ## ## === /\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\\___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\_______/ _ _ ____ _ _ | |__ ___ ___ | |_|___ \\ __| | ___ ___| | _____ _ __ | \u0026#39;_ \\ / _ \\ / _ \\| __| __) / _` |/ _ \\ / __| |/ / _ \\ \u0026#39;__| | |_) | (_) | (_) | |_ / __/ (_| | (_) | (__| \u0026lt; __/ | |_.__/ \\___/ \\___/ \\__|_____\\__,_|\\___/ \\___|_|\\_\\___|_| Boot2Docker version 17.10.0-ce, build HEAD : 34fe485 - Wed Oct 18 17:16:34 UTC 2017 Docker version 17.10.0-ce, build f4ffd25 docker@default-2:~$ Worker Node에도 docker network ls [OPTIONS]로 Swarm Manager에서 생성한 Overlay Network가 존재하는지 확인합니다.\ndocker@default-2:~$ docker network ls NETWORK ID NAME DRIVER SCOPE ec983de00160 bridge bridge local 33c85f089e4a docker_gwbridge bridge local 48f1f71c59f0 host host local 3fbffkmpwe24 ingress overlay swarm b66bb1d6602e none null local Worker Node에서는 아직 생성된 Overlay Network를 확인할 수 없습니다. 이는 Docker가 해당 Overlay Network를 필요할 때만, Host에 확장시키기 때문입니다. 따라서, 해당 Network에서 생성된 Service에서 Task될 때, Network 목록에 나타나게 됩니다.\nService Network 연결 Overlay Network에 연결할 Service를 docker service create [OPTIONS] IMAGE [COMMAND] [ARG...]으로 생성합니다. --network Option을 설정하여, 생성된 Overlay Network에 해당 Service를 추가할 수 있습니다.\ndocker@default:~$ docker service create --name myservice \\ \u0026gt; --network overnet \\ \u0026gt; --replicas 3 \\ \u0026gt; ubuntu sleep infinity 0pqg1f550559cuoxcjvhenb30 overall progress: 3 out of 3 tasks 1/3: running [==================================================\u0026gt;] 2/3: running [==================================================\u0026gt;] 3/3: running [==================================================\u0026gt;] verify: Service converged 생성된 Service와 Container의 정보를 확인합니다.\ndocker@default:~$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS 0pqg1f550559 myservice replicated 3/3 ubuntu:latest docker@default:~$ docker service ps myservice ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 7u0odxhvmqam myservice.1 ubuntu:latest default-2 Running Running 25 seconds ago 0ylu27keha3b myservice.2 ubuntu:latest default Running Running 25 seconds ago pfeozzgdnu97 myservice.3 ubuntu:latest default-3 Running Running 25 seconds ago docker@default:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fb057acbf8fa ubuntu:latest \u0026#34;sleep infinity\u0026#34; 29 seconds ago Up 29 seconds myservice.2.0ylu27keha3b3cieivu4vh4bd docker@default:~$ Worker Node에서 Overlay Network가 생성되었는지 docker network ls [OPTIONS]와 docker network inspect [OPTIONS] NETWORK [NETWORK...]로 확인합니다.\ndocker@default-2:~$ docker network ls NETWORK ID NAME DRIVER SCOPE ec983de00160 bridge bridge local 33c85f089e4a docker_gwbridge bridge local 48f1f71c59f0 host host local 3fbffkmpwe24 ingress overlay swarm b66bb1d6602e none null local fvyh0us5pkb5 overnet overlay swarm docker@default-2:~$ docker network inspect overnet [ { \u0026#34;Name\u0026#34;: \u0026#34;overnet\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;fvyh0us5pkb50dd0qbk4z8537\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2017-11-09T08:14:08.731645101Z\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;swarm\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;overlay\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: false, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: null, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;10.0.0.1\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Ingress\u0026#34;: false, \u0026#34;ConfigFrom\u0026#34;: { \u0026#34;Network\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;ConfigOnly\u0026#34;: false, \u0026#34;Containers\u0026#34;: { \u0026#34;40c48c6eb4e56fab5f6f222c31bfe55915126033dcb416267d78ff197c712583\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;myservice.1.7u0odxhvmqam2s6zcx722bfvx\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;a5d198bb2f1f31cb01e0368e8095d222288d0844ef56bcc5789e6f081425093d\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:0a:00:00:06\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;10.0.0.6/24\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;overnet-sbox\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;overnet-endpoint\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;aeea6fa5e54ec37d640ffcbe81d3206550dc972382265de9b5592e9b2bd1a3f2\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:0a:00:00:03\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;10.0.0.3/24\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;\u0026#34; } }, \u0026#34;Options\u0026#34;: { \u0026#34;com.docker.network.driver.overlay.vxlanid_list\u0026#34;: \u0026#34;4098\u0026#34; }, \u0026#34;Labels\u0026#34;: {}, \u0026#34;Peers\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;99176f06426d\u0026#34;, \u0026#34;IP\u0026#34;: \u0026#34;192.168.99.101\u0026#34; }, { \u0026#34;Name\u0026#34;: \u0026#34;ac69cf321f95\u0026#34;, \u0026#34;IP\u0026#34;: \u0026#34;192.168.99.102\u0026#34; }, { \u0026#34;Name\u0026#34;: \u0026#34;8c65bb22c846\u0026#34;, \u0026#34;IP\u0026#34;: \u0026#34;192.168.99.100\u0026#34; } ] } ] Overlay Network로 통신 확인하기 Worker Node에서 docker network inspect [OPTIONS] NETWORK [NETWORK...]로 Overlay Network의 상세정보를 확인하여 실행 중인 Container의 IP정보를 확인합니다.\ndocker@default-2:~$ docker network inspect overnet [ { \u0026#34;Name\u0026#34;: \u0026#34;overnet\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;fvyh0us5pkb50dd0qbk4z8537\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2017-11-09T08:14:08.731645101Z\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;swarm\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;overlay\u0026#34;, ... 생략 ... \u0026#34;Containers\u0026#34;: { \u0026#34;40c48c6eb4e56fab5f6f222c31bfe55915126033dcb416267d78ff197c712583\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;myservice.1.7u0odxhvmqam2s6zcx722bfvx\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;a5d198bb2f1f31cb01e0368e8095d222288d0844ef56bcc5789e6f081425093d\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:0a:00:00:06\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;10.0.0.6/24\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;overnet-sbox\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;overnet-endpoint\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;aeea6fa5e54ec37d640ffcbe81d3206550dc972382265de9b5592e9b2bd1a3f2\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:0a:00:00:03\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;10.0.0.3/24\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;\u0026#34; } }, ... 생략 ... } ] 다시 Docker Swarm의 Manager Node에 접속하고 실행 중인 Container ID를 확인한 뒤, Container에 접속합니다.\ndocker@default:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fb057acbf8fa ubuntu:latest \u0026#34;sleep infinity\u0026#34; 29 seconds ago Up 29 seconds myservice.2.0ylu27keha3b3cieivu4vh4bd docker@default:~$ docker exec -it fb057acbf8fa /bin/bash root@fb057acbf8fa:/# Networking Test를 위해, iptuils-ping을 다음의 명령어를 실행하여 설치합니다.\nroot@fb057acbf8fa:/# apt-get update \u0026amp;\u0026amp; apt-get install iputils-ping ... 생략 ... Processing triggers for libc-bin (2.23-0ubuntu9) ... 앞에서 확인한 Worker Node에서 실행 중인 Container의 IP로 Networking Test를 진행합니다.\nroot@fb057acbf8fa:/# ping 10.0.0.6 PING 10.0.0.6 (10.0.0.6) 56(84) bytes of data. 64 bytes from 10.0.0.6: icmp_seq=1 ttl=64 time=0.785 ms 64 bytes from 10.0.0.6: icmp_seq=2 ttl=64 time=0.811 ms 64 bytes from 10.0.0.6: icmp_seq=3 ttl=64 time=0.688 ms 64 bytes from 10.0.0.6: icmp_seq=4 ttl=64 time=0.558 ms 64 bytes from 10.0.0.6: icmp_seq=5 ttl=64 time=0.519 ms 64 bytes from 10.0.0.6: icmp_seq=6 ttl=64 time=0.510 ms 64 bytes from 10.0.0.6: icmp_seq=7 ttl=64 time=0.535 ms 64 bytes from 10.0.0.6: icmp_seq=8 ttl=64 time=0.534 ms ^C --- 10.0.0.6 ping statistics --- 8 packets transmitted, 8 received, 0% packet loss, time 6999ms rtt min/avg/max/mdev = 0.510/0.617/0.811/0.119 ms Overlay Network로 Service Discovery 확인하기 다시 Manager Node의 Container로 돌아와, 아래의 명령어를 실행합니다.\nroot@fb057acbf8fa:/# cat /etc/resolv.conf nameserver 127.0.0.11 options ndots:0 여기서 관심을 가져야할 값은 nameserver 127.0.0.11입니다. 이 값은 Container 내부에서 실행되고 있는 DNS Resolver들에게 보내지고, 모든 Docker Container는 이 주소가 포함되어있는 내장 DNS Server를 실행합니다. Service이름으로 ping명령을 실행하여 Networking Test를 진행합니다.\nroot@fb057acbf8fa:/# ping myservice PING myservice (10.0.0.5) 56(84) bytes of data. 64 bytes from 10.0.0.5: icmp_seq=1 ttl=64 time=0.026 ms 64 bytes from 10.0.0.5: icmp_seq=2 ttl=64 time=0.058 ms 64 bytes from 10.0.0.5: icmp_seq=3 ttl=64 time=0.050 ms 64 bytes from 10.0.0.5: icmp_seq=4 ttl=64 time=0.047 ms 64 bytes from 10.0.0.5: icmp_seq=5 ttl=64 time=0.047 ms ^C --- myservice ping statistics --- 5 packets transmitted, 5 received, 0% packet loss, time 3997ms rtt min/avg/max/mdev = 0.026/0.045/0.058/0.012 ms Container내에서 exit명령으로 빠져나와 docker service inspect [OPTIONS] SERVICE [SERVICE...]로 Service의 상세정보를 확인합니다. ping을 통해 나온 결과에 출력된 10.0.0.5의 주소가 현재 Service의 Virtual IP인 것을 확인할 수 있습니다.\nroot@fb057acbf8fa:/# exit exit docker@default:~$ docker service inspect myservice [ { \u0026#34;ID\u0026#34;: \u0026#34;0pqg1f550559cuoxcjvhenb30\u0026#34;, \u0026#34;Version\u0026#34;: { \u0026#34;Index\u0026#34;: 289 }, \u0026#34;CreatedAt\u0026#34;: \u0026#34;2017-11-09T08:14:08.625888401Z\u0026#34;, \u0026#34;UpdatedAt\u0026#34;: \u0026#34;2017-11-09T08:14:08.627636861Z\u0026#34;, \u0026#34;Spec\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;myservice\u0026#34;, \u0026#34;Labels\u0026#34;: {}, \u0026#34;TaskTemplate\u0026#34;: { \u0026#34;ContainerSpec\u0026#34;: { \u0026#34;Image\u0026#34;: \u0026#34;ubuntu:latest@sha256:6eb24585b1b2e7402600450d289ea0fd195cfb76893032bbbb3943e041ec8a65\u0026#34;, \u0026#34;Args\u0026#34;: [ \u0026#34;sleep\u0026#34;, \u0026#34;infinity\u0026#34; ], \u0026#34;StopGracePeriod\u0026#34;: 10000000000, \u0026#34;DNSConfig\u0026#34;: {} }, ... 생략 ... }, \u0026#34;EndpointSpec\u0026#34;: { \u0026#34;Mode\u0026#34;: \u0026#34;vip\u0026#34; } }, \u0026#34;Endpoint\u0026#34;: { \u0026#34;Spec\u0026#34;: { \u0026#34;Mode\u0026#34;: \u0026#34;vip\u0026#34; }, \u0026#34;VirtualIPs\u0026#34;: [ { \u0026#34;NetworkID\u0026#34;: \u0026#34;fvyh0us5pkb50dd0qbk4z8537\u0026#34;, \u0026#34;Addr\u0026#34;: \u0026#34;10.0.0.5/24\u0026#34; } ] } } ] Worker Node에서도 Service이름으로 ping명령을 실행하게 되면, Service Virtual IP로 실행되어 출력되는 것을 확인할 수 있습니다.\nroot@40c48c6eb4e5:/# ping myservice PING myservice (10.0.0.5) 56(84) bytes of data. 64 bytes from 10.0.0.5: icmp_seq=1 ttl=64 time=0.030 ms 64 bytes from 10.0.0.5: icmp_seq=2 ttl=64 time=0.054 ms 64 bytes from 10.0.0.5: icmp_seq=3 ttl=64 time=0.046 ms 64 bytes from 10.0.0.5: icmp_seq=4 ttl=64 time=0.048 ms 64 bytes from 10.0.0.5: icmp_seq=5 ttl=64 time=0.084 ms 64 bytes from 10.0.0.5: icmp_seq=6 ttl=64 time=0.053 ms --- myservice ping statistics --- 6 packets transmitted, 6 received, 0% packet loss, time 4998ms rtt min/avg/max/mdev = 0.030/0.052/0.084/0.017 ms 왜 Docker라는 것이 나타났는가란 주제에서 시작하여, Docker Cluster구성과 내부 Network까지, 총 8가지의 주제로 Docker System에 대해서 생각해봤습니다. Image Registry, Container 보안 등 더 살펴봐야할 내용은 많지만, Docker에 대한 주제는 이쯤에서 일단 마무리 짓고, 이제부터는 Container 환경에서 실질적으로 Application들이 어떤 식으로 구축되고 운영되어야 하는지에 대해서 먼저 고민해 봐야겠습니다.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/docker/swarm-architecture/",
        "title": "[Docker 기본(7/8)] Docker Swarm의 구조와 Service 배포하기",
        "tags": ["docker", "swarm", "service", "rolling update", "container ochestration"],
        "description": "",
        "content": " 이제는 구성된 Docker Swarm에 Application을 배포해보겠습니다. Docker Swarm에 Application Image를 배포하기 위해선, Service를 생성해야 합니다. Service는 큰 Application Context 내의 Microservice들의 Image를 의미하며, 예로 HTTP Server, Database 또는 분산 환경에서 실행하고자 하는 다양한 유형의 Runtime Program들이 여기에 속한다고 할 수 있습니다. Service를 생성하고자 할 때, 사용할 Container Image와 Container 내에서 실행할 명령을 지정합니다. 또한, 다음과 같은 Option들을 정의하여 사용합니다.\n Docker Swarm 외부에서 접속할 수 있는 Port Docker Swarm 내부의 다른 Service와 통신하기 위한 Overlay Network CPU 및 Memory 사용에 대한 정책 Rolling Update 정책 Image의 Replica 개수  Service Service가 독립형 Container들을 직접 실행하는 것에 비해 갖는 주요 장점 중 하나는, 수동으로 Service를 다시 시작할 필요없이 연결된 Network 및 Volume 등의 구성을 수정할 수 있습니다. Docker는 Configuration을 수정하고 만료된 Configuration의 Service Task를 중지할 것이며, 원하는 Configuration과 일치하는 새로운 Task를 생성할 것입니다. Docker가 Swarm Mode에서 실행 중이라고 한다면, Service들 뿐만 아니라, Swarm에 참여하고 있는 모든 Docker Host들 위에서도 독립 실행형 Container들을 실행할 수 있습니다. Service와 독립형 Container의 주요 차이점은 Service는 Manager Node에서만 관리할 수 있고, 독립형 Container들은 모든 Docker Daemon에서 실행될 수 있다는 점입니다.\nService가 Task를 Task가 Container를 Service를 배포할 때, 먼저 Manager Node는 Service 정의서로 생성할 Service에 대한 상태 및 설정 정보를 받아들입니다. 그런 다음, 타 Node들에서 Service에 대한 복제 Task가 실행되도록 Scheduling합니다. 이 때, Node들에 위치한 Task는 서로 독립적으로 실행됩니다. Container는 격리된 Process이며, 각각의 Task는 정확히 하나의 Container를 호출하게 됩니다. 일단 Container가 기동되면, Scheduler는 Task가 실행 중인 상태임을 인식하고, 만약 Container의 상태 검사를 실패하거나 종료된다면, Task도 종료됩니다.\nTask는 Docker Swarm의 가장 작은 Scheduling 단위입니다. Serivce를 생성하거나 수정하기 위해 Service의 상태를 정의하면, Ochestrator는 Task를 Scheduling하여 정의된 상태로 Service를 구체화시킵니다. 예를들어, Ochestrator에게 Microservice를 항상 3개의 Instance로 유지하도록 Service를 정의한다고 했을 때, Ochestrator는 3개의 Task를 생성시킵니다. 각각의 Task는 Scheduler가 Container를 생성하여 채우는 Slot이라고 할 수 있으며 Container는 Task가 인스턴스화된 것으로, 할당 - 준비 - 실행의 단방향 메커니즘으로 진행됩니다. 만약 Task가 실행에 실패하면, Ochestrator는 해당 Task와 Container를 제거한 다음, 이를 대체할 새로운 Task를 생성합니다. 아래 그림은 Docker Swarm에서 Service 생성 요청을 받아들이고, Task를 Worker Node에 Scheduling하는 방법을 나타내고 있습니다.\n출처: Docker Docs - How services work\n Service 배포 유형 Service 배포에는 Replicatied와 Global이라는 두 가지 유형이 있습니다. Replicated Service는 사용자가 원하는 수만큼 Task를 동일하게 생성하여 실행되는 Service입니다(가장 일반적인 Service유형이며, 일반적인 Application을 배포할 때 사용된다고 생각하면 됩니다). Global Service는 모든 Node에서 하나의 Task를 실행하는 Service이며, 미리 지정된 Task의 개수가 없습니다. Swarm에 Node를 추가할 때마다, Ochestrator는 Task를 만들고 Scheduler는 Task를 새로운 Node에 할당합니다. Global Service는 Monitoring Agent, Virus 백신 Scanner 또는 Swarm의 모든 Node에서 실행해야하는 Container들을 들 수 있습니다. 아래 그림은 황색으로 표현된 3개의 Replicated Service와 회색으로 표현된 Global Service를 보여줍니다.\n출처: Docker Docs - How services work\n Service의 부하 분산 처리 Docker Swarm의 Manager Node는 Service가 수 많은 Request를 수용할 수 있도록 하기위해, Ingress Load Balancing이라는 기능을 사용합니다. 이를 위해, Manager Node가 자동으로 해당 Service의 노출 Port(PublishedPort)를 지정하거나, 관리자가 직접 해당 Service를 위한 PublishedPort를 설정할 수도 있습니다(만약 Port를 지정하지 않을 경우, Manager Node는 30000 ~ 32767 범위내의 Port를 자동으로 할당합니다). 이 후 외부에서는 Node가 현재 Task를 실행 중인지 여부와 관계없이, Cluster의 모든 Node의 PublishedPort를 통해서 Service에 접근할 수 있습니다.\n또한, Docker Swarm은 내부 DNS Component를 갖고 있어서, DNS 항목에 있는 각 Service를 자동으로 할당할 수 있습니다. Manager Node는 내부 Load Balancing을 사용하여, Service의 DNS 이름에 따라 Cluster 내의 Service 간에 요청을 분산시킵니다. Cluster 내에 위치한 Container간 내부 Load Balancing과 Cluster에 진입하는 Traffic들에 대한 외부 Load Balancing 모두 처리가 가능하며, 이는 Docker Engine에 내장된 Load Balancing 기능에 의해서 수행됩니다.\nService 배포하기 이제부터는 직접 Service를 Docker Swarm에 배포해 보겠습니다(아래의 예제는 Docker 공식문서에서 .\nSwarm Manager에서 Service 배포 Docker Swarm의 Manager Node에 접속합니다.\n$ docker-machine ssh default ## . ## ## ## == ## ## ## ## ## === /\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\\___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\_______/ _ _ ____ _ _ | |__ ___ ___ | |_|___ \\ __| | ___ ___| | _____ _ __ | \u0026#39;_ \\ / _ \\ / _ \\| __| __) / _` |/ _ \\ / __| |/ / _ \\ \u0026#39;__| | |_) | (_) | (_) | |_ / __/ (_| | (_) | (__| \u0026lt; __/ | |_.__/ \\___/ \\___/ \\__|_____\\__,_|\\___/ \\___|_|\\_\\___|_| Boot2Docker version 17.10.0-ce, build HEAD : 34fe485 - Wed Oct 18 17:16:34 UTC 2017 Docker version 17.10.0-ce, build f4ffd25 docker@default:~$ Service를 docker service create [OPTIONS] IMAGE [COMMAND] [ARG...]으로 생성합니다.\ndocker@default:~$ docker service create -p 80:80 --name web nginx:latest 1lc92552oo4mxh25l4iur54pj overall progress: 1 out of 1 tasks 1/1: running [==================================================\u0026gt;] verify: Service converged Service가 생성되었는지 docer service ls [OPTIONS]으로 Service 목록을 확인합니다.\ndocker@default:~$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS 1lc92552oo4m web replicated 1/1 nginx:latest *:80-\u0026gt;80/tcp 생성된 Service가 정상적으로 기동되었는지 docker service ps [OPTIONS] SERVICE [SERVICE...]로 확인합니다.\ndocker@default:~$ docker service ps web ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS u3pmniq55z8a web.1 nginx:latest default Running Running 43 seconds ago docker service inspect [OPTIONS] SERVICE [SERVICE...]로 생성된 Serivce의 상세정보를 확인합니다. --pretty Option으로 JSON형식의 Data를 사람이 읽기 쉽운 형태로 변환하여 확인할 수 있습니다.\ndocker@default:~$ docker service inspect web [ { \u0026#34;ID\u0026#34;: \u0026#34;1lc92552oo4mxh25l4iur54pj\u0026#34;, \u0026#34;Version\u0026#34;: { \u0026#34;Index\u0026#34;: 68 }, \u0026#34;CreatedAt\u0026#34;: \u0026#34;2017-11-07T01:19:38.721156306Z\u0026#34;, \u0026#34;UpdatedAt\u0026#34;: \u0026#34;2017-11-07T01:19:38.721966563Z\u0026#34;, \u0026#34;Spec\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;Labels\u0026#34;: {}, \u0026#34;TaskTemplate\u0026#34;: { \u0026#34;ContainerSpec\u0026#34;: { \u0026#34;Image\u0026#34;: \u0026#34;nginx:latest@sha256:9fca103a62af6db7f188ac3376c60927db41f88b8d2354bf02d2290a672dc425\u0026#34;, \u0026#34;StopGracePeriod\u0026#34;: 10000000000, \u0026#34;DNSConfig\u0026#34;: {} }, \u0026#34;Resources\u0026#34;: { \u0026#34;Limits\u0026#34;: {}, \u0026#34;Reservations\u0026#34;: {} }, \u0026#34;RestartPolicy\u0026#34;: { \u0026#34;Condition\u0026#34;: \u0026#34;any\u0026#34;, \u0026#34;Delay\u0026#34;: 5000000000, \u0026#34;MaxAttempts\u0026#34;: 0 }, \u0026#34;Placement\u0026#34;: { \u0026#34;Platforms\u0026#34;: [ { \u0026#34;Architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;OS\u0026#34;: \u0026#34;linux\u0026#34; }, { \u0026#34;OS\u0026#34;: \u0026#34;linux\u0026#34; }, { \u0026#34;Architecture\u0026#34;: \u0026#34;arm64\u0026#34;, \u0026#34;OS\u0026#34;: \u0026#34;linux\u0026#34; }, { \u0026#34;Architecture\u0026#34;: \u0026#34;386\u0026#34;, \u0026#34;OS\u0026#34;: \u0026#34;linux\u0026#34; }, { \u0026#34;Architecture\u0026#34;: \u0026#34;ppc64le\u0026#34;, \u0026#34;OS\u0026#34;: \u0026#34;linux\u0026#34; }, { \u0026#34;Architecture\u0026#34;: \u0026#34;s390x\u0026#34;, \u0026#34;OS\u0026#34;: \u0026#34;linux\u0026#34; } ] }, \u0026#34;ForceUpdate\u0026#34;: 0, \u0026#34;Runtime\u0026#34;: \u0026#34;container\u0026#34; }, \u0026#34;Mode\u0026#34;: { \u0026#34;Replicated\u0026#34;: { \u0026#34;Replicas\u0026#34;: 1 } }, \u0026#34;UpdateConfig\u0026#34;: { \u0026#34;Parallelism\u0026#34;: 1, \u0026#34;FailureAction\u0026#34;: \u0026#34;pause\u0026#34;, \u0026#34;Monitor\u0026#34;: 5000000000, \u0026#34;MaxFailureRatio\u0026#34;: 0, \u0026#34;Order\u0026#34;: \u0026#34;stop-first\u0026#34; }, \u0026#34;RollbackConfig\u0026#34;: { \u0026#34;Parallelism\u0026#34;: 1, \u0026#34;FailureAction\u0026#34;: \u0026#34;pause\u0026#34;, \u0026#34;Monitor\u0026#34;: 5000000000, \u0026#34;MaxFailureRatio\u0026#34;: 0, \u0026#34;Order\u0026#34;: \u0026#34;stop-first\u0026#34; }, \u0026#34;EndpointSpec\u0026#34;: { \u0026#34;Mode\u0026#34;: \u0026#34;vip\u0026#34;, \u0026#34;Ports\u0026#34;: [ { \u0026#34;Protocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;TargetPort\u0026#34;: 80, \u0026#34;PublishedPort\u0026#34;: 80, \u0026#34;PublishMode\u0026#34;: \u0026#34;ingress\u0026#34; } ] } }, \u0026#34;Endpoint\u0026#34;: { \u0026#34;Spec\u0026#34;: { \u0026#34;Mode\u0026#34;: \u0026#34;vip\u0026#34;, \u0026#34;Ports\u0026#34;: [ { \u0026#34;Protocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;TargetPort\u0026#34;: 80, \u0026#34;PublishedPort\u0026#34;: 80, \u0026#34;PublishMode\u0026#34;: \u0026#34;ingress\u0026#34; } ] }, \u0026#34;Ports\u0026#34;: [ { \u0026#34;Protocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;TargetPort\u0026#34;: 80, \u0026#34;PublishedPort\u0026#34;: 80, \u0026#34;PublishMode\u0026#34;: \u0026#34;ingress\u0026#34; } ], \u0026#34;VirtualIPs\u0026#34;: [ { \u0026#34;NetworkID\u0026#34;: \u0026#34;3fbffkmpwe24mhpphqjxyudo9\u0026#34;, \u0026#34;Addr\u0026#34;: \u0026#34;10.255.0.12/16\u0026#34; } ] } } ] docker@default:~$ docker service inspect --pretty web ID: 1lc92552oo4mxh25l4iur54pj Name: web Service Mode: Replicated Replicas: 1 Placement: UpdateConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Update order: stop-first RollbackConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Rollback order: stop-first ContainerSpec: Image: nginx:latest@sha256:9fca103a62af6db7f188ac3376c60927db41f88b8d2354bf02d2290a672dc425 Resources: Endpoint Mode: vip Ports: PublishedPort = 80 Protocol = tcp TargetPort = 80 PublishMode = ingress Docker Swarm의 Manager Node에서 빠져나와 docker-machine ls [OPTIONS] [ARG...]로 각 Worker Node의 URL을 확인한 뒤, 접속합니다.\n$ docker-machine ls NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS default - virtualbox Running tcp://192.168.99.100:2376 v17.10.0-ce default-2 - virtualbox Running tcp://192.168.99.101:2376 v17.10.0-ce default-3 - virtualbox Running tcp://192.168.99.102:2376 v17.10.0-ce  Manager Node  Worker Node   Worker Node에서 Service 배포 구성한 Worker Node들 중 한 Node에 접속합니다.\n$ docker-machine ssh default-2 ## . ## ## ## == ## ## ## ## ## === /\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\\___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\_______/ _ _ ____ _ _ | |__ ___ ___ | |_|___ \\ __| | ___ ___| | _____ _ __ | \u0026#39;_ \\ / _ \\ / _ \\| __| __) / _` |/ _ \\ / __| |/ / _ \\ \u0026#39;__| | |_) | (_) | (_) | |_ / __/ (_| | (_) | (__| \u0026lt; __/ | |_.__/ \\___/ \\___/ \\__|_____\\__,_|\\___/ \\___|_|\\_\\___|_| Boot2Docker version 17.10.0-ce, build HEAD : 34fe485 - Wed Oct 18 17:16:34 UTC 2017 Docker version 17.10.0-ce, build f4ffd25 docker@default-2:~$ Service를 docker service create [OPTIONS] IMAGE [COMMAND] [ARG...]로 생성합니다. Service는 생성되지 않으며, Swarm Manager가 아닌 Node에서는 Cluster 상태를 변경할 수 없다는 Error Message를 확인할 수 있습니다.\ndocker@default-2:~$ docker service create -p 80:80 --name web nginx:latest Error response from daemon: This node is not a swarm manager. Worker nodes can\u0026#39;t be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager. 배포된 Service 수정하기 지금부터는 Docker Swarm에 배포된 Service를 통해, Scaling과 배포된 Application의 Version Update/Rollback 기능을 사용해보겠습니다.\nScaling docker service ls [OPTIONS]으로 Service 목록을 확인합니다. 현재 생성된 Service의 Replica 개수가 1인 것을 알 수 있습니다.\ndocker@default:~$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS te07odz7sd0d web replicated 1/1 nginx:latest *:80-\u0026gt;80/tcp Service 목록 중 Service 하나를 선택하여 docker service scale SERVICE=REPLICAS [SERVICE=REPLICAS...]로 원하는 개수만큼 Scaling합니다.\ndocker@default:~$ docker service scale web=5 web scaled to 5 overall progress: 5 out of 5 tasks 1/5: running [==================================================\u0026gt;] 2/5: running [==================================================\u0026gt;] 3/5: running [==================================================\u0026gt;] 4/5: running [==================================================\u0026gt;] 5/5: running [==================================================\u0026gt;] verify: Service converged Service가 Scaling 되었는지 docker service ls [OPTIONS]으로 확인합니다. REPLICAS열에서 지정한 수만큼 변경되었는지 확인할 수 있습니다.\ndocker@default:~$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS 1lc92552oo4m web replicated 5/5 nginx:latest *:80-\u0026gt;80/tcp docker service ps [OPTIONS] SERVICE [SERVICE...]로 실제로 몇 개의 Task가 실행중인지 확인합니다. 지정한 개수만큼 Task가 늘어난 것을 확인할 수 있습니다.\ndocker@default:~$ docker service ps web ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS u3pmniq55z8a web.1 nginx:latest default Running Running about a minute ago p8i67ito9574 web.2 nginx:latest default Running Running 13 seconds ago lxtyceb71nk9 web.3 nginx:latest default-2 Running Running 13 seconds ago yd7ijk5qyqaa web.4 nginx:latest default-3 Running Running 13 seconds ago q84hr00ompib web.5 nginx:latest default-3 Running Running 13 seconds ago Rollilng Update docker service inspect [OPTIONS] SERVICE [SERVICE...]로 생성된 Serivce의 상세정보를 확인합니다. 해당 Service는 Latest Version(1.13.6)의 Nginx Image를 바탕으로 생성된 것을 확인할 수 있습니다.\ndocker@default:~$ docker service inspect --pretty web ID: 1lc92552oo4mxh25l4iur54pj Name: web Service Mode: Replicated Replicas: 5 Placement: UpdateConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Update order: stop-first RollbackConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Rollback order: stop-first ContainerSpec: Image: nginx:latest@sha256:9fca103a62af6db7f188ac3376c60927db41f88b8d2354bf02d2290a672dc425 Resources: Endpoint Mode: vip Ports: PublishedPort = 80 Protocol = tcp TargetPort = 80 PublishMode = ingress Service의 Image를 docker service update [OPTIONS] SERVICE를 사용하여 다른 Version의 Image로 변경합니다.\ndocker@default:~$ docker service update --image nginx:1.12.2 web web overall progress: 5 out of 5 tasks 1/5: running [==================================================\u0026gt;] 2/5: running [==================================================\u0026gt;] 3/5: running [==================================================\u0026gt;] 4/5: running [==================================================\u0026gt;] 5/5: running [==================================================\u0026gt;] verify: Service converged Service가 Update되었는지, docker service ps [OPTIONS] SERVICE [SERVICE...]로 확인합니다. 기존 Task의 상태는 Shutdown으로 변경되었으며, 각각의 REPLICA마다 새로운 Task들이 실행중임을 확인할 수 있습니다.\ndocker@default:~$ docker service ps web ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS p3zonnmr967b web.1 nginx:1.12.2 default-2 Running Running about a minute ago u3pmniq55z8a \\_ web.1 nginx:latest default Shutdown Shutdown 2 minutes ago t6tlduvxpjlx web.2 nginx:1.12.2 default-2 Running Running 40 seconds ago p8i67ito9574 \\_ web.2 nginx:latest default Shutdown Shutdown 41 seconds ago oww8cmtlnc14 web.3 nginx:1.12.2 default-3 Running Running 44 seconds ago lxtyceb71nk9 \\_ web.3 nginx:latest default-2 Shutdown Shutdown 44 seconds ago yl4af19swwk7 web.4 nginx:1.12.2 default Running Running 48 seconds ago yd7ijk5qyqaa \\_ web.4 nginx:latest default-3 Shutdown Shutdown about a minute ago yj7q0bxxm2zh web.5 nginx:1.12.2 default-3 Running Running 2 minutes ago q84hr00ompib \\_ web.5 nginx:latest default-3 Shutdown Shutdown 3 minutes ago 추가적으로, docker service inspect [OPTIONS] SERVICE [SERVICE...]로 Service의 Image에 대한 상세정보를 확인합니다. Image가 지정한 Version의 Image로 변경되었음을 확인할 수 있습니다.\ndocker@default:~$ docker service inspect --pretty web ID: 1lc92552oo4mxh25l4iur54pj Name: web Service Mode: Replicated Replicas: 5 UpdateStatus: State: completed Started: 3 minutes ago Completed: 28 seconds ago Message: update completed Placement: UpdateConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Update order: stop-first RollbackConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Rollback order: stop-first ContainerSpec: Image: nginx:1.12.2@sha256:5269659b61c4f19a3528a9c22f9fa8f4003e186d6cb528d21e411578d1e16bdb Resources: Endpoint Mode: vip Ports: PublishedPort = 80 Protocol = tcp TargetPort = 80 PublishMode = ingress Roll Back docker service rollback [OPTIONS] SERVICE로 이전 상태의 버전으로 복구합니다.\ndocker@default:~$ docker service rollback web web rollback: manually requested rollback overall progress: rolling back update: 5 out of 5 tasks 1/5: running [\u0026gt; ] 2/5: running [\u0026gt; ] 3/5: running [\u0026gt; ] 4/5: running [\u0026gt; ] 5/5: running [\u0026gt; ] verify: Service converged 추가적으로, docker service inspect [OPTIONS] SERVICE [SERVICE...]로 Service의 Image에 대한 상세정보를 확인합니다. 1.12.2 Version으로 생성된 Service의 상태가 Shutdown으로 변경되었고, 다시 Latest Version으로 Task가 생성되어 Running 상태임을 확인할 수 있습니다.\ndocker@default:~$ docker service ps web ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS ksilhkwx7lkj web.1 nginx:latest default-2 Running Running 21 seconds ago p3zonnmr967b \\_ web.1 nginx:1.12.2 default-2 Shutdown Shutdown 22 seconds ago u3pmniq55z8a \\_ web.1 nginx:latest default Shutdown Shutdown 3 hours ago 9ctk9l8n1lmp web.2 nginx:latest default-2 Running Running 18 seconds ago t6tlduvxpjlx \\_ web.2 nginx:1.12.2 default-2 Shutdown Shutdown 18 seconds ago p8i67ito9574 \\_ web.2 nginx:latest default Shutdown Shutdown 3 hours ago cv5z21ep2u6p web.3 nginx:latest default-3 Running Running 25 seconds ago oww8cmtlnc14 \\_ web.3 nginx:1.12.2 default-3 Shutdown Shutdown 26 seconds ago lxtyceb71nk9 \\_ web.3 nginx:latest default-2 Shutdown Shutdown 3 hours ago xetjjjb5ed5t web.4 nginx:latest default Running Running 10 seconds ago yl4af19swwk7 \\_ web.4 nginx:1.12.2 default Shutdown Shutdown 10 seconds ago yd7ijk5qyqaa \\_ web.4 nginx:latest default-3 Shutdown Shutdown 3 hours ago ftcfg2xwuj8u web.5 nginx:latest default Running Running 14 seconds ago yj7q0bxxm2zh \\_ web.5 nginx:1.12.2 default-3 Shutdown Shutdown 14 seconds ago q84hr00ompib \\_ web.5 nginx:latest default-3 Shutdown Shutdown 3 hours ago 추가적으로, docker service inspect [OPTIONS] SERVICE [SERVICE...]로 Service의 Image에 대한 상세정보를 확인합니다. Image가 초기 지정한 Version의 Image로 변경되었음을 확인할 수 있습니다.\ndocker@default:~$ docker service inspect --pretty web ID: 1lc92552oo4mxh25l4iur54pj Name: web Service Mode: Replicated Replicas: 5 UpdateStatus: State: rollback_completed Started: 4 minutes ago Message: rollback completed Placement: UpdateConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Update order: stop-first RollbackConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Rollback order: stop-first ContainerSpec: Image: nginx:latest@sha256:9fca103a62af6db7f188ac3376c60927db41f88b8d2354bf02d2290a672dc425 Resources: Endpoint Mode: vip Ports: PublishedPort = 80 Protocol = tcp TargetPort = 80 PublishMode = ingress Service 삭제하기 docker service ls [OPTIONS]와 docker service ps [OPTIONS] SERVICE [SERVICE...]로 현재 실행중인 Service 목록을 확인합니다.\ndocker@default:~$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS 1lc92552oo4m web replicated 5/5 nginx:latest *:80-\u0026gt;80/tcp docker@default:~$ docker service ps web ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS ksilhkwx7lkj web.1 nginx:latest default-2 Running Running 21 seconds ago p3zonnmr967b \\_ web.1 nginx:1.12.2 default-2 Shutdown Shutdown 22 seconds ago u3pmniq55z8a \\_ web.1 nginx:latest default Shutdown Shutdown 3 hours ago 9ctk9l8n1lmp web.2 nginx:latest default-2 Running Running 18 seconds ago t6tlduvxpjlx \\_ web.2 nginx:1.12.2 default-2 Shutdown Shutdown 18 seconds ago p8i67ito9574 \\_ web.2 nginx:latest default Shutdown Shutdown 3 hours ago cv5z21ep2u6p web.3 nginx:latest default-3 Running Running 25 seconds ago oww8cmtlnc14 \\_ web.3 nginx:1.12.2 default-3 Shutdown Shutdown 26 seconds ago lxtyceb71nk9 \\_ web.3 nginx:latest default-2 Shutdown Shutdown 3 hours ago xetjjjb5ed5t web.4 nginx:latest default Running Running 10 seconds ago yl4af19swwk7 \\_ web.4 nginx:1.12.2 default Shutdown Shutdown 10 seconds ago yd7ijk5qyqaa \\_ web.4 nginx:latest default-3 Shutdown Shutdown 3 hours ago ftcfg2xwuj8u web.5 nginx:latest default Running Running 14 seconds ago yj7q0bxxm2zh \\_ web.5 nginx:1.12.2 default-3 Shutdown Shutdown 14 seconds ago q84hr00ompib \\_ web.5 nginx:latest default-3 Shutdown Shutdown 3 hours ago docker service rm SERVICE [SERVICE...]으로 Service를 삭제 합니다.\ndocker@default:~$ docker service rm web web Service가 정상적으로 삭제되었는지 docker service ls [OPTIONS]와 docker service ps [OPTIONS] SERVICE [SERVICE...]로 확인합니다.\ndocker@default:~$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS docker@default:~$ docker service ps web no such service: web Docker Swarm의 Service는 배포된 Application들의 고가용성을 보장하고 있습니다. Rolling Update와 Roll Back 기능을 제공함으로써, 배포 시 발생할 수 있는 서비스 중단 상황에 대한 대비도 할 수 있도록 되어 있으며, 정의된 Replica의 개수만큼 정상동작하는 Container들을 유지시켜주기도 합니다. 또한, Traffic에 따라 Load Balancing이 적절히 이루어지도록 관리됩니다. 이 모든 것들이 유기적으로 동작할 수 있었던 점은, Cluster Node간 또는 배포된 Service나 Container간 연결된 Docker의 Network 구조 때문이라 생각할 수 있습니다. 따라서, 다음 챕터에서는 Docker의 Network에 대해서 알아보도록 하겠습니다.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/docker/swarm/",
        "title": "[Docker 기본(6/8)] Docker의 Container Ochestartion: Swarm",
        "tags": ["docker", "swarm", "cluster", "container ochestration"],
        "description": "",
        "content": " 지금까지는 단일 Docker Machine에서 Container를 실행시켜 서비스를 제공했습니다. 그렇다면, 현 상태로 실제 서비스를 운영할 수 있을까요? 한참 부족합니다! 장애없이 원활한 서비스를 제공하기 위해서는, 내결함성, 고가용성 등등 많은 사항들을 고려하여야 합니다. 결국은 단일 Machine으로는 사용할 수 없고 Cluster 형태로 구성되어야하며, 그 위에서 서비스들은 여러개의 Instance로 부하가 분산되어야 하며, 장애가 발생하더라도 스스로 복구 할 수 있는 구조로 실행되어야 합니다. 마지막으로 이것들이 자동화되어 있어야합니다. 수 십개에서 수 백개로 나뉘어진 Service들을 사람이 직접 관리할 수 없기 때문입니다.\nContainer Ochestration: Docker Swarm mode 이 때, 필요한 것이 Container Ochestration이며, Docker에서 제공하고 있는 Tool이 바로 Swarm이라 할 수 있습니다. Docker Swarm은 Docker Engine에 Ochestration Layer를 구현하여, Cluster 관리 및 Cluster에 배포되는 Container들에 대한 Resource, Network, Security, Fault Torelance등 Ochestartion 기능을 사용할 수 있게 해줍니다(영어 뜻 그대로, Container들을 지휘하는 지휘자 또는 조율자로 기억하면 쉽게 이해가 될 것 같습니다).\nContainer Ochestration Tool은 Swarm만 있는 것은 아닙니다. Google에서 Kubernetes, Aphace Mesos등이 있으며, 더 확장해서 Ochestation Tool들을 관리해주는 Rancher라는 것도 있습니다. 현재는 Kubernetes가 가장 많이 사용되고 있으며, 가장 탁월한 성능을 자랑한다고 볼 수 있습니다. 반면 Docker Swarm은 제 개인적인 의견으로는 Kubernetes 보다 가볍고, 사용하기 쉽지만\u0026hellip; Kubernetes의 성능과 기능에 비해서 대규모 서비스 운영에 대해서는 부족하다고 생각됩니다. 저도 현재는 Kubernetes를 사용하고 있지만, Container Ochestration에 대한 이해와 Local 환경에서의 직접 조작해볼 수 있는 것(Kubernetes보다는 쉽게 Cluster를 구성하여 Container Ochestration을 접해볼 수 있다)은 Swarm이 간편하게 할 수 있기 때문에 정리해보고자 합니다.\nKubernetes에 대한 내용은 Tag: Kubernetes에서 확인할 수 있습니다.\n Architecture 먼저, Swarm은 다음과 같은 구조로 설계되어 있습니다.\n출처: Docker Swarm Architecture\n Docker Swarm은 Swarm Mode에서 실행되고 있는 Manager Node 및 Worker Node로 동작하는 여러 Docker Host들로 구성됩니다. 여기서, Docker Host는 Manager, Worker 또는 두 역할 모두 수행 할 수 있습니다.\nNodes Node는 Swarm에 참여하는 Docker Engine의 Instance들 입니다. 하나의 물리적인 Server 또는 Cloud Server에서 하나 이상의 Node를 실행할 수 있지만, 일반적으로 Production 환경에서의 Swarm은 여러 물리적인 Server와 Cloud System에 걸쳐 분산된 Docker Node들이 배포되는 형태로 사용됩니다.\nManager Node Application을 Swarm에 배포하기 위해서는, 먼저 Manager Node에 Service라는 이름의 정의서를 전달해야 합니다. 그 후, Manager Node는 Worker Node들에 Task라고 불리는 작업 단위를 전달하여, Service를 정상적으로 생성합니다. 기본적으로, Manager Node는 Service를 Worker Node로 실행하지만, Manager만을 위한 Task를 Manager Node에서만 실행하도록 구성할 수도 있습니다. 또한, Manager Node는 안정적인 Swarm 상태를 유지하는데 필요한 Ochestration 및 Cluster 관리 기능들을 수행합니다. 많은 Manager Node들 중에서 Ochestration 작업을 수행할 단일 리더를 선출하기도 합니다.\n Cluster 상태 유지 Scheduling Service Swarm Mode의 HTTP API Endpoints 제공  Worker Node Worker Node는 Manager Node로부터 Task를 수신받고 Container를 실행합니다. 즉 Schedule에 대한 결정을 내리거나 Swarm Mode의 HTTP API를 제공하지 않고, 오로지 Container를 실행시키는 것이 유일한 목적인 Docker Instance입니다. Worker Node에서는 각각의 Agent들이 존재하며, Worker Node에서 실행되고 할당된 Task의 상태에 대해서 Manager Node에 알립니다. 이를 통해, Manager Node가 Worker Node들이 안정적인 상태를 유지할 수 있도록 합니다.\nManager와 Worker Node 구성 Docker Cluster 즉, Swarm Mode를 사용하는 가장 큰 이유 중 하나는 바로 Fault-Tolerance 이점을 활용하기 위해서라고 할 수 있을 것입니다. 이를 위해서, Docker의 고가용성 요구사항에 따라 홀수개의 Node로 구성하는 것을 권장하고 있습니다. Swarm을 Multi Manager로 구성할 경우, Manager Node의 중단 시간 없이 Manager Node에서 발생한 오류를 복구할 수 있습니다. 다음을 참고하여 Swarm의 Manager Node를 구성할 수 있습니다.\n 3-Manager Swarm은 최대 1개 Manager Node의 손실에 대해서 허용 가능 5-Manager Swarm은 최대 2개 Manager Node의 동신 손실에 대해서 허용 가능 n 개의 Manager Cluster는 최대 (n-1)/2개의 Manager Node의 손실에 대해서 허용 가능 Docker는 최대 7개의 Manager Node를 구성할 것을 권장합니다.  더 많은 Manager Node를 추가한다고 해서, Swarm의 확장성 또는 성능이 향상된다는 의미는 아닙니다.\n Manager Node 하나로만 구성된 Swarm을 구성할 수 있지만, 반대로 Manager Node 없이 Worker Node로만 구성할 수는 없습니다. 기본적으로, 모든 Manager Node들은 Worker의 역할도 같이 수행합니다. Single Manager Node로 구성된 Cluster에서, docker service create와 같은 명령을 실행하면, 스케쥴러는 모든 Task를 Local Engine에 배치시킵니다.\nNode간 역할 변경 docker node promete를 사용하여, Worker Node를 Manager Node로 변경할 수 있습니다. 예를들어, Manager Node를 오프라인 상태로 유지해야 할 필요가 있을 때, 임시로 Worker Node를 Manager Node로 변경하여 사용할 수 있을 것입니다. 반대로, docker node demote를 사용하여, Manager Node를 Worker Node로 강등시킬 수도 있습니다.\nDocker Swarm 구성 이제부터는 Container Ochestration을 위한 Docker Swarm 환경을 구성해보겠습니다. 구성하고자할 Docker는 Manager Node 1대, Worker Node 2대 입니다.\nSwarm Mode를 사용하기 위해서는 Docker 1.12.0 이상의 Version을 설치하시기 바랍니다.\n Docker Server 구성 Swarm을 구성하기 위해, docker-machine으로 Docker Server를 3대 구성합니다.\n$ docker-machine create --driver virtualbox default Running pre-create checks... (default) Default Boot2Docker ISO is out-of-date, downloading the latest release... (default) Latest release for github.com/boot2docker/boot2docker is v17.10.0-ce (default) Downloading /Users/1000jaeh/.docker/machine/cache/boot2docker.iso from https://github.com/boot2docker/boot2docker/releases/download/v17.10.0-ce/boot2docker.iso... (default) 0%....10%....20%....30%....40%....50%....60%....70%....80%....90%....100% Creating machine... (default) Copying /Users/1000jaeh/.docker/machine/cache/boot2docker.iso to /Users/1000jaeh/.docker/machine/machines/default/boot2docker.iso... (default) Creating VirtualBox VM... (default) Creating SSH key... (default) Starting the VM... (default) Check network to re-create if needed... (default) Waiting for an IP... Waiting for machine to be running, this may take a few minutes... Detecting operating system of created instance... Waiting for SSH to be available... Detecting the provisioner... Provisioning with boot2docker... Copying certs to the local machine directory... Copying certs to the remote machine... Setting Docker configuration on the remote daemon... Checking connection to Docker... Docker is up and running! To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env default $ docker-machine create --driver virtualbox default-2 Running pre-create checks... Creating machine... (default-2) Copying /Users/1000jaeh/.docker/machine/cache/boot2docker.iso to /Users/1000jaeh/.docker/machine/machines/default-2/boot2docker.iso... (default-2) Creating VirtualBox VM... (default-2) Creating SSH key... (default-2) Starting the VM... (default-2) Check network to re-create if needed... (default-2) Waiting for an IP... Waiting for machine to be running, this may take a few minutes... Detecting operating system of created instance... Waiting for SSH to be available... Detecting the provisioner... Provisioning with boot2docker... Copying certs to the local machine directory... Copying certs to the remote machine... Setting Docker configuration on the remote daemon... Checking connection to Docker... Docker is up and running! To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env default-2 $ docker-machine create --driver virtualbox default-3 Running pre-create checks... Creating machine... (default-3) Copying /Users/1000jaeh/.docker/machine/cache/boot2docker.iso to /Users/1000jaeh/.docker/machine/machines/default-3/boot2docker.iso... (default-3) Creating VirtualBox VM... (default-3) Creating SSH key... (default-3) Starting the VM... (default-3) Check network to re-create if needed... (default-3) Waiting for an IP... Waiting for machine to be running, this may take a few minutes... Detecting operating system of created instance... Waiting for SSH to be available... Detecting the provisioner... Provisioning with boot2docker... Copying certs to the local machine directory... Copying certs to the remote machine... Setting Docker configuration on the remote daemon... Checking connection to Docker... Docker is up and running! To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env default-3 생성된 Docker Machine의 목록을 확인합니다.\n$ docker-machine ls NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS default - virtualbox Running tcp://192.168.99.100:2376 v17.10.0-ce default-2 - virtualbox Running tcp://192.168.99.101:2376 v17.10.0-ce default-3 - virtualbox Running tcp://192.168.99.102:2376 v17.10.0-ce  Manager Node 선정 구성된 docker-machine 중 하나에 ssh로 접속하여, docker swarm init [OPTIONS] 명령어를 통해서 Manager Node로 설정합니다.\n$ docker-machine ssh default ## . ## ## ## == ## ## ## ## ## === /\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\\___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\_______/ _ _ ____ _ _ | |__ ___ ___ | |_|___ \\ __| | ___ ___| | _____ _ __ | \u0026#39;_ \\ / _ \\ / _ \\| __| __) / _` |/ _ \\ / __| |/ / _ \\ \u0026#39;__| | |_) | (_) | (_) | |_ / __/ (_| | (_) | (__| \u0026lt; __/ | |_.__/ \\___/ \\___/ \\__|_____\\__,_|\\___/ \\___|_|\\_\\___|_| Boot2Docker version 17.10.0-ce, build HEAD : 34fe485 - Wed Oct 18 17:16:34 UTC 2017 Docker version 17.10.0-ce, build f4ffd25 docker@default:~$docker swarm init --advertise-addr 192.168.99.100 Swarm initialized: current node (xhegxfmzkjrqgrv9594zhvhsd) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-36uho8g8hn0dunpx85sx700ahvx27gd18b5x4gfqlf0fcz7fmk-569z62jorhk2d12lhdwjkt6t3 192.168.99.100:2377 To add a manager to this swarm, run \u0026#39;docker swarm join-token manager\u0026#39; and follow the instructions. Work Node 구성 Manager Node로 선정한 docker-machine 이외의 docker-machine들을 ssh로 접속하여 docker swarm join [OPTIONS] HOST:PORT 명령어를 통해 Worker Node로 지정합니다. docker swarm join은 docker swarm init 실행 후, Manager Node에 접속할 수 있는 Token과 Host, Port정보가 담겨서 자동으로 생성됩니다.\n docker-default-2\n$ docker-machine ssh default-2 ## . ## ## ## == ## ## ## ## ## === /\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\\___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\_______/ _ _ ____ _ _ | |__ ___ ___ | |_|___ \\ __| | ___ ___| | _____ _ __ | \u0026#39;_ \\ / _ \\ / _ \\| __| __) / _` |/ _ \\ / __| |/ / _ \\ \u0026#39;__| | |_) | (_) | (_) | |_ / __/ (_| | (_) | (__| \u0026lt; __/ | |_.__/ \\___/ \\___/ \\__|_____\\__,_|\\___/ \\___|_|\\_\\___|_| Boot2Docker version 17.10.0-ce, build HEAD : 34fe485 - Wed Oct 18 17:16:34 UTC 2017 Docker version 17.10.0-ce, build f4ffd25 docker@default-2:~$docker swarm join --token SWMTKN-1-36uho8g8hn0dunpx85sx700ahvx27gd18b5x4gfqlf0fcz7fmk-569z62jorhk2d12lhdwjkt6t3 192.168.99.100:2377 This node joined a swarm as a worker.  docker-defulat-3  $ docker-machine ssh default-3 ## . ## ## ## == ## ## ## ## ## === /\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\\___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\_______/ _ _ ____ _ _ | |__ ___ ___ | |_|___ \\ __| | ___ ___| | _____ _ __ | \u0026#39;_ \\ / _ \\ / _ \\| __| __) / _` |/ _ \\ / __| |/ / _ \\ \u0026#39;__| | |_) | (_) | (_) | |_ / __/ (_| | (_) | (__| \u0026lt; __/ | |_.__/ \\___/ \\___/ \\__|_____\\__,_|\\___/ \\___|_|\\_\\___|_| Boot2Docker version 17.10.0-ce, build HEAD : 34fe485 - Wed Oct 18 17:16:34 UTC 2017 Docker version 17.10.0-ce, build f4ffd25 docker@default-3:~$docker swarm join --token SWMTKN-1-36uho8g8hn0dunpx85sx700ahvx27gd18b5x4gfqlf0fcz7fmk-569z62jorhk2d12lhdwjkt6t3 192.168.99.100:2377 This node joined a swarm as a worker.  Docker Swarm 구성 확인 Manager Node에 접속하여 docker node ls [OPTIONS]로 Docker의 Node 구성 목록을 확인합니다. Manager Node는 별도로 표시되어 있는 것을 확인할 수 있습니다.\ndocker@default:~$ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS xhegxfmzkjrqgrv9594zhvhsd * default Ready Active Leader s06l9jsl6ejucnpvkwh9pyr2d default-2 Ready Active 9v5f2zdbof0hdbb82kvaeizyo default-3 Ready Active 지금까지 진행하면, Docker Swarm을 관리하는 Manager Node 1개, 실질적으로 Container들이 배포되어 실행되는 Worker Node 2개가 묶여 Cluster 환경을 구성한 것입니다. 이런 Swarm 환경에서는 처음에 언급했다시피 Container를 직접 실행시키는 것이 아니라, Service라는 단위로 배포를 합니다. 다음에는 Service를 배포하여, Container들이 Docker Cluster 환경에서 어떻게 동작하는지 확인해보겠습니다.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/docker/volume/",
        "title": "[Docker 기본(5/8)] Volume을 활용한 Data 관리",
        "tags": ["docker", "volume"],
        "description": "",
        "content": " 우리는 Container의 Writable Layer에 Data를 저장할 수 있다는 것을 알고 있습니다. 하지만, 여기에는 몇 가지 문제점이 존재합니다.\n Container가 삭제되면 Data도 같이 삭제됩니다. 또한, 다른 프로세스에서 Container에 저장된 Data를 사용하기 어렵습니다. Container의 Writable Layer에는 Container가 실행 중인 Host Machine과 밀접하게 연결됩니다. 따라서, Data를 다른 곳으로 쉽게 옮길 수 없습니다. Container의 Writable Layer에 Data를 저장하기 위해서는 File System을 관리하는 Storage Driver가 필요합니다. Storage Driver는 Linux 커널을 사용하여 공용 File System을 제공합니다. 이 기능은 Host File System에 직접 쓰는 data volume보다 성능이 떨어집니다.  Docker는 Data를 안전하게 존속시킬 수 있는 방식으로 volume, bind mounts, tmpfs의 3가지 방식을 제공합니다(어떤 것을 사용해야할 지 모를 때는 volume를 사용하시기 바랍니다). 아래는 Container의 Data 관리 방식들과 사용 사례에 대해서 자세히 살펴보도록 하겠습니다.\n올바른 Mount 유형 선택 방법 어떤 유형의 Mount를 사용하든, Data는 Container 내에서 동일하게 보이며, Container File System의 폴더나 개별적인 파일들로 표시됩니다. 올바른 Mount유형을 선택할 때 기준이 될 volume, bind mounts, tmpfs mount간의 가장 큰 차이점은, Data가 Docker Host내에서 어디에 존재하는지 입니다.\n출처: Docker Docs - Use volumes\n Data 저장을 위한 최선의 선택  volume는 Docker(Linux에서는 /var/lib/docker/volume/)가 관리하는 Host File System의 일부에 Data가 저장됩니다. Non-Docker 프로세스들이 File System의 해당 부분을 수정해서는 안됩니다. Docker에서 Data를 존속시킬 수 있는 Best한 방법입니다.  Host의 File System 원하는 곳에 저장  bind mount는 Data가 Host System의 어디에든지 저장될 수 있습니다. 저장되는 Data는 System File이거나 Directory일 수 있습니다. Docker Host 또는 Docker Container의 Non-Docker 프로세서들이 언제든지 저장된 Data를 수정할 수 있습니다.  Host System의 Memory에 저장  tmpfs mount는 Host System의 Memory에만 Data가 저장되며, 절대로 Host의 File System에는 저장되지 않습니다.  Mount 유형 각 Mount 유형에 대해서 자세히 살펴보겠습니다.\nvolume  Docker가 생성하고 관리하는 방식입니다. docker volume create 명령을 사용하여 명시적으로 voluems를 생성하거나, Container나 Service 생성 중에 volume을 생성할 수 있습니다. volume이 생성되면, Data는 Docker Host의 디렉토리에 저장됩니다. 해당 volume을 Container에 Mount하면, Host의 디렉토리가 Mount가 됩니다. 이는 volume이 Docker에 의해 관리되고 Host System과 분리된다는 점을 제외한다면, bind mount와 유사하게 동작합니다. 동시에 volume을 여러 Container에 Mount할 수 있습니다. 실행 중인 Container가 volume을 사용하지 않아도, 해당 volume은 Docker에서 계속 사용할 수 있으며, 자동으로 삭제되지 않습니다. docker volume prune을 사용하여 사용하지 않는 volume을 정리할 수 있습니다. volume을 Mount할 때, 이름을 명시적으로 지정하여 사용할 수 있으며, 익명으로도 사용할 수 있습니다. 익명 volume은 처음 Mount될 때 명시적으로 이름이 부여되지 않기 때문에, Docker는 주어진 Docker Host내에서 고유한 임의의 이름을 해당 volume에 부여합니다. 이름이 지정된 방식과는 상관 없이 volume은 동일한 방식으로 작동합니다. 또한, volume은 원격 Host와 Cloud Provider가 Data를 저장할 수 있는 volume drivers 사용을 지원하고 있습니다.  bind mount  bind mount는 Docker 초기부터 사용할 수 있었던 방식으로 volume에 비해 기능이 제한적입니다. bind mount를 사용하면, Host System의 파일 또는 디렉토리가 Container에 Mount됩니다. 파일 또는 디렉토리는 Host System의 전체 경로로 참조되지만, 미리 Docker Host에 존재할 필요는 없습니다. 없을 경우, 참조된 경로로 파일 또는 디렉토리가 생성됩니다. bind mount는 매우 효과적이지만, Host Machine의 File System 디렉토리 구조에 의존적입니다. Docker CLI 명령어로 bind mount를 관리할 수 없습니다.  bind mount는 Container에서 실행 중인 프로세스들이 Host File System의 중요한 시스템 파일들이나 디렉토리의 생성, 수정, 삭제 명령을 통해 변경시킬 수 있습니다. 이는 Host System의 Non-Docker 프로세스들에게 충돌이 발생하거나, 보안에 큰 영향을 줄 수 있습니다. 따라서, bind mount보다는 volume를 사용하는 것을 권장합니다.\n tmfs mount  tmpfs mount는 Docker Host 또는 Container내의 디스크에서 Data가 유지되지 않습니다. 비영구적인 상태 정보나 민감 정보들 같이 Container의 생명주기와 맞춰서 Data를 보존하고자 할 때 사용할 수 있습니다. 예를 들어, Docker Cluster인 Swarm Service는 내부적으로 tmps mount를 사용하여 Secret 정보를 Service의 Container에 Mount하여 사용합니다.  bind mount 및 volume는 -v 또는 --volume Flag를 사용하여 Container에 모두 Mount할 수 있지만, 각 구문은 약간 차이가 있습니다. tmpfs mount의 경우, --tmpfs Flag를 사용합니다. 그러나 Docker 17.06 이상에서는 bind mount, volume, tmpfs mount에 대해 Container와 Service 모두에 --mount Flag를 사용하는 것이 구문이 더 명확이기 때문에 권장합니다.\nMount 유형별 사용 사례 volume 사용 사례 volume은 Docker Container 및 Service에서 Data를 유지하는 기본적인 방법으로, 다음과 같은 경우에 volume을 사용합니다.\n 실행 중인 여러 Container 간에 Data 공유가 필요한 경우  volume생성에 대해서 명시하지 않은 경우, Container에 처음 Mount될 때 생성됩니다. Container가 중지되거나 제거되더라도 생성된 volume은 그대로 유지됩니다. 여러 Container가 동일한 volume을 읽기/쓰기 또는 읽기 전용으로 동시에 Mount할 수 있습니다. volume은 명시적으로 선언하여 제거합니다.  Docker Host가 특정 디렉토리나 파일 구조를 가질 수 없는 경우  volume은 Container Runtime으로부터 Docker Host의 설정을 분리하는데 도움이 됩니다.  로컬외의 원격 Host 및 Cloud Provider에 Container의 Data를 저장하려는 경우  한 Docker Host에서 다른 Docker Host로 Data를 백업이나 복구 또는 마이그레이션을 하는 경우 volume을 사용하면 더 좋습니다. volume을 사용하여 Container를 중지한 다음 volume의 디렉토리(예: /var/lib/docker/volume/\u0026lt;volume-name\u0026gt;)를 백업할 수 있습니다.   bind mount 사용 사례 일반적인 경우, 가능하면 bind mount보다 volume를 사용하십시오. 다음과 같은 경우에 bind mount를 사용합니다.\n Host Machine에서 Container로 설정 파일을 공유해야하는 경우  Docker는 Host Machine의 /etc/resolv.conf를 각 Container에 bind mount하여 DNS Resolution을 제공하고 있습니다.  Docker Host 개발 환경과 Container 간 소스코드 또는 빌드된 아티팩트를 공유  예를 들어, Maven의 target/ 디렉토리를 Container에 Mount하고, Docker Host에서 Maven Project를 빌드할 때마다, Container에서 재 작성된 작성된 JAR/WAR에 접근할 수 있습니다. 만약 이런 방식으로 개발을 진행한다면, Production Dockerfile은 bind mount에 의존하지 않고도, Production-Ready 아티팩트를 Image에 직접 복사할 수 있습니다.  Docker Host의 파일 또는 디렉토리 구조가 Container가 요구하는 bind mount와 일치하는 경우  volume 또는 bind mounts 사용 시, 유의사항 volume 또는 bind mount를 사용하는 경우, 다음의 사항에 대해 유의하시기 바랍니다.\n File 또는 Directory 있는 Container 내의 Directory에 비어있는 volume를 Mount하면, 해당 File 또는 Directory가 volume으로 전달(복사)됩니다. 마찬가지로, 아직 존재하지 않는 특정 volume을 지정하고 Container를 시작하면, 비어있는 volume이 생성됩니다. 이는 다른 Container에 필요한 Data를 미리 갖출 수 있는 좋은 방법입니다. File 또는 Directory가 있는 Container 내의 Directory에 bind mount 또는 비어있지 않은 volume를 Mount한다면, Linux Host에서 File을 /mnt에 저장하고 나서 USB 드라이브가 /mnt에 Mount되는 것처럼 기존에 존재하던 File 또는 Directory가 Mount에 의해 가려지게 됩니다. /mnt의 컨텐츠들은 USB 드라이브가 Unmount될 때까지, USB 드라이브의 컨텐츠들에 의해서 가려지게 됩니다. 가려진 파일들은 제거되거나 대체된 것은 아니지만, bind mount 또는 volume이 Mount되어 있는 동안은 접근할 수 없습니다.  tmpfs mount 사용 사례 tmpfs mount는 Host System이나 Container 내에서 Data를 유지하지 않아야할 때 가장 적합합니다. 이는 보안상의 이유가 될 수도 있고, Application이 많은 양의 비영구적인 상태의 Data를 작성해야 할 때, Container의 성능을 보호하기 위한 것일 수도 있습니다.\nvolume Mount 하기 volume을 사용하기 전에, volume을 Mount하기 위한 Flag에 대해서 확인해 보겠습니다.\n-v 또는 \u0026ndash;mount 선택 원래는 독립형 Container에서는 -v 또는 --volume가 사용되었고, Docker Cluster인 Swarm Mode의 Service에서는 --mount가 사용되었습니다. 하지만, Docker 17.06부터 독립형 Container에서도 --mount를 사용할 수 있게 되었습니다. 두 Flag 간 가장 큰 차이점은 -v구문은 모든 옵션들을 하나의 Field에 결합하여 사용하고, --mount구문은 옵션을 분리하여 사용합니다. 따라서, --mount가 보다 명확하고 자세하게 정보를 확인할 수 있습니다.\n-v 또는 --volume 보다 --mount의 사용성이 더 쉽기 때문에 --mount를 사용하는 것을 권장합니다. 만약 특정 volume driver 옵션들이 필요하다면, --mount를 사용해야 합니다.\n 다음은 Flag별 사용법입니다.\n-v 또는 --volume  콜론(:)으로 구분된 세 개의 필드로 구성되어있습니다. 필드의 순서가 정확해야 하며, 각 필드의 의미는 명확하지 않습니다. 이름이 명시된 volume의 경우, 첫번째 필드는 volume의 이름이며, Host Machine에서 유일해야 합니다. 익명 volume의 경우, 첫번째 필드는 생략됩니다. 두번째 필드는 File 또는 Direcoty가 Container에 Mount될 경로입니다. 세번째 필드는 선택사항이며, 쉼표(,)로 구분된 옵션 목록(예: ro(읽기전용을 나타내는 옵션))입니다.  --mount  쉼표(,)로 구분되고 각각 여러 key-value 쌍으로 구성됩니다. --mount 구문은 -v 또는 --volume보다 길지만, key의 순서는 중요치 않으며 Flag의 값을 이해하기 쉽게 되어있습니다. type은 bind, volume, tmpfs로 Mount 유형을 지정합니다. source는 volume의 이름입니다. 익명 volume을 사용하고자 할 때는, 해당 필드를 생략해도 됩니다. source 또는 src로 지정할 수 있습니다. destination은 파일 또는 디렉토리가 Container에 Mount될 경로를 나타냅니다. destination, dst 또는 target으로 지정할 수 있습니다. readonly 옵션이 있으면 Container에 읽기 전용으로 Mount됩니다. 두 번 이상 지정할 수 있는 volume-opt옵션은 옵션 이름과 해당 값으로 구성된 key-value 쌍을 사용합니다.  -v와 \u0026ndash;mount 사이의 동작 차이 bind mount와 달리, volume의 모든 옵션들을 --mount와 -v Flag에서 사용할 수 있습니다. 단, Service에서 volume을 사용하고자 한다면, --mount만 지원됩니다.\nvolume 생성 및 삭제하기 docker volume create [OPTIONS] [VOLUME]로 volume을 생성합니다.\n$ docker volume create my-vol my-vol docker volume ls [OPTIONS]로 volume의 전체 목록을 확인합니다.\n$ docker volume ls DRIVER VOLUME NAME local c6d60670aa3b05a06956839587ce608c67dbc0b14e5f13590f526ff149382bb0 local my-vol docker volume inspect [OPTIONS] VOLUME [VOLUMES...]로 생성한 volume의 상세 정보를 확인합니다.\n$ docker volume inspect my-vol [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2017-11-10T06:08:36Z\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: {}, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volume/my-vol/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;my-vol\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] docker volume rm [OPTIONS] VOLUME [VOLUME...]로 생성한 volume를 삭제합니다.\n$ docker volume rm my-vol my-vol Container에 volume Mount하기 --mount Flag를 사용하여 Container 기동 시, volume을 Mount합니다.\n$ docker run -d \\ \u0026gt; -it \\ \u0026gt; --name devtest \\ \u0026gt; --mount source=myvol2,target=/app \\ \u0026gt; nginx:latest Unable to find image \u0026#39;nginx:latest\u0026#39; locally latest: Pulling from library/nginx bc95e04b23c0: Pull complete a21d9ee25fc3: Pull complete 9bda7d5afd39: Pull complete Digest: sha256:9fca103a62af6db7f188ac3376c60927db41f88b8d2354bf02d2290a672dc425 Status: Downloaded newer image for nginx:latest 6de449868f75c83a425ae33f46dde69f3287edb56af88dc616582fb0c3622166 생성된 Container의 상세 정보를 확인합니다. Mounts 필드에서 volume에 대한 상세정보를 확인할 수 있습니다. Mount 형식은 volume으로 source 및 destination의 경로를 확인할 수 있으며, 읽기/쓰기가 모두 가능한 것을 확인할 수 있습니다.\n$ docker inspect devtest [ { \u0026#34;Id\u0026#34;: \u0026#34;6de449868f75c83a425ae33f46dde69f3287edb56af88dc616582fb0c3622166\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2017-11-10T06:17:32.273492731Z\u0026#34;, \u0026#34;Path\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;Args\u0026#34;: [ \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34; ], ... 생략 ... \u0026#34;Mounts\u0026#34;: [ { \u0026#34;Type\u0026#34;: \u0026#34;volume\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;myvol2\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;/var/lib/docker/volume/myvol2/_data\u0026#34;, \u0026#34;Destination\u0026#34;: \u0026#34;/app\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;z\u0026#34;, \u0026#34;RW\u0026#34;: true, \u0026#34;Propagation\u0026#34;: \u0026#34;\u0026#34; } ], ... 생략 ... } } ] Container를 중지시키고, 생성한 Container와 volume을 제거합니다.\n$ docker container stop devtest devtest $ docker container rm devtest devtest $ docker volume rm myvol2 myvol2 Docker Cluster인 Swarm Mode의 Service도 Container와 동일한 방식으로 volume을 사용할 수 있습니다. Service를 시작하고 volume을 정의하면, 각 Service Container들은 자체 로컬 volume을 사용합니다. 로컬 volume driver를 사용하는 경우, Container들 중 어느 것도 해당 Data들을 공유할 수 없지만 일부 volume driver는 공유 Storage를 지원합니다. Docker for AWS와 Docker for Azure는 모두 Cloudstor 플러그인을 사용하여 영구 저장소를 지원합니다.\nContainer로 volume에 Data 적재하기 Container를 시작할 때 새로운 volume을 생성하고, 해당 Container에 Mount할 Directory 내에 File 또는 Directory가 있으면(예: /app/) Directory의 내용이 volume으로 복사됩니다. 그러면 Container는 volume을 Mount하여 사용하고, 다른 Container도 미리 채워진 volume내의 컨턴츠에 접근할 수 있습니다.\n먼저 Container를 실행할 때, destination을 /usr/share/nginx/html로 지정하여, 해당 Directory의 내용으로 새로운 nginx-vol라는 이름의 volume에 Data가 채워질 수 있도록 설장합니다. 해당 Directory는 Nginx가 기본 HTML 컨텐츠를 저장하는 곳입니다.\n$ docker run -d \\ \u0026gt; -it \\ \u0026gt; --name=nginxtest \\ \u0026gt; --mount source=nginx-vol,destination=/usr/share/nginx/html \\ \u0026gt; nginx:latest 777e64d14c6ae173c00372d23130e43fe8d14f6160c94ef4dd7ac7b247ec110d 생성된 Container의 상세 정보를 확인합니다. Mounts 필드에서 volume에 대한 상세정보를 확인할 수 있습니다.\n$ docker inspect nginxtest [ { \u0026#34;Id\u0026#34;: \u0026#34;777e64d14c6ae173c00372d23130e43fe8d14f6160c94ef4dd7ac7b247ec110d\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2017-11-10T06:34:08.21801753Z\u0026#34;, \u0026#34;Path\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;Args\u0026#34;: [ \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34; ], ... 생략 ... \u0026#34;Mounts\u0026#34;: [ { \u0026#34;Type\u0026#34;: \u0026#34;volume\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;nginx-vol\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;/var/lib/docker/volume/nginx-vol/_data\u0026#34;, \u0026#34;Destination\u0026#34;: \u0026#34;/usr/share/nginx/html\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;z\u0026#34;, \u0026#34;RW\u0026#34;: true, \u0026#34;Propagation\u0026#34;: \u0026#34;\u0026#34; } ], ... 생략 ... } } ] Container를 중지시키고, 생성한 Container와 volume을 제거합니다.\n$ docker container stop nginxtest nginxtest $ docker container rm nginxtest nginxtest $ docker volume rm nginx-vol nginx-vol 읽기전용 volume Mount하기 일부 Application 개발의 경우, Container가 Docker Host로 변경 사항을 다시 전달할 수 있도록 bind mount하여 사용합니다. 반대로, 다른 Container들은 단지 Data를 읽기만 하여야 하고, 수정하지 못해야 할 때도 있습니다. 즉, 여러 Container가 동일한 volume을 Mount할 수 있으며, 일부 Container는 읽기/쓰기로, 그 외 Container는 읽기전용으로 Mount하여 사용할 수도 있습니다.\n--mount에 readonly 옵션을 추가하여 Container와 volume을 생성합니다.\n$ docker run -d \\ \u0026gt; -it \\ \u0026gt; --name=nginxtest \\ \u0026gt; --mount source=nginx-vol,destination=/usr/share/nginx/html,readonly \\ \u0026gt; nginx:latest 44f1138b38c1b4df9b1bb4b4d14e7181af05ee0dcc153e634c7db5b17c509635 생성된 Container의 상세 정보를 확인합니다. Mounts 필드에서 volume에 대한 상세정보를 확인할 수 있습니다. readonly 옵션이 적용되어, RW Key의 value가 false로 되어 있는 것을 확인할 수 있습니다.\n$ docker inspect nginxtest [ { \u0026#34;Id\u0026#34;: \u0026#34;44f1138b38c1b4df9b1bb4b4d14e7181af05ee0dcc153e634c7db5b17c509635\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2017-11-10T06:38:25.060891516Z\u0026#34;, \u0026#34;Path\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;Args\u0026#34;: [ \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34; ], ... 생략 ... \u0026#34;Mounts\u0026#34;: [ { \u0026#34;Type\u0026#34;: \u0026#34;volume\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;nginx-vol\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;/var/lib/docker/volume/nginx-vol/_data\u0026#34;, \u0026#34;Destination\u0026#34;: \u0026#34;/usr/share/nginx/html\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;z\u0026#34;, \u0026#34;RW\u0026#34;: false, \u0026#34;Propagation\u0026#34;: \u0026#34;\u0026#34; } ], ... 생략 ... } } ] Container를 중지시키고, 생성한 Container와 volume을 제거합니다.\n$ docker container stop nginxtest nginxtest $ docker container rm nginxtest nginxtest $ docker volume rm nginx-vol nginx-vol 이렇게 volume을 사용하여, Container의 Data들을 영구적으로 존속시킬 수 있었습니다. 또한 다양한 Driver를 사용하여 NFS 또는 AWS의 S3와 같은 외부 Storage System과의 연동을 통해 Data를 Docker Machine간에 Data를 공유할 수도 있습니다. 이 때, volume driver는 Storage System을 추상화하여 사용하기 때문에, Application의 로직 변경없이도 자유롭게 Data를 유지시킬 수 있습니다.\nContainer란 무엇인가라는 주제부터 시작해서 Volume을 활용한 Container Data관리까지, Application이 Container화하여 어떤 식으로 동작하고, 무엇을 고려해야할지에 대해서 기초적인 항목들을 확인하였습니다. 현재까지는 모든 것들을 단일 Docker Machine 환경으로 한정하여 생각했습니다. 다음에는 Docker Cluster인 Swarm을 구성하여 이런 Container들을 어떤 방식으로 Orchestration할 수 있는지 알아봐야겠네요.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/docker/docker-build-push/",
        "title": "[Docker 기본(4/8)] docker build &amp; push",
        "tags": ["docker", "image", "container", "dockerfile"],
        "description": "",
        "content": " Docker 환경에 Service 및 Application을 구동시키 위한 전체적인 과정은 다음과 같습니다.\ngraph BT; A[Dockerfile] --|build| B[Images] B --|push| C[Docker Registry] C --|pull| B subgraph Local Docker Instance B --|Run| D[Container] end  지금까지는, Docker Registry(Docker Hub)에 배포되어 있는 있는 Image를 이용해 Container를 구동시켰습니다. 이제부터는 위에서 정리한 과정대로, Dockerfile로 Image를 생성하고 다른 Machine에서 배포된 Image로 Container를 실행시켜 보겠습니다.\n본 예제는 About storage drivers에 있는 예제를 참고하여 진행했습니다.\n 신규 Image 생성에서 배포까지 Dockerfile 작성 image-build-test 폴더를 생성한 뒤, 다음의 내용이 포함된 Dockerfile을 생성합니다.\nFROMubuntu:16.10COPY . /app image-build-test 폴더의 내부 구조는 다음과 같습니다.\n/image-build-test $ tree . └── Dockerfile 0 directories, 1 file Image Build image-build-test 폴더 내에서 생성한 Dockerfile로 ${username}/my-base-image:1.0이란 이름의 Image를 Build합니다.\n여기서, ${username}는 Docker에 로그인 가능한 계정입니다.\n /image-build-test $ docker build -t ${username}/my-base-image:1.0 -f Dockerfile . Sending build context to Docker daemon 11.26kB Step 1/2 : FROM ubuntu:16.10 16.10: Pulling from library/ubuntu dca7be20e546: Pull complete 40bca54f5968: Pull complete 61464f23390e: Pull complete d99f0bcd5dc8: Pull complete 120db6f90955: Pull complete Digest: sha256:8dc9652808dc091400d7d5983949043a9f9c7132b15c14814275d25f94bca18a Status: Downloaded newer image for ubuntu:16.10 ---\u0026gt; 7d3f705d307c Step 2/2 : COPY . /app ---\u0026gt; 6087ccdf7873 Successfully built 6087ccdf7873 Successfully tagged ${username}/my-base-image:1.0 처음 Image를 Build하게 되면, Local Instance에 저장된 Base Image(여기서는 Ubuntu Image입니다)가 존재하지 않기 때문에, Docker Registry(Docker Hub)로부터 Pull을 받습니다. 그 후, Dockerfile의 COPY명령어가 새로운 Layer로 생성되고, ${username}/my-base-image:1.0 Image가 Build됩니다.\n생성된 Image를 확인해 보겠습니다.\n/image-build-test $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE ${username}/my-base-image 1.0 6087ccdf7873 22 seconds ago 107MB ubuntu 16.10 7d3f705d307c 8 months ago 107MB Container를 Run 방금 생성한 Image를 Local에서 실행시켜 보겠습니다. 정상적으로 실행된다면, 아래와 같이 Container ID가 나타나고 docker ps로 실행중인 해당 Container를 목록에서 확인할 수 있습니다.\n/image-build-test $ docker run -dit --name ${username}_container ${username}/my-base-image:1.0 bash 8517d3ff5b8a4233ea716190beb73bfdbb4401afee656dff7d694307d304b2e4 /image-build-test $ docker ps -s CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES SIZE 8517d3ff5b8a ${username}/my-base-image:1.0 \u0026#34;bash\u0026#34; 3 seconds ago Up 4 seconds ${username}_container 0B (virtual 107MB) Image를 Docker Hub에 배포 이제는 생성한 Image를 Docker Registry에 배포해 보겠습니다. 따로, Docker Registry를 구성하지 않았기 때문에 배포되는 장소는 Docker Hub이며, 정상적으로 배포하기 위해서 먼저 Login을 하겠습니다.\n/image-build-test $ docker login Login with your Docker ID to push and pull images from Docker Hub. If you don\u0026#39;t have a Docker ID, head over to https://hub.docker.com to create one. Username (${username}): ${username} Password: Login Succeeded 로그인이 정상적으로 되었다면 docker push로 생성한 Image를 Docker Registry에 등록하겠습니다.\n/image-build-test $ docker push ${username}/my-base-image:1.0 The push refers to repository [docker.io/${username}/my-base-image] 1b9e33c4403d: Pushed fcc11235a441: Mounted from library/ubuntu 0ba4c05a8843: Mounted from library/ubuntu 47a3ebbaa644: Mounted from library/ubuntu 31eed92f5a23: Mounted from library/ubuntu 57145c01eb80: Mounted from library/ubuntu 1.0: digest: sha256:baa607ec007033e47ad2d26c5f38da7d95023efad2a76da2aa857e1e4d5a756f size: 1564 Console에 찍힌 로그를 보면, Unbuntu Image는 이미 Docker Registry에 존재하기 때문에 Mount만 되고, 그 외에 생성한 Layer만 추가로 Push되는 것을 볼 수 있습니다.\nDocker Hub에서 Push된 Image를 확인할 수 있습니다.\n Hello World를 만들어 보자 처음에 hello-world를 실행시켜 봤었습니다. 지금부터는 직접 구현해 보겠습니다.\nDockerfile 변경 기 생성된 Dockerfile을 Dockerfile.base로 변경 한 후, ${username}/my-base-image:1.0 Image에 CMD Layer가 추가된 Image를 생성하는 Dockerfile을 새로 생성합니다.\nFROM${username}/my-base-image:1.0CMD/app/hello.sh 다음과 같은 내용으로 hello.sh를 생성합니다.\n#!/bin/sh echo \u0026#34;Hello world\u0026#34; hello.sh 파일을 저장하고 실행가능한 상태로 변경합니다.\n/image-build-test $ chmod +x hello.sh 변경된 폴더 구조는 다음과 같습니다.\n/image-build-test $ tree . ├── Dockerfile ├── Dockerfile.base └── hello.sh 0 directories, 3 files Hello World Image Build 변경된 Dockerfile로 ${username}/my-final-image:1.0이란 이름의 Image를 Build합니다.\n/image-build-test $ docker build -t ${username}/my-final-image:1.0 -f Dockerfile . Sending build context to Docker daemon 11.26kB Step 1/2 : FROM ${username}/my-base-image:1.0 ---\u0026gt; 6087ccdf7873 Step 2/2 : CMD /app/hello.sh ---\u0026gt; Running in 8b7b0383d636 Removing intermediate container 8b7b0383d636 ---\u0026gt; c0d79183f707 Successfully built c0d79183f707 Successfully tagged ${username}/my-final-image:1.0 처음에 Build했을 때 보다는 월등히 빠르게 Build되는 것을 알 수 있습니다. ${username}/my-final-image:1.0의 Base Image가 이미 Local에 존재하기 때문에 CMD에 해당하는 Layer만 생성하면 되기 때문입니다.\n생성된 Image를 확인합니다.\n/image-build-test $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE ${username}/my-final-image 1.0 c0d79183f707 14 seconds ago 107MB ${username}/my-base-image 1.0 6087ccdf7873 4 minutes ago 107MB ubuntu 16.10 7d3f705d307c 8 months ago 107MB docker history [OPTIONS] IMAGE로 각 Image를 구성하는 Layer를 한번 확인해 보겠습니다. 두 Image 사이에서 공유하는 Layer는 암호화된 ID가 동일하다는 것을 확인할 수 있습니다.\n${username}/my-base-image:1.0\n /image-build-test $ docker history ${username}/my-base-image:1.0 IMAGE CREATED CREATED BY SIZE COMMENT 6087ccdf7873 4 minutes ago /bin/sh -c #(nop) COPY dir:d9ce0b6a08037dcd1… 6.26kB 7d3f705d307c 8 months ago /bin/sh -c #(nop) CMD [\u0026#34;/bin/bash\u0026#34;] 0B \u0026lt;missing\u0026gt; 8 months ago /bin/sh -c mkdir -p /run/systemd \u0026amp;\u0026amp; echo \u0026#39;do… 7B \u0026lt;missing\u0026gt; 8 months ago /bin/sh -c sed -i \u0026#39;s/^#\\s*\\(deb.*universe\\)$… 2.78kB \u0026lt;missing\u0026gt; 8 months ago /bin/sh -c rm -rf /var/lib/apt/lists/* 0B \u0026lt;missing\u0026gt; 8 months ago /bin/sh -c set -xe \u0026amp;\u0026amp; echo \u0026#39;#!/bin/sh\u0026#39; \u0026gt; /… 745B \u0026lt;missing\u0026gt; 8 months ago /bin/sh -c #(nop) ADD file:6cd9e0a52cd152000… 107MB ${username}/my-final-image:1.0\n /image-build-test $ docker history ${username}/my-final-image:1.0 IMAGE CREATED CREATED BY SIZE COMMENT c0d79183f707 50 seconds ago /bin/sh -c #(nop) CMD [\u0026#34;/bin/sh\u0026#34; \u0026#34;-c\u0026#34; \u0026#34;/app… 0B 6087ccdf7873 5 minutes ago /bin/sh -c #(nop) COPY dir:d9ce0b6a08037dcd1… 6.26kB 7d3f705d307c 8 months ago /bin/sh -c #(nop) CMD [\u0026#34;/bin/bash\u0026#34;] 0B \u0026lt;missing\u0026gt; 8 months ago /bin/sh -c mkdir -p /run/systemd \u0026amp;\u0026amp; echo \u0026#39;do… 7B \u0026lt;missing\u0026gt; 8 months ago /bin/sh -c sed -i \u0026#39;s/^#\\s*\\(deb.*universe\\)$… 2.78kB \u0026lt;missing\u0026gt; 8 months ago /bin/sh -c rm -rf /var/lib/apt/lists/* 0B \u0026lt;missing\u0026gt; 8 months ago /bin/sh -c set -xe \u0026amp;\u0026amp; echo \u0026#39;#!/bin/sh\u0026#39; \u0026gt; /… 745B \u0026lt;missing\u0026gt; 8 months ago /bin/sh -c #(nop) ADD file:6cd9e0a52cd152000… 107MB 다시 Push ${username}/my-final-image:1.0 Image를 Docker Registry에 Push합니다. Docker Registry에는 ${username}/my-base-image:1.0 Image의 Layer들이 이미 존재하고 있기 때문에, 추가된 Layer(Dockerfile에 추가된 CMD)만 Push됩니다.\n/image-build-test $ docker push ${username}/my-final-image:1.0 The push refers to repository [docker.io/${username}/my-final-image] 1b9e33c4403d: Mounted from ${username}/my-base-image fcc11235a441: Mounted from ${username}/my-base-image 0ba4c05a8843: Mounted from ${username}/my-base-image 47a3ebbaa644: Mounted from ${username}/my-base-image 31eed92f5a23: Mounted from ${username}/my-base-image 57145c01eb80: Mounted from ${username}/my-base-image 1.0: digest: sha256:cd26aa2cb82dbecdb064b524e3f3796493b106fd2dae4a9a2320856c46288dad size: 1564 다른 곳에서 Hello World를 실행해보자 새로운 환경에서 지금 만든 Image를 Container로 실행시켜 보겠습니다. 그 전에 docker-machine create로 새로운 Docker 환경을 구성하겠습니다.\n/image-build-test $ docker-machine create ${username} Running pre-create checks... (${username}) Default Boot2Docker ISO is out-of-date, downloading the latest release... (${username}) Latest release for github.com/boot2docker/boot2docker is v18.03.0-ce (${username}) Downloading /Users/${username}/.docker/machine/cache/boot2docker.iso from https://github.com/boot2docker/boot2docker/releases/download/v18.03.0-ce/boot2docker.iso... (${username}) 0%....10%....20%....30%....40%....50%....60%....70%....80%....90%....100% Creating machine... (${username}) Copying /Users/${username}/.docker/machine/cache/boot2docker.iso to /Users/${username}/.docker/machine/machines/${username}/boot2docker.iso... (${username}) Creating VirtualBox VM... (${username}) Creating SSH key... (${username}) Starting the VM... (${username}) Check network to re-create if needed... (${username}) Waiting for an IP... Waiting for machine to be running, this may take a few minutes... Detecting operating system of created instance... Waiting for SSH to be available... Detecting the provisioner... Provisioning with boot2docker... Copying certs to the local machine directory... Copying certs to the remote machine... Setting Docker configuration on the remote daemon... Checking connection to Docker... Docker is up and running! To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env ${username} 새로 구성한 Docker 환경에 docker-machine ssh로 접속하겠습니다.\n/image-build-test $ docker-machine ssh ${username} ## . ## ## ## == ## ## ## ## ## === /\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\\___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\_______/ _ _ ____ _ _ | |__ ___ ___ | |_|___ \\ __| | ___ ___| | _____ _ __ | \u0026#39;_ \\ / _ \\ / _ \\| __| __) / _` |/ _ \\ / __| |/ / _ \\ \u0026#39;__| | |_) | (_) | (_) | |_ / __/ (_| | (_) | (__| \u0026lt; __/ | |_.__/ \\___/ \\___/ \\__|_____\\__,_|\\___/ \\___|_|\\_\\___|_| Boot2Docker version 18.03.0-ce, build HEAD : 404ee40 - Thu Mar 22 17:12:23 UTC 2018 Docker version 18.03.0-ce, build 0520e24 docker@${username}:~$ 지금 만든 Image를 Pull받고,..\ndocker@${username}:~$ docker pull ${username}/my-final-image:1.0 1.0: Pulling from ${username}/my-final-image dca7be20e546: Pull complete 40bca54f5968: Pull complete 61464f23390e: Pull complete d99f0bcd5dc8: Pull complete 120db6f90955: Pull complete cb2cf9c669f2: Pull complete Digest: sha256:cd26aa2cb82dbecdb064b524e3f3796493b106fd2dae4a9a2320856c46288dad Status: Downloaded newer image for ${username}/my-final-image:1.0 새로 구성된 환경이라 Image의 모든 Layer들을 내려받습니다. 그리고, docker run을 실행하면 !!!\ndocker@${username}:~$ docker run ${username}/my-final-image:1.0 Hello world 처음에 정리했던 Build - Push - Pull - Run의 과정으로 Container가 실행되는 것을 확인할 수 있었습니다. Docker Engine이 설치된 어떤 환경에서도 지금 만든 Hello world는 실행될 것입니다. 여기서 더 나아가 Hello world는 다양한 메시지를 입력받거나 혹은, 저장되어 있는 메시지들을 출력하는 Application으로 발전할 수도 있습니다. 그렇다면, 이 Application의 기능이 Upgrade되어, 새로운 버전의 Image를 만들고 새로운 Container를 기동시켜야 한다면, 변경된 내용(입력받거나 혹은 저장된 메시지들)들은 어떻게 될까요?\n우리는 이전에 Image와 Container의 Layer 구조를 살펴보면서, 해당 Container에 읽기/쓰기가 가능한 Layer내에 변경된 내용들이 저장된다는 것을 알고 있습니다. 그리고, 이 데이터들은 해당 Container와 같이 소멸된다는 것도 알고 있습니다. 그럼, Container가 소멸되었다가 새로 생성되어도, 이전의 데이터들을 유지하여 사용할 방법은 없을까요? 이 다음에는 Docker의 Volume을 이용하여, 데이터를 보존하는 방법에 대해서 확인해볼 필요가 있겠네요.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/docker/what-is-container/",
        "title": "[Docker 기본(3/8)] Container는 뭘까?",
        "tags": ["docker", "container", "virtualization", "layer"],
        "description": "",
        "content": " Docker는 Application의 배포와 운영을 쉽게 도와주는 \u0026lsquo;Containers as a Service(CaaS) Platform이며, Client-Server Model로 동작하고 있습니다. 여기서 Container란, 운영체제의 커널이 하나의 인스턴스가 아닌, 여러 개의 격리된 인스턴스들을 갖출 수 있도록 하는 서버 가상화 방식입니다. 이러한 인스턴스들은 이를 소유하고 있는 Host Machine과 사용자의 관점에서 바라보면, 실제 서버인 것처럼 보입니다.\n가상화(Virtualization) Container는 가상화 방식의 한 종류입니다. 하지만, 흔히 우리가 알고있는 가상화(Virtualization)는 VMWare Workstation과 같은 소프트웨어를 이용해, Linux나 MacOS위에 Windows를 올려서 사용하는 그런 것을 떠올릴 것입니다. 이런 방식들이 하드웨어와 소프트웨어를 결합하여 가상 머신(VM)을 만들어 사용하는, Platform 가상화를 의미합니다. Platform 가상화는 주어진 하드웨어 Platform 위에서 제어 프로그램, 즉 Host 소프트웨어를 통해 실행됩니다. Host 소프트웨어는 Host 환경 내의 Guest 소프트웨어에 맞춰 가상 머신(VM)을 만들어 냅니다. Guest 소프트웨어는 완전한 운영체제(즉, 위의 예에서 Windows가 Guest 소프트웨어입니다)이며, 독립된 하드웨어 Platform에 설치된 것처럼 실행됩니다.\n가상 머신(VM)의 전가상화 여기서, 가상 머신(VM)은 하나의 서버를 여러 서버로 전환시키는 물리적 하드웨어의 추상화로 Hypervisor1 기반의 시스템 가상화입니다. Hypervisor는 기반이 되는 시스템 안에 또 다른 시스템을 구동 시킬 수 있게 시스템의 각 요소들을 가상화해서 제공합니다. 이 때, 기반 시스템을 보통 Host 시스템이라 하고 Hypervisor 위에서 돌아가는 시스템을 가상 시스템을 의미합니다. Hypervisor는 Host 시스템의 자원을 기반으로 가상 시스템이 독립적으로 움직일 수 있도록 합니다. 따라서, Hypervisor를 사용하여 여러 대의 VM을 단일 시스템에서 실행할 수 있지만, 호스트 시스템의 하드웨어 자원에 제한을 받습니다.\n출처: Docker Docs - Containers and virtual machines\n Container의 반가상화 반면, Container는 공유된 운영체제에서 격리되어 실행할 수 있는 형식으로 소프트웨어를 가상화하는 방법입니다. Hypervisor처럼 시스템의 전반적인 것을 가상화하는 것이 아닌, Application을 구동할 수 있는 환경 즉, CPU와 Memory 영역 등을 가상화하고 구동하는데 필요한 운영체제나 라이브러리는 호스트 시스템과 공용으로 사용합니다.\n출처: Docker Docs - Containers and virtual machines\n VM VS Container 두 가상화 방식간의 차이는 어떤 것이 있을까요?\nVM은 CPU, 메모리, 디스크는 물론이고 그 안에서 구동 중인 운영체제와 각종 라이브러리까지 전체 영역을 독립적으로 구분되어 운영됩니다. 따라서, 각 VM에는 운영체제의 전체 번들, 하나 이상의 응용 프로그램, 필요한 바이너리 및 라이브러리 전체가 포함되어 있기 때문에 수십 GB를 차지합니다. 반면, Container는 전체 운영 체제를 번들로 제공하지 않습니다. 단지, 소프트웨어가 실행할 때 필요한 라이브러리 및 설정들만 포함되기 때문에 VM보다 적은 공간을 차지합니다. 일반적으로 Container를 생성하는 Image의 크기는 수십 MB정도 입니다. 따라서, Container는 VM보다 공간을 효율적으로 사용할 수 있고, 가볍습니다.\n동작 방식에서도 차이가 있습니다. Hypervisor 기반의 가상 시스템에서 Application을 구동하기 위해서는 해당 가상 머신이 완전히 실행된 후에 Application이 동작됩니다. 따라서, 실제 Application이 구동되기 까지의 시간이 길게 걸리지만, Container는 이미 시스템이 동작되고 있는 상태에서 Application 부분만 가상화되어 실행되기 때문에 속도가 훨씬 빠릅니다(거의 즉시 시작됩니다). 또한, 가상화된 시스템이 운영체제 등을 동작시키기 위해 사용하는 자원을 Container에서는 사용하고 있지 않기 때문에, 자원의 소비율 역시 VM에 비해서 적습니다.\nContainer의 한계 하지만, Container에도 한계점은 존재합니다. 기존 Hypervisor 기반의 가상화 기술보다 Container가 뛰어난 부분은 앞서 얘기한 것처럼 속도가 빠르고 자원 소비율이 적기 때문에, 같은 자원을 갖고 있는 Host 시스템에서 더 많은 Application을 가상화 기반 위에 동작시킬 수 있습니다. 하지만 Docker의 기반이 되는 Container기술은 Application의 구동 환경을 가상화하기 때문에, 호스트 시스템에서 제공하는 운영체제와 같은 환경에서만 제공한다는 단점이 있습니다. 즉, 일반 Docker 환경(Linux)에서 Windows Application 실행은 현재로선 어렵다는 것입니다. 이는 Container 기술 자체가 운영체제와 실행 바이너리, 라이브러리를 공유하면서 Application 자체의 실행 환경만 가상화하여 제공하는 기술이기 때문에 어쩔 수 없는 상황입니다.\n현재는, Enterprise Edition에서 Windows환경을 지원하고 있습니다. 자세한 내용은 Install Docker에서 \u0026ldquo;Supported platforms\u0026rdquo;항목을 확인하시기 바랍니다.\n Container 지금까지는, Container가 어떤 것인지에 대해서 살펴봤습니다. Container는 가상화 기술의 한 종류이며, 이는 속도가 빠르고 자원 소비율이 적다는 장점이 있는 것을 알 수 있었습니다. 그럼, 지금부터는 Container가 어떻게 생성되고, 동작하는지에 대해서 알아보겠습니다.\nImage와 Container의 Layer 먼저 Container에 대해서 살펴보기 전에, Container 실행의 주체가 되는 Image에 대해 알아보겠습니다.\nImage와 Layer Image는 Dockerfile이라는 Build 명세서를 바탕으로 생성됩니다. 아래는 Latest Version(16.04 Version) Ubuntu Image의 Dockerfile입니다(Github: docker-brew-ubuntu-core/xenial/Dockerfile).\nFROM scratch ADD ubuntu-xenial-core-cloudimg-amd64-root.tar.gz / # a few minor docker-specific tweaks # see https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap RUN set -xe \\ ... 중략 ... # delete all the apt list files since they\u0026#39;re big and get stale quickly RUN rm -rf /var/lib/apt/lists/* # this forces \u0026#34;apt-get update\u0026#34; in dependent images, which is also good # enable the universe RUN sed -i \u0026#39;s/^#\\s*\\(deb.*universe\\)$/\\1/g\u0026#39; /etc/apt/sources.list # make systemd-detect-virt return \u0026#34;docker\u0026#34; # See: https://github.com/systemd/systemd/blob/aa0c34279ee40bce2f9681b496922dedbadfca19/src/basic/virt.c#L434 RUN mkdir -p /run/systemd \u0026amp;\u0026amp; echo \u0026#39;docker\u0026#39; \u0026gt; /run/systemd/container # overwrite this with \u0026#39;CMD []\u0026#39; in a dependent Dockerfile CMD [\u0026#34;/bin/bash\u0026#34;] Ubuntu의 Dockerfile은 다음의 명령어들로 구성되어 있습니다.\n FROM ADD 4개의 RUN CMD  Dockerfile로 부터 Image가 구성될 때, Dockerfile의 각각의 명령어가 Layer로 구성됩니다. 만약 docker build [OPTIONS] PATH | URL | - 를 사용하여 위의 Dockerfile로부터 Ubuntu를 Image로 Build하려할 때, FROM을 제외한 6개의 명령어가 각각의 Layer로 구성됩니다.\nDockerfile의 FROM scratch는 비어있는 Image를 생성하는 명령어로, Layer를 구성하지 않습니다.\n Image의 Layer가 실제로 Dockerfile의 명령어로 이루어져 있는지 docker history [OPTIONS] IMAGE를 사용하여 확인해보겠습니다.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 747cb2d60bbe 2 weeks ago 122MB $ docker history 747cb2d60bbe IMAGE CREATED CREATED BY SIZE COMMENT 747cb2d60bbe 2 weeks ago /bin/sh -c #(nop) CMD [\u0026#34;/bin/bash\u0026#34;] 0B \u0026lt;missing\u0026gt; 2 weeks ago /bin/sh -c mkdir -p /run/systemd \u0026amp;\u0026amp; echo \u0026#39;... 7B \u0026lt;missing\u0026gt; 2 weeks ago /bin/sh -c sed -i \u0026#39;s/^#\\s*\\(deb.*universe\\... 2.76kB \u0026lt;missing\u0026gt; 2 weeks ago /bin/sh -c rm -rf /var/lib/apt/lists/* 0B \u0026lt;missing\u0026gt; 2 weeks ago /bin/sh -c set -xe \u0026amp;\u0026amp; echo \u0026#39;#!/bin/sh\u0026#39; \u0026gt;... 745B \u0026lt;missing\u0026gt; 2 weeks ago /bin/sh -c #(nop) ADD file:5b334adf9d9a225... 122MB 총 6개의 행을 확인할 수 있으면, CREATED BY열에서 해당 Layer가 어떤 명령어로 생성되었는지 확인할 수 있습니다. 이렇게 생성된 Layer를 Image Layer라 하며, Image Layer는 수정이 불가능하며, 오로지 읽기만 가능합니다.\nIMAGE열이 \u0026lt;missing\u0026gt;으로 표현된 Layer는 해당 Layer가 다른 시스템에 의해 작성되었으며, Local에서 사용할 수 없음을 의미합니다.\n Container와 Image 오로지 읽기만 가능한, Image Layer의 최상단에 읽기/쓰기가 가능한 얇은 Layer가 추가되어 실행할 수 있는데, 이것이 바로 Container입니다. 새로 추가된 Layer는 Container Layer라 합니다. Container Layer에는 Container 기동 중 발생하는 모든 변경 사항들(파일 생성, 수정, 삭제 등)이 작성되고 저장됩니다. 아래는 현재 Local Repository에 실제로 저장된 Ubuntu의 Image Layer입니다.\n/ # ls /var/lib/docker/overlay2/ 14ed0fd37eebdc612b798270d3685a6a6c1a6c42dde696aa4048533dfe1a86fe 21535fcab696330a4d6de5bed2a06e65f8547ca6d3cdaacf6dd636f41a4d27cf 8803ef05c1bd6845eade31d9aba92f538700971e630cf274b3ccd11dc6a8a304 fc25a819ae26279427b3203b1acd4309cb622de19479fe1ce5890635c0052740 fc83da02f8a1ec7f9cc40e512aafe550dd1030883ddc82c56e59d229dcda60f2 l  Docker의 Image와 Container가 저장된 위치는 /var/lib/docker/\u0026lt;storage_driver\u0026gt;/ 경로로 확인할 수 있습니다. 단, MacOS와과 Windows 개발환경은 설치된 Docker 가상 Machine에 직접 접속하여야 확인 가능합니다.\n 그럼 Ubuntu Image로부터 Container를 기동하겠습니다.\n$ docker run -dit --name ubuntu-local ubuntu /bin/bash 8c0ee2a29f48b2e9ac27ed26f87aa4f32705dd52e3bea228528e6d9fce210e76 $ docker ps -s CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES SIZE 8c0ee2a29f48 ubuntu \u0026#34;/bin/bash\u0026#34; Less than a second ago Up 4 seconds ubuntu-local 0B (virtual 122MB) 아래와 같이, Docker Machine에 직접 접속해보면, Container가 기동되면서 Container Layer가 추가된 것을 확인할 수 있습니다.\n/ # ls /var/lib/docker/overlay2/ 14ed0fd37eebdc612b798270d3685a6a6c1a6c42dde696aa4048533dfe1a86fe 21535fcab696330a4d6de5bed2a06e65f8547ca6d3cdaacf6dd636f41a4d27cf 8803ef05c1bd6845eade31d9aba92f538700971e630cf274b3ccd11dc6a8a304 + c7a75746f5f12bcbb5ae03c20a65f080b2e7c9f1ec897491309ac3843454cff8 + c7a75746f5f12bcbb5ae03c20a65f080b2e7c9f1ec897491309ac3843454cff8-init fc25a819ae26279427b3203b1acd4309cb622de19479fe1ce5890635c0052740 fc83da02f8a1ec7f9cc40e512aafe550dd1030883ddc82c56e59d229dcda60f2 l  기동 중인 Container를 삭제하겠습니다.\n$ docker stop ubuntu-local ubuntu-local $ docker rm ubuntu-local ubuntu-local $ docker ps -s CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES SIZE 이전에 생성되었던 Container Layer는 삭제되었고, Image Layer만 남아있는 것을 알 수 있습니다.\n/ # ls /var/lib/docker/overlay2/ 14ed0fd37eebdc612b798270d3685a6a6c1a6c42dde696aa4048533dfe1a86fe 21535fcab696330a4d6de5bed2a06e65f8547ca6d3cdaacf6dd636f41a4d27cf 8803ef05c1bd6845eade31d9aba92f538700971e630cf274b3ccd11dc6a8a304 fc25a819ae26279427b3203b1acd4309cb622de19479fe1ce5890635c0052740 fc83da02f8a1ec7f9cc40e512aafe550dd1030883ddc82c56e59d229dcda60f2 l  정리하면, 다음과 같습니다.\n Image는 Layer의 Set으로 이루어져 있으며, 각각의 Layer는 Dockerfile의 명령어에 해당합니다. Image Layer는 읽기전용 Layer로 수정할 수 없습니다. Container를 기동하면, 읽기/쓰기가 가능한 Container Layer가 Image Layer Set의 최상단에 추가됩니다. Container 기동 중 발생한 모든 행위는 Container Layer에 기록되며, Container가 삭제되면 해당 Container Layer도 삭제됩니다(단, Image Layer는 삭제되지 않습니다).  아래는 Image와 Container Layer를 도식화한 것입니다.\n출처: Docker Docs - Images and layers\n Layer의 공유 나아가, 하나의 Image로 부터 여러개의 Container가 기동될 때는 어떻게 되는지 살펴보겠습니다. 먼저 Ubuntu Image로 부터 ubuntu-local-NUM 이름의 Container들을 기동시킵니다.\n$ docker run -dit --name ubuntu-local-1 ubuntu /bin/bash \\ \u0026amp;\u0026amp; docker run -dit --name ubuntu-local-2 ubuntu /bin/bash \\ \u0026amp;\u0026amp; docker run -dit --name ubuntu-local-3 ubuntu /bin/bash cc2f62db98b4ed5aadc6904229db1c1f2f58d69f3ef4c120eb78402a12a96f7d f921b84e42d42bdad629f58398ac18d942c25c3451238a361f841b0b8d630523 87fccd76cd1d195bc25dd0e30203c3d3f056bb69a2851a1feef8026386ccb440 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 87fccd76cd1d ubuntu \u0026#34;/bin/bash\u0026#34; About an hour ago Up About an hour ubuntu-local-3 f921b84e42d4 ubuntu \u0026#34;/bin/bash\u0026#34; About an hour ago Up About an hour ubuntu-local-2 cc2f62db98b4 ubuntu \u0026#34;/bin/bash\u0026#34; About an hour ago Up About an hour ubuntu-local-1 Local Docker에 실제로 저장된 Container와 Layer입니다. 각 Container 별 Container Layer가 추가되었으나, Image Layer는 추가되지 않았습니다.\n/ # ls /var/lib/docker/containers 87fccd76cd1d195bc25dd0e30203c3d3f056bb69a2851a1feef8026386ccb440 cc2f62db98b4ed5aadc6904229db1c1f2f58d69f3ef4c120eb78402a12a96f7d f921b84e42d42bdad629f58398ac18d942c25c3451238a361f841b0b8d630523 / # ls /var/lib/docker/overlay2/ 14ed0fd37eebdc612b798270d3685a6a6c1a6c42dde696aa4048533dfe1a86fe + 15123a9c6da09e3ac10a96846059a7af7bdfc040d7895eae40f22df12c8f3e5a + 15123a9c6da09e3ac10a96846059a7af7bdfc040d7895eae40f22df12c8f3e5a-init 21535fcab696330a4d6de5bed2a06e65f8547ca6d3cdaacf6dd636f41a4d27cf + 2de0c7fc0c717cb2b834b33857d369b92599bfd53078a16bfa7690af2200b727 + 2de0c7fc0c717cb2b834b33857d369b92599bfd53078a16bfa7690af2200b727-init 8803ef05c1bd6845eade31d9aba92f538700971e630cf274b3ccd11dc6a8a304 + ed0d6800996f0b1e0d7f0ae845128a89c9029aa6019dd3ea2efed965e4bee760 + ed0d6800996f0b1e0d7f0ae845128a89c9029aa6019dd3ea2efed965e4bee760-init fc25a819ae26279427b3203b1acd4309cb622de19479fe1ce5890635c0052740 fc83da02f8a1ec7f9cc40e512aafe550dd1030883ddc82c56e59d229dcda60f2  기동 중인 Container의 대락젹인 크기는 docker ps -s으로 확인할 수 있습니다.\n$ docker ps -s CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES SIZE 87fccd76cd1d ubuntu \u0026#34;/bin/bash\u0026#34; About an hour ago Up About an hour ubuntu-local-3 0B (virtual 122MB) f921b84e42d4 ubuntu \u0026#34;/bin/bash\u0026#34; About an hour ago Up About an hour ubuntu-local-2 0B (virtual 122MB) cc2f62db98b4 ubuntu \u0026#34;/bin/bash\u0026#34; About an hour ago Up About an hour ubuntu-local-1 0B (virtual 122MB)  size: 각 Container의 Container Layer에서 사용하고 있는 Data 양 virtual size: Image Layer에서 사용하고 있는 Data 양.  Local Docker의 File System에 저장된 실제 Container 크기입니다.\n/ # du -sh /var/lib/docker/containers/* 32.0K /var/lib/docker/containers/87fccd76cd1d195bc25dd0e30203c3d3f056bb69a2851a1feef8026386ccb440 32.0K /var/lib/docker/containers/cc2f62db98b4ed5aadc6904229db1c1f2f58d69f3ef4c120eb78402a12a96f7d 32.0K /var/lib/docker/containers/f921b84e42d42bdad629f58398ac18d942c25c3451238a361f841b0b8d630523 지금까지의 과정에서 다음의 2가지 사실을 확인할 수 있습니다.\n 같은 Image를 바탕으로 Container가 기동될 때, Container Layer만 추가된다. 대략적으로 보이는 Container의 크기와 실제 저장된 크기는 차이가 있다. (docker ps -s로 나타난 각 Container의 크기는 122MB로 보이지만, 실제 File System에 저장된 Container는 32.0K입니다.)  이를 종합적으로 봤을 때, 동일한 Image를 바탕으로 기동된 Container는 각각의 Container Layer만 추가되며, Image Layer는 공유하여 사용한다는 사실을 알 수 있습니다(실제로 Docker는 동일한 Image에서 기동된 Container들은 Image Layer를 100%공유하여 사용합니다). 또한, Container는 개별적으로 Container Layer를 갖고 해당 Layer에 모든 변경된 내용이 저장되기 때문에, 여러 Container가 동일한 기본 Image를 공유하여 사용하면서 Container 자신의 Data 상태를 가질 수 있습니다. 이러한 특성으로 인해, Docker Container는 더욱 가벼워지고 기동 속도가 빨라질 수 있는 것입니다.\nContainer Layer의 파일 쓰기 및 변경 대한 자세한 내용은 Docker Documents에서 About images, containers, and storage drivers의 copy-on-write Strategy를 참고하시기 바랍니다.\n 이를 다이어그램으로 표현하면 다음과 같습니다.\n출처: Docker Docs - Images and layers\n Ubuntu Image를 통해, Image와 Container의 Layer 구조를 확인함으로써 Docker가 왜 가볍고 빠른지 알 수 있었습니다. 그렇다면, 이제 남은 것은 직접 만들어보는 것 밖에 없네요.\n 하이퍼바이저(hypervisor)는 호스트 컴퓨터에서 다수의 운영체제를 동시에 실행하기 위한 논리적 Platform을 말합니다. 가상화 머신 모니터(Virtual Machine Monitor, 줄여서 VMM)라고도 부릅니다. [return]   "
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/docker/docker-architecture/",
        "title": "[Docker 기본(2/8)] Docker&#39;s Skeleton",
        "tags": ["docker", "docker daemon", "docker registry", "docker object"],
        "description": "",
        "content": " Docker는 Container를 구동시킬 수 있는 환경만 구성되어 있다면, Application들을 한 번의 Build로 어디서든 구동시킬 수 있습니다. 하지만 1차원적으로만 살펴보면, Java Application이 JVM 위에서 실행되는 모습과 크게 다르지 않아 보입니다. Java Application도 JVM만 설치되어 있다면, 어디에서든 실행되죠. 마찬가지로, Python Application도 동일합니다. 그렇다면, 이 모든 것들이 결국은 똑같이 생겼고, 단지 실행되는 주체(Container, JAR, py)만 다른 것일까요?\nClient-Server Model Docker는 서비스의 요청자(Docker Client)와 제공자(Docker Server)간의 작업이 분리되어 동작하는 Client-Server Model로 되어있으며, Docker Client는 REST API를 사용하여 Docker Server를 제어합니다.\n Docker Client: docker CLI Docker Server: docker daemon Docker REST API  그리고, Docker Server(docker daemon)는 Docker Client로부터 받은 요청에 따라, 다음의 Docker Object들을 생성하고 관리합니다.\n Image Container Network Data Volumes  출처: Docker Docs - Docker Engine\n 좀 더 자세히 보면 Docker Client는 Docker Daemon과 UNIX Socket 또는 REST API를 사용하여 통신을 하며, Docker Daemon이 Container를 구축, 실행 및 배포할 수 있도록 합니다. Docker Client와 Daemon은 동일한 시스템에서 실행될 수도 있고, Docker Client를 원격으로 Docker Daemon에 연결하여 사용할 수도 있습니다.\n출처: Docker Docs - Docker architecture\n Docker Daemon Docker Daemon(dockerd)은 Client로부터 API 요청을 수신하고 Image, Container, Network 및 Volume과 같은 Docker Object를 관리합니다. Daemon은 Docker 서비스를 관리하기 위해 다른 Daemon과 통신할 수 있습니다.\nDocker Client Docker Client(docker)는 사용자가 Docker Daemon과 통신하는 주요 방법입니다. docker run과 같은 명령을 사용하면 Docker Client는 해당 명령을 Docker Daemon으로 전송하여 명령을 수행하게 합니다. docker 명령은 Docker API를 사용하며, Docker Client는 둘 이상의 Docker Daemon과 통신 할 수 있습니다.\nDocker Registry Docker Registry는 Docker Image를 저장합니다. Docker Hub는 누구나 사용할 수있는 Public Registry이며, Docker는 기본적으로 Docker Hub에서 Image를 찾아 Container를 구성하도록 되어 있습니다. 예를들어, docker pull을 사용하여 Image를 Registry에서 Local로 내려받을 수 있으며, docker push를 통해 Local의 Image를 Registry에 저장할 수도 있습니다. Docker Registry는 개개인이 구성할 수도 있으며, Docker의 Enterprise Edition에서 제공되는 Docker Trusted Registry이 포함된 Docker Datacenter를 사용할 수도 있습니다.\nDocker Object Docker Object는 Docker Daemon에 의해, 생성 및 관리되는 Image / Container / Network / Volume 등의 개체를 말합니다.\nImage Image는 Docker Container를 생성하기 위한 읽기 전용 Template입니다. Image들은 다른 Image 기반 위에 Customizing이 추가되어 만들어질 수 있으며, 이렇게 만들어진 Image는 Docker Registry에 Push한 뒤 사용할 수 있습니다. Image는 Dockerfile에 Image를 만들고 실행하는 데 필요한 단계를 명령어로 정의하여 생성합니다. Dockerfile에 정의된 각각의 명령어들은 Image의 Layer를 생성하며, 이러한 Layer들이 모여 Image를 구성합니다. Dockerfile을 변경하고 Image를 다시 구성하면 변경된 부분만 새로운 Layer로 생성됩니다. 이러한 Image의 Layer구조는 Docker가 타 가상화 방식과 비교할 때, 매우 가볍고 빠르게 기동할 수 있는 요인이 됩니다.\nContainer Container는 Docker API 사용하여 생성, 시작, 중지, 이동 또는 삭제 할 수 있는 Image의 실행가능한 Instance를 나타냅니다. Container를 하나 이상의 Network에 연결하거나, 저장 장치로 묶을 수 있으며, 현재 상태를 바탕으로 새로운 Image를 생성할 수도 있습니다. 기본적으로 Container는 Host 또는 다른 Container로부터 격리되어 있으며, Network / Storage와 다른 하위 시스템에 대한 접근을 직접 제어 할 수 있습니다. Container는 생성되거나 시작될 때, 구성 옵션 및 Image로부터 정의됩니다. Container가 제거될 때는 영구 저장소에 저장되지 않은 변경 사항은 모두 해당 Container와 같이 사라집니다.\nService Service를 사용하면, 여러 개의 Docker Daemon들로 이루어진 영역 내에서 Container들을 확장(Scaling)시킬 수 있습니다. Service는 \u0026lsquo;특정 시간동안 사용 가능한 Service의 Replica 개수\u0026rsquo;와 같은 상태 정보들을 직접 정의하여 사용할 수 있습니다. 기본적으로 Service는 Docker Daemon들 간의 Load Balancing을 제공하고 있기 때문에, 사용자 관점에서는 단일 Application으로 보입니다.\n여러 개의 Docker Daemon들이 모여 Cluster 형태를 이루고 있는 것을 Docker Swarm이라고 합니다. Docker Swarm은 Manager와 Worker Node로 구성되어 있으며, Docker Engine은 Docker Version 1.12 이상에서 Swarm Mode를 지원합니다.\n Container를 기동시켜 보자 지금까지, Docker가 어떻게 생겼는지 그 구성요소들을 하나씩 확인해 봤습니다. 그렇다면, 간단하게 Public Image로붙터 Container를 기동시키면서, 각각의 요소들을 다시 한번 확인해 보겠습니다.\n 먼저, docker search [OPTIONS] TERM로 Docker Store와 Hub로부터 Image를 찾습니다.\n$ docker search ubuntu NAME DESCRIPTION STARS OFFICIAL AUTOMATED ubuntu Ubuntu is a Debian-based Linux operating s... 6710 [OK] dorowu/ubuntu-desktop-lxde-vnc Ubuntu with openssh-server and NoVNC 139 [OK] rastasheep/ubuntu-sshd Dockerized SSH service, built on top of of... 112 [OK] ansible/ubuntu14.04-ansible Ubuntu 14.04 LTS with ansible 87 [OK] ubuntu-upstart Upstart is an event-based replacement for ... 80 [OK] neurodebian NeuroDebian provides neuroscience research... 40 [OK] ubuntu-debootstrap debootstrap --variant=minbase --components... 31 [OK] nuagebec/ubuntu Simple always updated Ubuntu docker images... 22 [OK] tutum/ubuntu Simple Ubuntu docker images with SSH access 19 1and1internet/ubuntu-16-nginx-php-phpmyadmin-mysql-5 ubuntu-16-nginx-php-phpmyadmin-mysql-5 16 [OK] ppc64le/ubuntu Ubuntu is a Debian-based Linux operating s... 11 aarch64/ubuntu Ubuntu is a Debian-based Linux operating s... 9 i386/ubuntu Ubuntu is a Debian-based Linux operating s... 8 darksheer/ubuntu Base Ubuntu Image -- Updated hourly 3 [OK] codenvy/ubuntu_jdk8 Ubuntu, JDK8, Maven 3, git, curl, nmap, mc... 3 [OK] 1and1internet/ubuntu-16-nginx-php-5.6-wordpress-4 ubuntu-16-nginx-php-5.6-wordpress-4 2 [OK] 1and1internet/ubuntu-16-apache-php-7.0 ubuntu-16-apache-php-7.0 1 [OK] pivotaldata/ubuntu-gpdb-dev Ubuntu images for GPDB development 0 pivotaldata/ubuntu A quick freshening-up of the base Ubuntu d... 0 1and1internet/ubuntu-16-healthcheck ubuntu-16-healthcheck 0 [OK] thatsamguy/ubuntu-build-image Docker webapp build images based on Ubuntu 0 1and1internet/ubuntu-16-sshd ubuntu-16-sshd 0 [OK] ossobv/ubuntu Custom ubuntu image from scratch (based on... 0 defensative/socat-ubuntu 0 [OK] smartentry/ubuntu ubuntu with smartentry 0 [OK] Docker Hub에는 Community Version의 Image들이 포함되어 있습니다. 누구나 새로운 Image를 Docker Hub에 Push할 수 있지만, 해당 Image들의 품질이나 호환성을 Docker가 보장하지 않습니다. 대신, Docker Store에는 공인된 업체를 통해 승인된 Image들이 포함되어 있습니다. 이러한 Image들은 Vendor들에 의해, 직접 게시되고 유지/관리됩니다. 또한 Docker Certified 로고는 Image에 대한 품질, 출처 및 지원에 대한 보증을 제공합니다. 공식 Image는 OFFCIAL로, 그 외의 Community Image는 AUTOMATED에 분류됩니다.\n Container로 구성할 Image를 Docker Store 및 Hub로부터 docker pull [OPTIONS] NAME[:TAG|@DIGEST]를 이용하여 Pull 받습니다.  $ docker pull ubuntu Using default tag: latest latest: Pulling from library/ubuntu ae79f2514705: Pull complete 5ad56d5fc149: Pull complete 170e558760e8: Pull complete 395460e233f5: Pull complete 6f01dc62e444: Pull complete Digest: sha256:506e2d5852de1d7c90d538c5332bd3cc33b9cbd26f6ca653875899c505c82687 Status: Downloaded newer image for ubuntu:latest Image를 Pull 받을 때, Image에 대한 Version을 Tag로 지정하여 받을 수 있으며, 지정되지 않을 경우 latest Version으로 Pull이 진행됩니다. 각 Image별 상세 정보는 Docker Store에서 Image를 검색하여 나오는 상세 페이지에서 확인할 수 있습니다.\n docker images로 Local Repository에 Pull된 Image 목록을 확인합니다.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 747cb2d60bbe 13 days ago 122MB hello-world latest 05a3bd381fc2 5 weeks ago 1.84kB  Local Repository에 Pull된 Image를 docker run [OPTIONS] IMAGE[COMMAND] [ARG...]로 Container를 생성하여 실행시킵니다. 개별 docker command의 옵션 및 상세 사용법은 docker run --help와 같이 사용하여 확인하실 수 있습니다.  $ docker run -i -t --name ubuntu-local ubuntu /bin/bash root@ebb7937d0325:/# docker ps로 실행되고 있는 Ubuntu Container를 확인할 수 있습니다.\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ebb7937d0325 ubuntu \u0026#34;/bin/bash\u0026#34; 3 minutes ago Up 3 minutes ubuntu-local Pull \u0026amp; Run. 끝? Docker에서 인증받은 공식 Ubuntu Image로 Container를 기동시켜 봤습니다. 진행했던 과정을 요약해보자면, Docker Client는 Docker Daemon에게 Ubuntu Image를 검색시킨 후, Docker Hub로부터 Image를 Local Registry에 내려 받았습니다. 그리고 docker run을 Docker Daemon에게 다시 요청하여, 내려받은 Image로부터 Container를 생성 및 기동 시켰습니다.\n그렇다면, Pull과 Run이 끝일까요? 이전에 우리는 Docker Hub에 배포된 Image로 \u0026ldquo;어디서든 Run\u0026rdquo;할 수 있다는 것을 확인했습니다. 이제는 다음 단계로 넘어가, 직접 Image를 Build하고, 해당 Image로부터 Container를 Run해보도록 하겠습니다.\n하지만, 그 전에! Build된 Image와 Container가 어떻게 생겼는지 궁금해지네요.\n  "
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/docker/hello-docker/",
        "title": "[Docker 기본(1/8)] Hello Docker!",
        "tags": ["docker", "caas", "container"],
        "description": "",
        "content": " Docker란 리눅스의 응용프로그램들을 소프트웨어 Container 안에 배치시키는 일을 자동화하는 오픈 소스 프로젝트로서, Docker 공식 문서에 따르면 Containers as a Service(CaaS) Platform으로 정의하고 있습니다.\nDocker는 홈페이지에 Docker의 기능을 아래와 같이 명시 하고 있습니다.\n Docker Container는 일종의 소프트웨어 실행에 필요한 모든 것들을 포함하는 완전한 파일 시스템 안에 감싼다. 여기에는 코드, 런타임, 시스템 도구, 시스템 라이브러리 등 서버에 설치되는 무엇이든 전부 아우른다. 이는 실행 중인 환경에 관계 없이 언제나 동일하게 실행될 것을 보증한다.\n Docker는 리눅스에서 운영체제 수준 가상화에 대한 추상화 및 자동화 계층을 추가적으로 제공합니다. 또한, 리눅스 커널, 통합 파일 시스템의 리소스 격리 기능을 사용하며, 이를 통해 독립적인 Container가 하나의 리눅스 인스턴스 안에서 실행할 수 있게 함으로써 가상 머신에 대한 유지보수 부담을 덜어줍니다.\nMonolithic에서 Microservice로 지금까지는, 절대로 변하지 않을 듯한 Infrastructure를 기반으로 OS/Runtime/Middleware는 잘 정돈되어져 있었고, 그 위에 수 많은 기능들을 제공하는 대형 Application이 실행되고 있었습니다. 해당 Application에서 장애란 있을 수 없는 일로 간주되어 왔습니다. 하지만, Application의 이런 구조는 Mobile 기기를 필두로 한 전반적인 IT 산업의 변화에 있어서 독이 되었고, 이는 곧 Application의 유연성을 크게 해치게 되었습니다.\n이런 흐름에 맞춰 Application들의 전체적인 구조는 모두 바뀌게 되었습니다. 대형 Application들은 다양한 기기와 환경들에 맞춰 빠르게 지원하기 위해 잘게 쪼개졌으며, 개발자들은 최상의 성능을 위해서 사용 가능한 Service들을 개별적으로 조립하여 제공하였습니다. 또한, 조립된 Service들에 맞춰 Infrastructure도 Public / Private / Virtualized된 다양한 환경을 구성하여 사용하게 되었습니다.\n그렇다면, 왜 Docker인가 이러한 변화 속에서, 다음과 같은 새로운 과제들이 나타나기 시작했습니다.\n 다양한 방식으로 조립된 서비스가 일관되게 상호 작용할 수 있도록 보장하는 방법은 무엇인가? 각 Service별 \u0026ldquo;Dependency Hell\u0026rdquo;을 피할 수 있는 방법은 무엇인가? 수많은 Application을 다양한 환경으로 신속하게 Migration 및 확장할 수 있는가? 각 환경에 배포된 Application들의 호환성이 보장 되는가? n개의 Service가 n개의 Infrastructure에서 잘 구동될 수 있도록 하는 수 많은 설정들을 어떻게 피할 수 있는가?  과제들에 대한 해결책으로 나타난 것이 Container입니다. Container는 Library, 시스템 도구, Code, Runtime 등 Application에 필요한 모든 것이 포함되어 있는 표준화된 유닛으로, 환경에 구애받지 않고 Application을 신속하게 배포 및 확장 할 수 있는 환경을 제공해 줍니다.\n따라서, Docker는 이런 Container들을 관리할 수 있는 Service를 제공함으로써, Application들이 다양한 환경 속에서 일관된 서비스를 제공할 수 있도록 보장합니다.\n Docker를 사용한다면 동일한 Application을 다양한 환경으로 빠르게 배포할 수 있습니다 Docker는 Application 및 Service를 Container를 사용하여 표준화된 환경에서 구동할 수 있도록 함으로써, 개발 주기를 단축시킵니다. 이러한 Container의 이점은 CI/CD Workflow에서 극대화됩니다.\n환경에 따라, 배포와 확장이 자유롭습니다 Docker는 개발자의 Laptop, Datacenter의 VM, Cloud 환경 또는, 여러 다양한 환경에 쉽게 이식하여 사용할 수 있습니다. Docker의 이런 특성으로 인해, 비즈니스 요구 사항에 맞춰 Application과 Service를 부하에 따라 동적으로 관리할 수 있으며, 거의 실시간으로 축소 또는 확장할 수 있습니다. 타 Platform에 비해 같은 Hardware에서 더 많은 작업을 수행할 수 있습니다. Docker는 가볍고 빠르게 동작하기 때문에, Hypervisor 기반의 Virtual Machine 보다 실용적이고 비용 효율적입니다. 따라서, 적은 Resources로 많은 작업을 수행해야하는 중소규모의 배포 환경 및 고밀도 밀집 환경에 이상적입니다.\nHello Docker Docker는 로컬 개발 환경 구성을 위한 Community Edition(CE) 과 실 운영 환경을 위한 Enterprise Edition(EE)의 두 가지 버전으로 제공되고 있습니다. Docker Community Edition(CE)은 각 운영체제 환경에 맞는 설치파일이 제공되고 있습니다.\n Docker for Mac Docker for Windows  Docker의 Version 및 지원 Platform에 대한 자세한 정보는 Install Docker에서 확인하실 수 있습니다.\n Docker 환경 구성 방법은 다음과 같습니다.\n 로컬 환경의 운영체제에 맞는 Docker 설치파일을 다운받아 설치한 뒤, 실행합니다. docker version으로 설치된 Docker의 Version을 확인합니다.\n$ docker version Client: Version: 18.03.0-ce-rc4 API version: 1.37 Go version: go1.9.4 Git commit: fbedb97 Built: Thu Mar 15 07:33:28 2018 OS/Arch: darwin/amd64 Experimental: false Orchestrator: swarm Server: Engine: Version: 18.03.0-ce-rc4 API version: 1.37 (minimum version 1.12) Go version: go1.9.4 Git commit: fbedb97 Built: Thu Mar 15 07:42:29 2018 OS/Arch: linux/amd64 Experimental: true $ docker-compose version docker-compose version 1.20.0-rc2, build 8c4af54 docker-py version: 3.1.1 CPython version: 3.6.4 OpenSSL version: OpenSSL 1.0.2n 7 Dec 2017 $ docker-machine version docker-machine version 0.14.0, build 89b8332  구성된 Docker 환경에 Server(Docker Daemon)과 Client외에도 docker-compose와 docker-machine Tool도 같이 설치된 것을 확인하실 수 있습니다. docker run 명령어로 hello-world Image를 Pull 받아서 Conatiner 생성하여 실행시킵니다. ```bash $ docker run hello-world Unable to find image \u0026lsquo;hello-world:latest\u0026rsquo; locally latest: Pulling from library/hello-world ca4f61b1923c: Pull complete Digest: sha256:97ce6fa4b6cdc0790cda65fe7290b74cfebd9fa0c9b8c38e979330d547d22ce1 Status: Downloaded newer image for hello-world:latest\nHello from Docker! This message shows that your installation appears to be working correctly.\nTo generate this message, Docker took the following steps:\n The Docker client contacted the Docker daemon. The Docker daemon pulled the \u0026ldquo;hello-world\u0026rdquo; image from the Docker Hub. (amd64) The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.  To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash\nShare images, automate workflows, and more with a free Docker ID: https://cloud.docker.com/\nFor more examples and ideas, visit: https://docs.docker.com/engine/userguide/ ```\n  위와 같이 정상적으로 메시지가 나타난다면, Docker 환경 구성이 완료된 것입니다.\n  한 번의 Build로 어디서든 Run 지금까지 Docker환경을 구성하여, hello-world라는 Image로부터 Container를 구동시켰습니다. Docker가 설치되어 있다면, hello-world는 어디에서든지 100% 동일한 형태로 실행할 것입니다. 이는 Application들이 더 이상 환경에 제약받지 않고, 일관된 서비스를 제공할 수 있음을 의미합니다.\n한 번의 Build로 어디서든 Run할 수 있는 Docker. 이런 편리함을 제공하는 Docker가 어떻게 구성되어 있는지, 어떤 식으로 동작되는지가 더욱 궁금해집니다.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/redis-overview/",
        "title": "Redis Overview(with Spring Boot)",
        "tags": ["spring boot", "redis"],
        "description": "",
        "content": " Redis는 Key/Value형태의 In-Memory 데이터베이스로서 데이터 저장 및 캐싱, 그리고 메시지 브로커로 널리 사용 되고 있습니다.\nRedis의 가장 큰 특징으로는 In-Memory 데이터베이스가 가지는 장점인 처리속도가 굉장히 빠르다는 점이 있습니다. 물론 이것 자체는 휘발성 데이터이기 때문에 케이스에 따라서 데이터를 덤프하는 스케쥴링을 통해 정합성 및 영속성을 관리할 수 있습니다. 하지만 이것도 불안하다 싶어 조금 더 나아간다면 고가용성을 보장하기 위해 Master-Slave 구조의 아키텍쳐링까지 설계할 수 있습니다.\n또한, Redis의 내부를 열어보면 데이터 스트럭쳐로 String, Hash, List, Sets, Sorted Sets등 기본적인 쿼리 뿐만 아니라 Bitmaps, Hyperloglogs, Geospatial indexes와 같은 Radius 쿼리까지도 지원하는 등 강력한 기능이 있습니다.\nPivotal에서도 강력한 Redis의 기능을 사용하기 위해 Spring Boot를 통해 손쉽게 여러 프로젝트와 연결할 수 있는 방법을 가이드 하고 있습니다.\n지금부터는 이런 Redis를 Spring Boot에서 사용하는 방법을 알아보겠습니다.\n Spring Boot, Maven를 사용해서 작성했습니다.\n Redis Dependency 추가 Redis를 Spring-boot에서 사용하기 위해서는 가장 먼저 Redis의 Dependency를 추가해야합니다.\n pom.xml에 다음과 같은 설정을 추가함\n \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  Redis 접속 정보를 property에 등록합니다.\n spring: redis: host: 127.0.0.1 port: 6379 password: password Redis Session Redis는 Spring session과 연동하여 Session에 대한 저장소를 Redis로 관리할 수 있습니다.\nDependency\n pom.xml에 Spring Session을 사용하기 위한 정보를 넣습니다.\n \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.session\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-session\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Annotation\n 해당 Spring Session을 Redis로 사용하겠다는 annotation을 넣습니다.\n import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.session.data.redis.config.annotation.web.http.EnableRedisHttpSession; @SpringBootApplication @EnableRedisHttpSession // Redis를 세션형태로 사용하겠다는 설정 public class RedisApplication { public static void main(String[] args) { SpringApplication.run(RedisApplication.class, args); } } Property\n 프로퍼티에서 session의 Store-type을 redis로 설정해줍니다.\n spring: session: store-type: redis 결과\n $redis-cli\u0026gt; keys spring:session*\n \u0026#34;spring:session:sessions:expires:e7e5a693-54b9-489f-b8c2-25c993de7ec9\u0026#34; \u0026#34;spring:session:expirations:1520229840000\u0026#34; \u0026#34;spring:session:sessions:e7e5a693-54b9-489f-b8c2-25c993de7ec9\u0026#34; Redis Cache Redis Cache는 Spring Cache를 Redis에서 처리 할 수 있도록 지원합니다.\nDependency\n pom.xml에 캐시를 사용하기 위한 Dependency를 추가합니다.\n \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-cache\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Annotation\n 해당 Spring Cache을 Redis로 사용하겠다는 annotation을 넣습니다.\n import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cache.annotation.EnableCaching; @SpringBootApplication @EnableCaching public class RedisApplication { public static void main(String[] args) { SpringApplication.run(RedisApplication.class, args); } } Property\nspring: cache: type: redis 사용하기\n// 사용할 Cache에 대한 configuration @CacheConfig(cacheNames=\u0026#34;cacheName\u0026#34;) // 캐싱을 할 대상 Method에 붙여주는 Annotation @Cacheable // 캐시된 대상을 비워줄 Method 위에 붙여주는 Annotation @CacheEvict(cacheNames=\u0026#34;cacheName\u0026#34;) Redis Transaction 레디스에서 데이터를 저장할때 트랜잭션을 통해 처리할 수 있도록 설정합니다.\n주요 명령어는 MULTI , EXEC , DISCARD, WATCH\n MULTI : 트랜잭션 블록의 시작을 표시. 이 구문 안에 포함된 구문들은 전부 Queue로 넣어진 후 EXEC를 만날 때까지 대기. EXEC : 이전에 대기중인 모든 명령을 트랜잭션에서 실행 하고 연결 상태를 정상으로 복원. DISCARD : 이전에 대기중인 모든 명령을 트랜잭션에서 플러시 해버리고 연결 상태를 정상으로 복원. WATCH : 트랜잭션의 조건부 실행을 감시하는 지정된 키를 마크. 이 watch가 걸려있는 key를 외부에서 접근하려고 시도하면 Discard를 발생시킴.   1) Redis Transaction\n 명시적으로 사용하기 위해서 커스터마이징을 수행\n // execute a transaction List\u0026lt;Object\u0026gt; txResults = redisTemplate.execute(new SessionCallback\u0026lt;List\u0026lt;Object\u0026gt;\u0026gt;() { public List\u0026lt;Object\u0026gt; execute(RedisOperations operations) throws DataAccessException { operations.multi(); operations.opsForSet().add(\u0026#34;key\u0026#34;, \u0026#34;value1\u0026#34;); // This will contain the results of all ops in the transaction  return operations.exec(); } }); System.out.println(\u0026#34;Number of items added to set: \u0026#34; + txResults.get(0)); 2) @Transactional Support\n Framework의 Transaction을 사용해서 처리\n /* Sample Configuration */ @Configuration public class RedisTxContextConfiguration { @Bean public StringRedisTemplate redisTemplate() { StringRedisTemplate template = new StringRedisTemplate(redisConnectionFactory()); // explicitly enable transaction support  template.setEnableTransactionSupport(true); return template; } @Bean public PlatformTransactionManager transactionManager() throws SQLException { return new DataSourceTransactionManager(dataSource()); } @Bean public RedisConnectionFactory redisConnectionFactory( // jedis, lettuce, srp,... );  @Bean public DataSource dataSource() throws SQLException { // ... } }  https://redis.io/topics/transactions https://docs.spring.io/spring-data/redis/docs/1.8.6.RELEASE/reference/html/#redis:pubsub:subscribe   Redis Persistence 레디스 데이터의 영속성을 보장하기 위해 Data를 백업하고 관리하는 방법입니다. Redis홈페이지에서 권장하는 방법은 RDB와 AOF를 모두 사용하여 데이터 정합성을 보장하는 것입니다.\n1) RDB 방식\n dump.rdb 이름으로 저장 장점\n 원하는 시점에서 복구데이터를 남길 수 있습니다. 재해 복구에 매우 유용. RDB는 AOF에 비해 큰 데이터 세트를 사용하여 재시작을 빠르게 수행.   단점\n Redis가 작동을 멈춘 경우(예 : 정전 후)에 데이터 손실 가능성을 최소화해야하는 경우 RDB가 좋지 않습니다. (예를 들어 데이터 세트에 대해 최소 5 분 및 100 회 기록 후, 여러 저장 지점을 가질 수 있음). RDB는 자식 프로세스를 사용하여 디스크에 저장하기 위해 fork ()를 자주 수행 해야 합니다. 데이터 세트가 크면 Fork ()에 시간이 많이 걸릴 수 있으며 CPU 성능이 좋지 않은 경우 Redis는 클라이언트에 수 밀리 초 또는 1 초간 서비스를 제공하지 않을 수 있습니다.  Synchronously 백업\n$ src/redis-cli 127.0.0.1:6379\u0026gt; help save SAVE - summary: Synchronously save the dataset to disk since: 1.0.0 group: server 127.0.0.1:6379\u0026gt; save  Background 백업  $ src/redis-cli 127.0.0.1:6379\u0026gt; help bgsave BGSAVE - summary: Asynchronously save the dataset to disk since: 1.0.0 group: server 127.0.0.1:6379\u0026gt; bgsave  2) AOF 방식\n appendonly.aof 이름으로 저장됩니다. 장점\n AOF Redis는 매 초 fsync를 수행하여 데이터 정합성에서 유리하다(1 초의 쓰기 정도만 손실 할 수 있습니다) AOF 로그는 추가 전용 로그이므로 전원이 중단 될 경우 손상 문제가 없습니다. 어떤 이유로 (디스크가 가득 차거나 다른 이유로) 로그가 절반으로 작성된 명령으로 끝나더라도 redis-check-aof 도구로 쉽게 수정할 수 있습니다. Redis는 너무 커지면 백그라운드에서 AOF를 자동으로 다시 쓸 수 있어 데이터의 유실이 최소화 됩니다. AOF에는 이해하기 쉽고 구문 분석 할 수있는 형식으로 차례로 모든 작업의 로그가 포함되어 있습니다. (예를 들어, FLUSHALL 명령을 사용하여 모든 오류를 플러시 했더라도 로그를 다시 작성하지 않으면 서버를 중지하고 최신 명령을 제거한 다음 Redis를 다시 시작하여 데이터 세트를 저장할 수 있습니다.)  단점\n AOF 파일은 대개 동일한 데이터에 해당하는 RDB 파일보다 용량이 큽니다. AFS는 정확한 fsync 정책에 따라 RDB보다 느릴 수 있습니다. AOF로 인해 다시로드 할 때 정확히 동일한 데이터 세트가 재현되지 않을 가능성이 있습니다.   서버 실행할 때 AOF 활성\n$ src/redis-server --appendonly yes  Redis-cli에서 AOF 활성 ```shell $ src/redis-cli 127.0.0.1:6379\u0026gt; help BGREWRITEAOF BGREWRITEAOF - summary: Asynchronously rewrite the append-only file since: 1.0.0 group: server  127.0.0.1:6379\u0026gt; BGREWRITEAOF ```\n  참조 :\nhttps://redis.io/topics/persistence\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/cf-apprelease/",
        "title": "CF에 Cloud Native Application 배포하기 ",
        "tags": ["cloud native application", "cloud foundry", "cf"],
        "description": "",
        "content": " PaaS에 Cloud Application을 배포하는 전 과정을 정리해보았습니다. Java로 만든 Cloud Native Application을 CF에 배포하는 간단한 실습인데요, 개발 환경 세팅부터 PaaS에 애플리케이션 배포하기까지의 흐름을 살펴볼 수 있어요. 코딩이 필요하지 않은 초보 개발자 위주의 간단한 실습이라 처음 접하는 사람도 쉽게 따라할 수 있으실꺼예요. 처음부터 끝까지 따라하면서 Cloud Native Application 개발을 시작해보세요!\n실습 과정 아래 과정의 순서대로 진행합니다.\n준비 사항 전제조건  CF 기반 PaaS  PaaS에 애플리케이션을 배포, 업데이트, 삭제, 스케일링 등의 작업을 할 수 있는 CF기반의 PaaS가 있어야 합니다.  시스템 요구사항  Java (1.8 버전 권장)   개발 환경 설정 Cloud Native Application을 만들기 위해 필요한 개발 환경에 대한 설명과 설치 방법입니다.\nFramework Spring Boot  Spring Framework를 사용하는 프로젝트를 쉽고 빠르게 개발할 수 있는 Spring Framework의 서브 프로젝트입니다.  Language Java  Java (1.8 버전 권장) JDK와 같이 설치됩니다.  Tool 개발에 필요한 도구들입니다.\nMaven  프로젝트 의존성 관리, 라이브러리 관리, 프로젝트 생명주기 관리, 빌드 기능을 제공하는 프로젝트 관리 도구입니다.  설치 방법 STS에서 제공하는 Maven 플러그인을 사용하므로 별도의 설치가 필요하지 않습니다.   STS  STS는 spring에서 제공하는 이클립스 기반의 Spring 개발에 최적화되어 있는 개발 도구입니다. 설치 방법\n https://spring.io/tools/sts/all 에 접속합니다. 컴퓨터 OS 사양에 맞는 최신 파일을 다운로드합니다. 다운로드한 .zip 파일을 원하는 폴더에 압축 해제 한 후, sts-version.RELEASE 폴더 안에 STS 실행 파일을 실행시킵니다. worksapce를 지정해주면 STS 설치가 완료됩니다.   JDK  JDK는 Java Development Kit으로, Java로 애플리케이션을 개발하기 위해 설치해야 합니다. JDK 버전은 안정성을 위해 최신 버전보다 1.8버전 사용을 권장합니다. 설치 방법\n JDK 설치 전, 설치 여부를 확인합니다.\na. 명령 프롬프트 실행\n  Window OS: 윈도우키 + R 을 눌러 실행창에 cmd 입력 Mac OS: 터미널 실행   b. 명령 프롬프트에 java -version 입력\nc. 화면에 Java의 버전이 표시되지 않으면 JDK가 미설치되어있으므로 2번 과정을 이어서 하세요.\n JDK 다운로드 페이지에 접속합니다.\n JDK → Accept License Agreement 체크 → 컴퓨터 OS 정보에 맞는 설치 파일을 선택하여 다운로드 후 설치합니다.\n Java 환경변수 설정 (Window OS만 해당)\nJDK를 새로 설치한 경우, Java 환경 변수 설정이 필요합니다.\na. 제어판 → 시스템 → 고급시스템 설정을 클릭시킵니다.\nb. 다이얼로그 하단 오른쪽의 환경 변수를 클릭합니다.\nc. 사용자 변수 항목에서 새로 만들기를 클릭하고 JAVA_HOME 변수를 생성합니다.\n  JAVA_HOME 설정  변수 이름: JAVA_HOME 변수 값: JDK 경로 (예시. C:\\Program Files\\Java\\jdk1.8.0_77)    d. 시스템 변수 항목에서 PATH 환경 변수를 다음과 같이 설정합니다.\n  변수 이름: PATH 변수 값  %JAVA_HOME%\\bin PATH 변수가 이미 등록되어있는 경우: 변수 값 뒤에 ;%JAVA_HOME%\\bin를 입력    e. 확인을 클릭하여 모든 창을 닫습니다.\n 1번 과정을 따라서 JDK가 정상적으로 설치되었는지 확인합니다.\n   CF CLI  PaaS에 애플리케이션을 배포할 때 사용할 명령어 도구 CF CLI( Cloud foundry Command line interface)를 설치합니다.  자세한 설치 방법이나 CF 명령어 정보는 CF CLI 사용하기 포스트를 참고하세요. 설치 방법  https://github.com/cloudfoundry/cli/releases에 접속합니다. 컴퓨터 OS 정보에 맞는 파일을 다운로드합니다.  다운로드한 파일의 압축을 풀어서, CF CLI를 설치합니다.  명령 프롬프트에 cf 명령어를 실행합니다.\n화면에 cf 정보가 표시되면 정상적으로 설치가 완료된 것입니다.   버전 관리 \u0026amp; 원격 저장소  Github  버전 관리 도구입니다. 설치 방법  STS에서 제공하는 플러그인을 사용하므로 별도의 설치가 필요하지 않습니다.   샘플 프로젝트 개발하기 Cloud Native Application 샘플 프로젝트를 Github에서 다운로드해서, STS에서 실행해 봅니다. 샘플 프로젝트 다운로드  Spring에서 제공하는 샘플 프로젝트를 사용합니다.\n 프로젝트 소스를 다운로드 받기 위해 https://github.com/cloudfoundry-samples/hello-spring-cloud로 이동합니다. Clone or download 버튼을 클릭하여 샘플 프로젝트의 주소를 복사합니다. STS를 실행합니다.  Git Repositories view를 추가합니다. STS 메뉴의 Window → Show view → Other\u0026hellip;. → Git → Git Repositories을 선택하세요. Git Repositories View 상단에, Clone a repository 또는, 상단의 왼쪽에서 세 번째 아이콘 Clone a Git repogitory and add the clone to this view를 클릭하고 CloneURI를 선택합니다. 복사한 Github 주소가 Location URI에 입력되었는지 확인합니다. Next → Next → Finish를 선택해서 소스 코드를 로컬 레파지토리에 다운로드합니다.  샘플 프로젝트 STS 설정  STS의 Git Repositories View에서 hello-spring-cloud 프로젝트 선택 후 마우스 오른쪽 버튼 클릭 → Import Maven Projects를 선택합니다. hello-spring-cloud 프로젝트가 Package Explorer View에 애플리케이션이 정상적으로 추가되었는지 확인합니다.  JDK 설정\n a. STS 메뉴 Window → Preference → Java → Installed JREs를 선택합니다.\nb. 화면에 JRE 만 있다면 JDK를 추가합니다.\nc. Add → Next → Directory 선택 후 JDK 경로를 입력합니다.\nd. 추가한 JDK의 체크박스에 체크하여 defalut로 지정해 줍니다.   샘플 프로젝트 설명 hello-spring-cloud 프로젝트는 Spring Boot 프로젝트입니다. resources폴더의 home.html 페이지가 홈 화면이고, 클라우드 환경에서 helloworld 패키지의 HomeController 클래스에서 각 데이터베이스의 클래스명과 url 정보를 표시하는 애플리케이션입니다.\n Spring Boot에 대한 자세한 가이드는 Spring Boot Document를 참고하세요.\n hello-spring-cloud 특징  hello-spring-cloud는 클라우드 환경과, 기존과 같은 물리 서버 환경 모두에서 서비스할 수 있는 형태로 구성되어 있습니다.  hello-spring-cloud 프로젝트에 설정되어 있는 서비스는 아래와 같습니다.  MySql Redis Mongo DB Rabbit MQ   해당 미들웨어의 의존성 관계는 pom.xml에서 확인할 수 있습니다.\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; . . . \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-amqp\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-mongodb\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; hello-spring-cloud 구조  hello-spring-cloud src/main/java  src/main/resource  static : CSS, Javascript의 Static Resource 파일 template : UI Templates 파일 application.properties : Spirng boot 기본 설정 파일   src/test/java src manifest.yml: PaaS 애플리케이션 배포 부가정보 설정 파일로 애플리케이션 이름 및 바인딩되는 서비스 정보들을 기입 pom.xml: maven 설정 파일  샘플 프로젝트 빌드하기 로컬 실행 hello-spring-cloud 프로젝트를 로컬 환경에서 구동해봅니다.\n 애플리케이션 run을 하는 방법 2가지가 있습니다. STS의 Package Explorer View에서 hello-spring-cloud 선택 → 마우스 오른쪽 버튼 → Run AS → Spring Boot App 선택  Boot Dashboard View에서 local 하위의 hello-spring-cloud 프로젝트 선택 → 마우스 오른쪽 버튼 → (Re)start 선택\n 웹 브라우저에서 http://localhost:8080으로 이동합니다.\n 화면에 Spring Cloud Demo Application 메시지가 출력되면 정상적으로 실행된 것입니다.\n  지금은 로컬 서버로 실행했기 때문에, 화면 중간에 \u0026lsquo;The application is not running in a cloud environment.\u0026rsquo; 문구를 볼 수 있습니다. 애플리케이션을 클라우드 환경에 배포하고 필요한 서비스를 바인딩하면 데이터를 정상적으로 가져오는 화면을 볼 수 있습니다. 이제 클라우드에 배포를 진행해보겠습니다.\n빌드  PaaS에 배포할 JAR 파일을 만들기 위해서 빌드를 진행합니다.\n 빌드 설정  STS의 Package Explorer View에서 hello-spring-cloud 선택 → 마우스 오른쪽 버튼 → Run AS → Run configurations\u0026hellip; → Maven build를 더블 클릭합니다. configuration 창에서 다음 설정 정보를 입력합니다.  Name: hello-spring-cloud Goal : clean install  Apply 선택해서 설정 정보를 저장합니다.  빌드  STS의 Package Explorer View에서 hello-spring-cloud 선택 → 마우스 오른쪽 버튼 → Run AS → Run configurations\u0026hellip; → Maven build를 선택하여 빌드합니다.\n workspace의 샘플 프로젝트 하위에 tartget 폴더에 JAR 파일이 생성되었는지 확인합니다.   애플리케이션 배포하기 샘플 애플리케이션을 CF CLI를 이용해서 PaaS에 배포합니다. PaaS에 애플리케이션 배포하기  1. manifest.yml 작성 manifest.yml은 CF PaaS에 애플리케이션을 배포하는데 필요한 정보들을 작성하는 설정 파일입니다.\napplications: - name: hello-spring-cloud instances: 1 host: hello-spring-cloud-${random-word} path: target/hello-spring-cloud-0.0.1.BUILD-SNAPSHOT.jar  설정 내용에 대한 간단한 설명입니다.\n PaaS 환경에서 사용할 애플리케이션 이름: hello-spring-cloud 인스탄스 갯수: 1개 host: hello-spring-cloud 뒤에 PaaS에서 설정하는 랜덤 문구를 사용 path: 빌드한 jar 경로   Manifest에 대한 자세한 설명은 Cloud Foundry Documentaion을 확인하세요.\n 2. PaaS 로그인  명령 프롬프트을 실행합니다.  명령 프롬프트에서 hello-spring-cloud 프로젝트 경로로 이동합니다.\n 예시\n$ cd C:/Users/hello-spring-cloud  PaaS에 로그인을 하기 위해 cf login -a [PaaS API URL] 같이 명령어를 입력합니다.\n [PaaS API URL] 로그인할 PaaS API URL 예시 Emacs $ cf login -a http://api.paas.sk.com/  전 제가 사용하고 있는 Open PaaS의 PaaS API인 http://api.paas.sk.com URL을 사용하였어요. 이 PaaS는 SK 주식회사 C\u0026amp;C에서 만든 PaaS로, 권한이 있는 사람만 사용이 가능해서 예시처럼 사용하진 못해요. 준비하신 PaaS 테스트하시길 바랍니다.  PaaS에 가입한 Email과 Password 정보를 입력합니다. 아래 예시를 참고하세요.\nhello-spring-cloud seoyoungahn$ cf login -a http://api.paas.sk.com/ API 엔드포인트: http://api.paas.sk.com/ Email\u0026gt; abc@sk.com Password\u0026gt; 인증 중... 확인  애플리케이션을 배포할 조직(Org)과 영역 (Space)을 선택합니다. 로그인이 성공하면 애플리케이션을 배포할 곳을 선택하는 창이 나옵니다.\n조직을 선택하면 조직 내 존재하는 영역 목록이 표시되고, 숫자키를 눌러서 내가 배포할 위치를 선택합니다. 선택을 마치면 API 엔드포인트, 사용자, 내가 선택한 조직과 영역 정보가 표시됩니다. 전 dtlab 조직의 dev 영역에 애플리케이션을 배포하려고 합니다.  조직 선택(또는 Enter를 눌러 건너뜀): 1.sollab 2.dtlab Org\u0026gt; 2 대상 지정된 조직 dtlab 영역 선택(또는 Enter를 눌러 건너뜀): 1.dev 2.prod Space\u0026gt; 1 대상 지정된 영역 dev API 엔드포인트: http://api.paas.sk.com(API 버전: 2.69.0) 사용자: abc@sk.com 조직: dtlab 영역: dev  3. 서비스 생성 애플리케이션에서 서비스를 사용할 수 있도록 서비스 인스탄스를 생성하고 바인딩하는 작업이 필요합니다. hello-spring-cloud 애플리케이션은 아래의 서비스를 사용할 것입니다.\n MySql Redis MongoDB Rabbit MQ  서비스 생성하는 과정은 요약하면 다음과 같습니다.\n 사용할 서비스의 인스탄스가 이미 생성되어 있는 지 확인. 생성되어 있는 인스탄스를 그대로 사용하려면 서비스 생성 과정 필요 없음. 없으면 서비스 인스탄스를 생성해야 한다.\n마켓플레이스에 cf에서 제공하는 서비스 정보를 이용하여 서비스 인스탄스를 생성!  그럼 사용할 서비스 인스탄스가 생성되어있는지 확인부터 해보겠습니다.\n3.1 서비스 확인 명령 프롬프트에 cf services 명령어를 입력하면 선택한 영역에서 사용할 수 있는 서비스의 목록이 표시됩니다.\n$ cf services 제가 선택한 조직/영역에서 사용할 수 있는 서비스 인스탄스가 뭐가 있는지 볼까요?\n$ cf services abc@sk.com(으)로 dtlab 조직/dev 영역의 서비스를 가져오는 중... 확인 이름 서비스 플랜 바인딩된 앱 마지막 조작 amqp-service RabbitMQ standard create 성공 mariadb-dtlabs MariaDB Mysql-Plan2-100con dtlabs-registration-service update 성공 test-mongo Mongo-DB default-plan dtlabs-registration-service create 성공 userbff_autoscaler CF-AutoScaler free dtlabs-user-bff-service create 성공 RabbitMQ, MariaDB, Mongo-DB, CF-AutoSacler 서비스 인스탄스들이 이미 존재하네요. 대부분의 서비스가 앱에 바인딩되어 사용 중인걸 보니 저희 팀원이 만들어서 사용 중인 서비스인 것 같아요.\n전 RabbitMQ 서비스는 이미 만들어진 amqp-service를 사용하고, MariaDB 서비스 인스탄스는 이미 존재하는 인스탄스 말고 제가 새로 만들어서 사용해볼께요. 그 외 서비스는 서비스가 생성되어 있지 않으니 서비스를 직접 생성해야겠네요.\n3.2 서비스 생성 서비스 인스탄스를 생성하려면 내가 사용할 서비스가 CF PaaS에 있어야 합니다. 명령 프롬프트에 cf marketplace 입력해서 PaaS에서 제공하는 서비스 중 우리가 사용할 서비스가 있는 지 확인해봅니다. $ cf marketplace abc@sk.com(으)로 dtlab 조직/dev 영역에서 서비스를 가져오는 중... 확인 서비스 플랜 설명 App-Autoscaler-beta autoscaler-free-plan (Beta Version) Automatically increase or decrease the number of application instances based on a policy you define. CF-AutoScaler free Automatically increase or decrease the number of application instances based on a policy you define. MariaDB Mysql-Plan1-5con, Mysql-Plan2-100con* A simple mysql implementation Mongo-DB default-plan A simple mongo implementation Mongo-DB-Dev default* A simple MongoDB service broker implementation-Test Object-Storage object-storage-1GB, object-storage-100GB* A simple object-storage implementation RabbitMQ standard RabbitMQ is a robust and scalable high-performance multi-protocol messaging broker. Redis shared-vm Redis service to provide a key-value store Redis-dev Free Plen* Shared Redis server * 해당 서비스 플랜에 연관된 비용이 있습니다. 서비스 인스턴스를 작성하면 이 비용이 발생합니다. 팁: 주어진 서비스의 개별 플랜에 대한 설명을 보려면 \u0026#39;cf marketplace -s SERVICE\u0026#39;를 사용하십시오. 우리가 사용할 서비스 MySql, Redis, MongoDB, Rabbit MQ 정보를 확인할 수 있어요. 이 정보로 서비스를 생성해볼께요.\n서비스 생성 명령어 cf create-service [서비스 명][plan][생성할 서비스명]를 이용합니다.\n[서비스명][서비스 플랜][생성할 서비스 인스타스명]에 cf marketplace로 조회한 서비스 정보를 입력하면 됩니다.\n서비스를 생성한 후엔, 잘 생성되었는 지 cf services 명령어를 입력하여 확인합니다.\n그럼 서비스를 하나씩 생성해보겠습니다.\n MySql 서비스 생성\n 마켓플레이스에서 MariaDB 서비스 정보 확인\nMySql과 호환되는 MariaDB 서비스를 사용하겠습니다.\n$ cf marketplace abc@sk.com(으)로 dtlab 조직/dev 영역에서 서비스를 가져오는 중... 확인 서비스 플랜 설명 MariaDB Mysql-Plan1-5con, Mysql-Plan2-100con* A simple mysql implementation (생략)  MariaDB 서비스 생성\n마켓플레이스에서 확인한 아래의 정보를 토대로, cf create 명령어를 실행합니다.\n[생성할 서비스 인스타스명]은 원하는 서비스명을 정해서 입력하면 됩니다. maridb-service라고 지어볼까요?     서비스 서비스 plan 생성할 서비스명     MariaDB Mysql-Plan1-5con, Mysql-Plan2-100con* maridb-service    명령 프롬프트창에 cf create-service [서비스 명][ 서비스 plan][생성할 서비스 인스타스명]을 실행합니다.\n$ cf create-service MariaDB Mysql-Plan1-5con mariadb-service abc@sk.com(으)로 dtlab 조직/dev 영역에 서비스 인스턴스 mariadb-service 작성 중... 확인 \u0026lsquo;확인\u0026rsquo; 글자가 표시되면서 서비스가 생성되었습니다.\n 명령 프롬프트에 cf services 명령어를 입력해서 내 space에 서비스가 제대로 생성되었는지 확인합니다. $ cf services 이름 서비스 플랜 바인딩된 앱 마지막 조작 mariadb-service MariaDB Mysql-Plan1-5con create 성공 mariadb-service 서비스가 잘 생성되었네요. 이제 막 생성한 서비스 인스탄스라 바인딩된 앱이 없어서 아무 정보가 없음을 확인할 수 있습니다.\n   같은 방식으로, Redis, Mongo DB, Rabbit MQ 서비스 인스탄스를 생성하고, cf service 명령어로 확인하면 됩니다. 아래를 참고하세요.\n Redis, Mongo DB Rabbit MQ 서비스 생성\n 마켓플레이스에서 서비스 정보 확인\n$ cf marketplace abc@sk.com(으)로 dtlab 조직/dev 영역에서 서비스를 가져오는 중... 확인 서비스 플랜 설명 Mongo-DB default-plan A simple mongo implementation RabbitMQ standard RabbitMQ is a robust and scalable high-performance multi-protocol messaging broker. Redis shared-vm Redis service to provide a key-value store (생략)  Redis, MongoDB, RabbitMQ 서비스 생성 서비스 정보\n   서비스 서비스 plan 생성할 서비스명     Mongo-DB default-plan mongodb-service   RabbitMQ standard amqp-service   Redis shared-vm redis-service      MongoDB 서비스 생성\n$ cf create-service Mongo-DB default-plan mongodb-service abc@sk.com(으)로 dtlab 조직/dev 영역에 서비스 인스턴스 mongodb-service 작성 중... 확인 RabbitMQ 서비스 생성\n$ cf create-service RabbitMQ standard amqp-service 위에서 말했듯이 전 RabbitMQ 서비스 인스탄스는 이미 생성되어 있는 amqp-service를 사용할 것이지만 위의 cf create-service 명령어를 실행해보면, amqp-service 서비스 인스탄스가 이미 생성되어 있어서 아래와 같이 \u0026ldquo;Service amqp-service이(가) 이미 있음\u0026rdquo; 메세지를 볼 수 있죠.\n$ cf create-service RabbitMQ standard amqp-service abc@sk.com(으)로 dtlab 조직/dev 영역에 서비스 인스턴스 amqp-service 작성 중... 확인 Service amqp-service이(가) 이미 있음 Redis 서비스 생성\n$ cf create-service Redis shared-vm redis-service abc@sk.com(으)로 dtlab 조직/dev 영역에 서비스 인스턴스 redis-service 작성 중... 확인 서비스 생성을 모두 완료했습니다.\n 명령 프롬프트에 cf services 명령어를 입력해서 내 영역에 서비스가 제대로 생성되었는지 확인해볼께요.  $ cf services abc@sk.com(으)로 dtlab 조직/dev 영역의 서비스를 가져오는 중... 확인 이름 서비스 플랜 바인딩된 앱 마지막 조작 amqp-service RabbitMQ standard create 성공 mariadb-service MariaDB Mysql-Plan1-5con create 성공 mongodb-service Mongo-DB default-plan create 성공 redis-service Redis shared-vm create 성공 (생략)   우리가 방금 만든 4가지 서비스가 모두 잘 생성되어있음 확인할 수 있습니다.\n  3. 애플리케이션 배포   명령 프롬프트에서 hello-spring-cloud 폴더 경로로 이동합니다.\n 예시\n$ cd Documents/workspace/hello-spring-cloud  하위에 manifest.yml 파일이 있는 지 확인합니다.\n Linux\n$ ls 1 LICENSE manifest.yml src system.properties 2 README.md pom.xml abc@sk.com target  Window OS  $ dir  명령 프롬프트에 cf push를 입력합니다.\n$ cf push  명령 프롬프트에 표시되는 애플리케이션의 배포 상태를 확인합니다. 애플리케이션이 시작되었다는 메시지가 확인되면 정상적으로 애플리케이션이 배포된 상태입니다.  Manifest 파일 /Users/seoyoungahn/git/hello-spring-cloud/manifest.yml 사용 abc@sk.com(으)로 dtlab 조직/dev 영역에서 hello-spring-cloud 앱 업데이트 중... 확인 hello-spring-cloud-persistent-hypnotization.paas.sk.com 라우트 작성 중... 확인 hello-spring-cloud에 hello-spring-cloud-persistent-hypnotization.paas.sk.com 바인드 중... 확인 hello-spring-cloud 업로드 중... 업로드 중인 앱 파일 원본 위치: /var/folders/mc/0_v3hb1j2j71t_g53qg8qjg40000gn/T/unzipped-app423622616 462.7K, 99 파일 업로드 Done uploading 확인 abc@sk.com(으)로 dtlab 조직/dev 영역에서 hello-spring-cloud 앱 시작 중... Downloading liberty_buildpack... Downloading staticfile_buildpack... Downloading go_buildpack... Downloading java_buildpack... Downloading java_offline_buildpack... Downloaded staticfile_buildpack Downloading ruby_buildpack... Downloaded go_buildpack Downloading nodejs_buildpack... Downloaded liberty_buildpack Downloading dotnet_core_buildpack... Downloaded java_offline_buildpack Downloading python_buildpack... Downloaded java_buildpack Downloading binary_buildpack... Downloaded ruby_buildpack Downloading nodejs_buildpack_v167... Downloaded nodejs_buildpack Downloading php_buildpack... Downloaded dotnet_core_buildpack Downloading java-test... Downloaded python_buildpack Downloaded binary_buildpack Downloaded java-test Downloaded nodejs_buildpack_v167 Downloaded php_buildpack Creating container Successfully created container Downloading app package... Downloaded app package (30.7M) Staging... | -----\u0026gt; Java Buildpack Version: v3.11 | https://github.com/cloudfoundry/java-buildpack.git#eba4df6 | | ------------------------------------ |\u0026gt; Downloading Open Jdk JRE 1.8.0_111 from https://java-buildpack.cloudfoundry.org/openjdk/trusty/x86_64/openjdk-1.8.0_111.tar.gz (0.9s) Expanding Open Jdk JRE to .java-buildpack/open_jdk_jre (1.2s) -----\u0026gt; Downloading Open JDK Like Memory Calculator 2.0.2_RELEASE from https://java-buildpack.cloudfoundry.org/memory-calculator/trusty/x86_64/memory-calculator-2.0.2_RELEASE.tar.gz (0.0s) Memory Settings: -Xss349K -Xmx681574K -XX:MaxMetaspaceSize=104857K -Xms681574K -XX:MetaspaceSize=104857K -----\u0026gt; Downloading Spring Auto Reconfiguration 1.12.0_RELEASE from https://java-buildpack.cloudfoundry.org/auto-reconfiguration/auto-reconfiguration-1.12.0_RELEASE.jar (0.0s) Exit status 0 Staging complete Uploading droplet, build artifacts cache... Uploading droplet... Uploading build artifacts cache... Uploaded build artifacts cache (44.9M) Uploaded droplet (75.8M) Uploading complete Destroying container Successfully destroyed container 0 / 1 인스턴스 실행 중, 1 시작 중 1 / 1 인스턴스 실행 중 앱 시작됨 확인 `CALCULATED_MEMORY=$($PWD/.java-buildpack/open_jdk_jre/bin/java-buildpack-memory-calculator-2.0.2_RELEASE -memorySizes=metaspace:64m..,stack:228k.. -memoryWeights=heap:65,metaspace:10,native:15,stack:10 -memoryInitials=heap:100%,metaspace:100% -stackThreads=300 -totMemory=$MEMORY_LIMIT) \u0026amp;\u0026amp; JAVA_OPTS=\u0026#34;-Djava.io.tmpdir=$TMPDIR -XX:OnOutOfMemoryError=$PWD/.java-buildpack/open_jdk_jre/bin/killjava.sh $CALCULATED_MEMORY\u0026#34; \u0026amp;\u0026amp; SERVER_PORT=$PORT eval exec $PWD/.java-buildpack/open_jdk_jre/bin/java $JAVA_OPTS -cp $PWD/. org.springframework.boot.loader.JarLauncher` 명령을 사용하여 hello-spring-cloud 앱이 시작되었습니다. abc@sk.com(으)로 dtlab 조직/dev 영역에서 hello-spring-cloud 앱의 상태 표시 중... 확인 요청된 상태: started 인스턴스: 1/1 사용법: 256M x 1 인스턴스 URL: hello-spring-cloud-unskillful-contractor.paas.sk.com, hello-spring-cloud-persistent-hypnotization.paas.sk.com 마지막으로 업로드함: Sun Mar 4 10:02:58 UTC 2018 스택: cflinuxfs2 빌드팩: java-buildpack=v3.11-https://github.com/cloudfoundry/java-buildpack.git#eba4df6 java-main open-jdk-like-jre=1.8.0_111 open-jdk-like-memory-calculator=2.0.2_RELEASE spring-auto-reconfiguration=1.12.0_RELEASE 상태 이후 CPU 메모리 디스크 세부사항 #0 실행 중 2018-03-04 07:08:50 PM 0.0% 944K / 256M 1.3M / 1G skcc09n00914:hello-spring-cloud seoyoungahn$   4. 서비스 바인딩 애플리케이션이 정상적으로 배포되었다면, \u0026lsquo;3. 서비스 생성\u0026rsquo;과정에서 생성한 서비스를 애플리케이션에 연결시켜야 합니다.\n이것을 서비스 바인딩이라고 하는데요, 서비스 바인딩할 때는 애플리케이션을 재시작해주어 서비스를 적용시켜야 합니다.\n그 과정을 명령어를 통해서 보자면,\n 명령 프롬프트에 cf bind-service [애플리케이션명][바인딩할 서비스명] 명령어로 애플리케이션에 서비스를 바인딩한다. 명령 프롬프트에 cf restage [애플리케이션명]으로 애플리케이션을 재시작한다.\n  입니다.\nmariadb-service를 바인딩해보겠습니다.\n mariadb-service 바인딩\n$ cf bind-service hello-spring-cloud mariadb-service$ cf restage hello-spring-cloud 제가 실행해보니 아래 메세지와 함께 서비스가 바인딩되고, 앱이 재시작하는 것을 확인할 수 있습니다.\n$ cf bind-service hello-spring-cloud mariadb-service 확인 팁: 환경 변수 변경사항을 적용하려면 \u0026#39;cf restage hello-spring-cloud\u0026#39;을(를) 사용하십시오. $ cf restage hello-spring-cloud abc@sk.com(으)로 dtlab 조직/dev 영역에서 hello-spring-cloud 앱 다시 스테이징 중... Staging app and tracing logs... Downloading java_buildpack... Downloading go_buildpack... Downloading java_offline_buildpack... Downloading binary_buildpack... Downloading liberty_buildpack... Downloading python_buildpack... Downloaded go_buildpack Downloading php_buildpack... Downloaded liberty_buildpack Downloading dotnet_core_buildpack... Downloaded java_offline_buildpack Downloading java-test... Downloaded binary_buildpack Downloaded java_buildpack Downloading ruby_buildpack... Downloading nodejs_buildpack_v167... Downloading nodejs_buildpack... Downloaded php_buildpack Downloaded python_buildpack Downloading staticfile_buildpack... Downloaded dotnet_core_buildpack Downloaded java-test Downloaded ruby_buildpack Downloaded nodejs_buildpack_v167 Creating container Downloaded nodejs_buildpack Downloaded staticfile_buildpack Successfully created container Downloading app package... Downloading build artifacts cache... Downloaded app package (30.7M) Staging... Downloaded build artifacts cache (44.9M) | -----\u0026gt; Java Buildpack Version: v3.11 | https://github.com/cloudfoundry/java-buildpack.git#eba4df6 | | ------------------------------------ |\u0026gt; Downloading Open Jdk JRE 1.8.0_111 from https://java-buildpack.cloudfoundry.org/openjdk/trusty/x86_64/openjdk-1.8.0_111.tar.gz (found in cache) Expanding Open Jdk JRE to .java-buildpack/open_jdk_jre (1.3s) -----\u0026gt; Downloading Open JDK Like Memory Calculator 2.0.2_RELEASE from https://java-buildpack.cloudfoundry.org/memory-calculator/trusty/x86_64/memory-calculator-2.0.2_RELEASE.tar.gz (found in cache) Memory Settings: -Xmx681574K -Xms681574K -XX:MetaspaceSize=104857K -XX:MaxMetaspaceSize=104857K -Xss349K -----\u0026gt; Downloading Spring Auto Reconfiguration 1.12.0_RELEASE from https://java-buildpack.cloudfoundry.org/auto-reconfiguration/auto-reconfiguration-1.12.0_RELEASE.jar (found in cache) Uploading droplet... Staging complete Exit status 0 Uploading droplet, build artifacts cache... Uploading build artifacts cache... Uploaded build artifacts cache (44.9M) Uploaded droplet (75.8M) Uploading complete Destroying container Waiting for app to start... 이름: hello-spring-cloud 요청된 상태: started 인스턴스: 1/1 사용법: 256M x 1 instances routes: hello-spring-cloud-unskillful-contractor.paas.sk.com, hello-spring-cloud-persistent-hypnotization.paas.sk.com 마지막으로 업로드함: Sun 04 Mar 19:02:58 KST 2018 스택: cflinuxfs2 빌드팩: java-buildpack=v3.11-https://github.com/cloudfoundry/java-buildpack.git#eba4df6 java-main open-jdk-like-jre=1.8.0_111 open-jdk-like-memory-calculator=2.0.2_RELEASE spring-auto-reconfiguration=1.12.0_RELEASE start command: CALCULATED_MEMORY=$($PWD/.java-buildpack/open_jdk_jre/bin/java-buildpack-memory-calculator-2.0.2_RELEASE -memorySizes=metaspace:64m..,stack:228k.. -memoryWeights=heap:65,metaspace:10,native:15,stack:10 -memoryInitials=heap:100%,metaspace:100% -stackThreads=300 -totMemory=$MEMORY_LIMIT) \u0026amp;\u0026amp; JAVA_OPTS=\u0026#34;-Djava.io.tmpdir=$TMPDIR -XX:OnOutOfMemoryError=$PWD/.java-buildpack/open_jdk_jre/bin/killjava.sh $CALCULATED_MEMORY\u0026#34; \u0026amp;\u0026amp; SERVER_PORT=$PORT eval exec $PWD/.java-buildpack/open_jdk_jre/bin/java $JAVA_OPTS -cp $PWD/. org.springframework.boot.loader.JarLauncher 상태 이후 CPU 메모리 디스크 세부사항 #0 실행 중 2018-03-04T10:17:48Z 0.0% 76.2M of 256M 157.3M of 1G  마찬가지로, redis-service, mongodb-service, amqp-service도 cf bind-service로 서비스를 바인딩하고, cf restage로 애플리케이션에 서비스를 적용시킵니다. 3가지 서비스를 한꺼번에 적용하고 restage할께요.\n redis-service 바인딩  $ cf bind-service hello-spring-cloud redis-service mongodb-service 바인딩\n$ cf bind-service hello-spring-cloud mongodb-service  amqp-service 바인딩   $ cf bind-service hello-spring-cloud amqp-service 앱 restage 바인딩한 서비스를 적용합니다.\n$ cf restage hello-spring-cloud  애플리케이션 서비스 인스탄스 Credential 확인\n명령 프롬프트에 cf env hello-spring-cloud을 입력하여, hello-spring-cloud 앱에 바인딩된 서비스의 Credential 정보가 출력됩니다.  abc@sk.com(으)로 dtlab 조직/dev 영역의 hello-spring-cloud 앱에 사용할 환경 변수를 가져오는 중... 확인 시스템 제공: { \u0026#34;VCAP_SERVICES\u0026#34;: { \u0026#34;MariaDB\u0026#34;: [ { \u0026#34;credentials\u0026#34;: { \u0026#34;hostname\u0026#34;: \u0026#34;123.12.34.56\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;op_bf120a4d_69d8_401f_bd19_f231d363bddb\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;9d113775\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;3306\u0026#34;, \u0026#34;uri\u0026#34;: \u0026#34;mysql://9d113775835c56eb:9d113775835c56eb@172.16.21.32:3306/op_bf120a4d_69d8_401f_bd19_f231d363bddb\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;9d113775835c56eb\u0026#34; }, \u0026#34;label\u0026#34;: \u0026#34;MariaDB\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;mariadb-service\u0026#34;, \u0026#34;plan\u0026#34;: \u0026#34;Mysql-Plan1-5con\u0026#34;, \u0026#34;provider\u0026#34;: null, \u0026#34;syslog_drain_url\u0026#34;: null, \u0026#34;tags\u0026#34;: [ \u0026#34;mysql\u0026#34;, \u0026#34;document\u0026#34; ], \u0026#34;volume_mounts\u0026#34;: [] } ], \u0026#34;Mongo-DB\u0026#34;: [ { \u0026#34;credentials\u0026#34;: { \u0026#34;hosts\u0026#34;: [ \u0026#34;123.45.67.88:9999\u0026#34; ], \u0026#34;name\u0026#34;: \u0026#34;c573258d-455e-4d6b-84cd-120af2f93657\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;60635047-3298-46cb-9b9f-1123343ddd\u0026#34;, \u0026#34;uri\u0026#34;: \u0026#34;mongodb://1734a4d7-0da1-4abe-848c-00d37e6ac715:60635047-3298-46cb-9b9f-145da122ef34@155.66.77.99:88888/c5d-455e-4d6b-84cd-120af2657\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;1734a4d7-0da1-4abe-848c-00d37e6ac715\u0026#34; }, \u0026#34;label\u0026#34;: \u0026#34;Mongo-DB\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;mongodb-service\u0026#34;, \u0026#34;plan\u0026#34;: \u0026#34;default-plan\u0026#34;, \u0026#34;provider\u0026#34;: null, \u0026#34;syslog_drain_url\u0026#34;: null, \u0026#34;tags\u0026#34;: [ \u0026#34;mongodb\u0026#34;, \u0026#34;document\u0026#34; ], \u0026#34;volume_mounts\u0026#34;: [] } ], (생략)  5. 애플리케이션 확인   cf apps로 배포한 애플리케이션의 URL 정보를 확인합니다.\n$ cf apps  화면에 표시되는 URL로 접속해서 애플리케이션이 정상 동작하는 지 확인합니다.   $ cf apps abc@sk.com(으)로 dtlab 조직/dev 영역의 앱 가져오는 중... 확인 이름 요청된 상태 인스턴스 메모리 디스크 URL hello-spring-cloud started 1/1 256M 1G hello-spring-cloud-unskillful-contractor.paas.sk.com, hello-spring-cloud-persistent-hypnotization.paas.sk.com (생략) 웹 브라우저로 URL에 접속했을 때, 아래 그림과 같이 Spring Cloud Demo Application 메시지가 출력되면 정상적으로 실행된 것입니다. URL 값에 표시된 http://hello-spring-cloud-unskillful-contractor.paas.sk.com 으로 접속해보겠습니다.   마치며 지금까지 Cloud Native Application을 개발하기 위한 간단한 개발 환경을 만들어보고, 실제로 샘플 프로젝트를 통해서 애플리케이션 빌드와 PaaS에 애플리케이션 배포까지 전반의 과정을 같이 해보았습니다. 어렵지 않죠? CF의 다양한 기능을 알고 싶다면 Cloud Foundry 공식 홈페이지에서 도움을 받을 수 있습니다. 참고하세요!\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/microservices-io/",
        "title": "Microservices io 페이지 훑어보기 ",
        "tags": ["microservice", "microservices.io", "cloud application"],
        "description": "",
        "content": " 최근 마이크로 서비스 아키텍처를 적용하는 사이트가 점차 많아지면서 Cloudz Labs에도 마이크로 서비스 아키텍처에 대한 문의가 부쩍 많아지고 있습니다.\nCloudZ Labs에서는 작년부터 마이크로 서비스 아키텍처 적용을 위해 필요한 기술들을 리서치하고 적용하는 업무를 진행하고 있습니다.\n마이크로 서비스 아키텍처에 관련된 항목을 잘 정리해둔 Microservices.io 페이지 내용을 훑어보며 CloudZ Labs에서 이미 적용 중인 내용과 보충할 내용 추가 리서치가 필요한 내용들을 정리해 볼 생각입니다.\nMicroservices.io 페이지를 정리한 내용입니다. http://microservices.io/\n마이크로 서비스 적용 방법 마이크로 서비스 아키텍처 적용 프로세스\n 패턴 결정 -\u0026gt; 패턴에 도입된 문제 해결 -\u0026gt; 둘 이상이 대안이 되는 경우 한가지 솔루션을 선택 -\u0026gt; 더 이상 해결할 문제가 없을때 까지 반복  모놀리식 vs 마이크로 서비스 패턴 결정 애플리케이션 아키텍처 패턴으로 모놀리식과 마이크로 서비스 패턴 중에 선택하고, 마이크로 서비스 패턴을 선택했다면 마이크로 서비스 아키텍처에서 고려되어야 할 문제들을 도출하고 해결 방법을 찾아야 함.\n한 가지 이상의 대안이 있을 경우 최선의 방법으로 한가지 선택.\nCore patterns Microservice  지속적인 전달/배치를 가능하게 하여 소프트웨어 개발을 가속화 각 구성요소는 독립적으로 배포 가능해야함 조직을 소규모로 하며(6-10명) 각 팀은 하나 이상의 서비스를 담당한다.  장점  서비스가 상대적으로 작음 개발자가 더 쉽게 이해 가능함 개발자가 생산성이 높아지고 배포 속도도 높아짐 다른 서비스와 독립적으로 배포됨으로 새 버전의 서비스를 쉽게 배포할 수 있음 독립적으로 확장이 용이 기술 스택에 대한 장기 약속을 없앰, 새로운 서비스를 만들 때 그 팀이 원하는 기술 스택으로 작성하면됨  단점  분산시스템을 만드는 추가 복잡성을 처리해야 함 테스트 어려움 서비스 간 통신 메커니즘 구현 필요 분산 트랜잭션을 구현하지 않고 여러 서비스에 걸쳐있는 유스케이스를 구현해야 함 여러 서비스에 걸처있는 유스케이스를 구현하려면 팀 간의 긴밀한 조정이 필요함 배포 복잡성, 운영 복잡성 증가된 메모리 소비  사례  넷플리스 아마존 : 2계층 아키텍처를 사용하다가 백엔드를 수백 개로 분리, 웹사이트에서는 100~150개의 API를 호출해서 웹페이지 구성 이베이 등   Decomposition 마이크로 아키텍처 적용 시 마이크로 서비스를 어떤 단위로 분리해서 운영할 지 결정 필요\n이와 관련되서 Cloudz Labs 에서는?  업무 담당자들과 마이크로서비스 분리 워크샵 진행 비지니스 관점, DDD 관점 등 다양한 방식으로 마이크로서비스 분리를 하기위한 방법 리서치 필요  Deployment patterns 마이크로 서비스는 어떻게 패키지 되고 배포되나?\n 서비스는 다양한 언어, 프레임워크, 프레임워크 버전으로 작성됨 각 서비스는 처리량 및 가용성을 위해서 여러 개의 서비스 인스턴스로 구성됨(스케일링) 서비스는 독립적으로 배포 가능하고 확장 가능해야 함 서비스 인스턴스 간에는 서로 격리되어야 함 서비스를 신속하게 구축하고 배포할 수 있어야 함 서비스에서 사용하는 리소스(CPI 및 메모리)를 제한할 수 있어야 함 각 서비스 인스턴스를 모니터링해야 함 신뢰할수 있는 배포 가능한 비용 효율적으로 응용프로그램을 배포해야 함  Muliple service instances per host Service Instance per host Service Instance per VM Service Instance per Container Serverless deployment Service Deployment platform  PaaS나 K8S를 활용한 배포 및 라우팅 로드밸런싱은 이 항목에 해당됨  이와 관련되서 Cloudz Labs 에서는?  PaaS나 K8S 같은 플랫폼에서 제공하는 라우팅 및 로드밸런싱 사용, 해당 메커니즘을 가이드 하고 있음 우리가 사용하지 않는 다른 배포 모델도 리서치 필요  Cross Cutting Concerns 분산된 모든 모듈에 연관된 기능을 어떻게 구현하고 운영하나?\nMicroservice chassis(공통을 위한 boilerplate 작성)  Externalized Configuration Logging Helth check Metrics Distributed Tracing → 서비스가 수백수천 개 되면 이런 공통 관련 설정을 수행하는데 보내는 시간도 무시할 수 없다. 그러므로 이미 결정된 공통 기능들을 바로 구성 가능한 보일러 플레이트(bolierplate) 형태로 작성해 두고 그것을 사용한다.  Externalized configuration  네트워크로 연결되는 모든 외부 서비스들의 사용을 구체화해서 환경별로 설정 파일 분리하고 외부 환경 변수 등을 (active property)활용해서 사용해야 함 설정에 따라 수정및 재컴파일 하지 않기 위함  이와 관련되서 Cloudz Labs 에서는?  Spring Boot용 bolierplate 작성 Config 분리(PaaS에서 Active property 사용해서 환경에 맞는 정보 사용) Config 서버 사용  Communication Style 서비스는 클라이언트 요청을 처리해야 하고 때에 따라서 다른 서비스와 통신이 필요한 경우도 있다.\nRemote Procedure Invocation 요청/응답기반 프로토콜\n REST gRPC Apache Thrift  이점\n 간단하고 친숙하다 요청/회신 쉽다.  단점\n 일반적인 요청/ 응답만 지원 (알림, 요청/비동기응답, 발행/구독, 발행/비동기 같은 패턴은 지원 안 함) 상호 작용이 지속되는 동안 클라이언트와 서버가 계속 사용 중이어야 함으로 가용성이 감소함 클라이언트는 사용하려는 서비스 인스턴스의 위치를 알고 있어야 함  Messaging 비동기 메세징 (메세징 채널을 통해 메시지를 교환하여 통신)\n Apach Kafka Rabbit MQ  이점\n 서비스와 클라이언트를 분리하는 느슨한 결합 consumer가 메시지를 처리할때 까지 메시지 브로커가 그 메세지를 버퍼링하므로 서비스의 가용성이 향상됨 요청/응답, 알림, 요청/비동기응답, 발행/구독, 발행/비동기응답을 포함한 다양한 통신 패턴 지원  단점\n 메시지 브로커의 가용성이 높아야 하며 이에 따라 브로커의 복잡성이 증가 요청/응답 스타일의 통신은 오히려 더 복잡해짐 클라이언트는 메시지 브로커의 위치를 알아야 함  Domain-specific protocol  SMTP 및 IMAP과 같은 이메일 프로토콜 RTMP, HLS 및 HSD와 같은 미디어 스트리밍 프로토콜 등  이와 관련되서 Cloudz Labs 에서는?  REST API 가이드 Message 관련해서는 다양한 아키텍처에 맞는 리서치 필요  External API API Gateway/Backend For Front-end 마이크로 서비스 기반 애플리케이션의 클라이언트는 개별 서비스에 어떻게 접근하나?\n 마이크로 서비스가 제공하는 API는 클라이언트가 필요로 하는 것과 다르다. 마이크로 서비스는 일반적으로 세분화된 API를 제공하고 클라이언트는 이를 조합해서 사용함 (많은 클라이언트가 사용 가능하게 하기위해서) 서비스 인스턴스 수와 위치가 동적으로 변경됨  서비스는 다양한 프로토콜을 사용할 수 있으며 일부 프로토콜은 웹 친화적이지 않을 수도 있다.  해결방법\n 모든 클라이언트에 대한 단일 진입점인 API Gateway를 구현하거나 각 클라이언트 종류에 대해 별도의 BFF를 구현한다.  API Gateway 단순히 API 프록시/라우트 역할만 수행 API aggregation BFF 각 클라이언트에 최적화된 API를 제공함   이점\n 마이크로 서비스로 백엔드가 분할되어 있는 방식을 클라이언트에게는 숨길 수 있다. 서비스 인스턴스 위치를 API Gateway에서 결정함 각 클라이언트에 최적의 API를 제공함 요청/응답 왕복 수를 줄인다. 클라이언트에서 백엔드 서비스를 여러 번 호출하는 복잡성을 단순화 시킨다. 표준이 아닌 프로토콜을 사용하는 백엔드 서비스와 통신 가능(프로토콜 변환)  단점\n 운영 복잡성: API Gateway 역시 개발, 배포 ,관리되어야 하는 하나의 서비스임 API Gateway layer 추가로 네트워크 레이턴시 증가함  검토\n API Gateway 구현방법 : Netty, Spring Reactor → NIO 기반 라이브러리   이와 관련되서 Cloudz Labs 에서는?  API Gateway(ZUUL) 가이드 BFF 가이드   Service Discovery 서비스 간에는 서로 호출이 필요함, 서비스의 인스턴스는 수와 위치가 동적으로 변경되기 때문에 클라이언트에서 이렇게 동적으로 변하는 서비스에 요청할 수 있는 메커니즘이 적용되어야 함\nClient-side Discovery  클라이언트에서 서비스 레지스트리에 서비스 인스턴스의 수와 위치를 얻음 Eureka 서비스 ↔ 서비스 레지스트리 통신으로 요청하려는 서비스의 정보를 얻고, 서비스 ↔ 요청 서비스 통신 수행  이점\n Server side Discovery 보다 이동 및 네트워크 layer가 적음  단점\n 서비스 레지스트리에 연결해야 함 scala나 javascript 같은 언어는 클라이언트 서비스 검색 로직을 구현해야 함  Server-side Discovery  라우터를 두고 이 라우터를 활용해서 통신 서비스 ↔ 라우터, 라우터 ↔ 서비스 레지스트리, 라우터 ↔ 요청할 서비스  ELS(Elastic Load Balancer)  이점\n Client Side Discovery에 비해서 클라이언트에서 처리할 코드가 없어서 간단함 일부 클라우드 환경에서 이 기능을 제공함(AWS Elastic Load Balancer)  단점\n 라우터가 클라우드 환경에서 제공하지 않는다면 관리해야 하는 또 하나의 운영 서비스가 됨 Client Side Discovery 보다 네트워크 레이어가 많음  이와 관련되서 Cloudz Labs 에서는?  Eureka를 활용한 Client Side Discovery 가이드(JAVA로 구성된 앱이 아닐 경우 대안 리서치 필요)  Reliability 하나의 요청을 처리하기 위해 여러 개의 서비스들이 협업하는 경우가 있음. 한서비스가 다른 서비스를 동기로 호출할 때 다른 서비스의 장애가 내 서비스의 장애가 되지 않도록 처리해야 함.\nCircuit Breaker  요청하는 서비스에서 요청의 연속실패 회수의 임계값을 설정하고 임계값을 초과하면 Circuit Breaker가 동작해서 서비스를 호출하려는 모든 시도가 즉시 실패하고 미리 등록한 fallback 함수를 수행함 Netfilx OSS hysrix  이점\n 서비스의 장애가 전파되지 않음  단점\n 적절한 임계치를 설정하는 것이 어려움  이와 관련되서 Cloudz Labs 에서는?  Hystrix 가이드  Data management 데이터 일관성 유지하면서 쿼리 수행 방법(데이터 베이스 관리 영역)\n이와 관련되서 Cloudz Labs 에서는?  마이크로 서비스 아키텍처에서 DB 나누고 운영하는 부분이 가장 어렵다.(계속 계속 리서치 필요)  Security 백엔드 API 서비스에 (마이크로 서비스) Client의 신원을 전달하는 방법은 무엇이 있을까.?\nAccess Token  토큰을 활용해서 요청을 인증함  이와 관련되서 Cloudz Labs 에서는?  Oauth 리서치 중  Testing 마이크로 서비스는 다양한 서비스로 구성됨. 다른서비스를 호출하면서 하나의 요청을 처리하는 경우도 있으므로 내 서비스 이외에 다른 서비스가 올바르게 작동하는지 확인하는 자동화된 테스트를 작성해야 한다.\nEnd-to-End Test는 어렵기 때문에.\nService Component Test  A test suite that tests a service in isolation using test doubles for any services that it invokes.  Service Integration Contract  서비스를 사용하는 다른서비스 개발자가 작성한 테스트 suite 활용  이와 관련되서 Cloudz Labs 에서는?  Contract Test 리서치  Observability Log aggregation 마이크로 서비스로 구성된 애플리케이션은 여러 서비스, 여러 인스턴스로 구성된다. 하나의 요청은 여러 서비스를 걸쳐 있을 수 있다.\n각 서비스 인스턴스는 수행 중인 작업에 대한 정보를 표준화된 형식으로 로그파일에 작성해야 하며 로그에는 오류, 경고, 정보 및 디버거 메시지가 들어있다.\n 중앙 집중식 로깅 서비스 사용 권장 많은 로그를 처리하기위해서는 많은 인프라 필요  Application Metrics 개별 작업에 대한 통계를 수집하는 service instument 활용\n 애플리케이션 동작에 대한 깊은 통찰력을 제공한다.  Audit logging  사용자가 최근 수행한 작업을 파악하는데 유용 DB에 사용자 활동 기록  Distributed tracing  마이크로 서비스의 하나의 요청은 여러 개의 서비스에 걸쳐 있을 수 있음. 이런 경우 하나의 요청을 확인하기 위한 분산되어있는 여러 로그를 한꺼번에 확인해야 될 필요성이 있음 Zipkin  Exception Tracking  exception을 집계하고 추적하고 알림을 주는 중앙 집중식 exception처리 서비스를 둔다.  Health Check API  유효한 서비스에만 요청이 되도록 서비스가 각 서비스가 health check되어야 함  Log Deployments and Changes  모든 배포와 변경사항을 기록해야 한다.  이와 관련되서 Cloudz Labs 에서는?  PaaS에서 로그 중앙 수집 방법 가이드 zipkin 가이드  UI patterns Server-side page fragement composition  각 마이크로 서비스에서 HTML 조각을 생성하고, UI 팀은 서비스별 HTML 조각을 합쳐서 페이지를 작성하는 페이지 템플릿을 제공한다.  Client-side UI composition  Each team develops a client-side UI component, such an AngularJS directive, that implements the region of the page/screen for their service. A UI team is responsible implementing the page skeletons that build pages/screens by composing multiple, service-specific UI components.  이와 관련되서 Cloudz Labs 에서는?  마이크로서비스에서 가장 효율적인 Front 구현에 대해서 지속적인 리서치 필요   마이크로서비스로 분행하는 방법(모델링)  서비스는 소규모 팀이(two pizza : 6-10명) 개발하고 테스트 할 수 있을 만큼 작아야 함 각 팀은 자율적이어야 하고 다른 팀과 최소한의 협업을 통해 서비스를 개발하고 배포하여야 함 OOD(Object Oriented Design), SRP(Single Resposibility Priciple), CCP(Common Closure Principle) 새롭고 변경된 요구 사항이 단일 서비스에만 영향을 미치도록 분해함 각 서비스는 구현을 캡슐화하는 API로 느슨하게 결합되어야 한다. 클라이언트에 영향을 주지 않고 구현을 변경해야 함 각 서비스는 테스트 가능해야 함  비지니스 역량으로 분해 분해방식  비지니스 기능에 해당하는 서비스 정의 비지니스 기능은 비지니스 아키텍처 모델링의 개념 온라인 상점의 비지니스 모델링\n  이점\n 비지니스 기능이 이미 안정적이기 때문에 안정적인 아키텍처가 됨(이해 쉬움) 개발팀은 기술적인 기능보다는 비지니스 가치를 제공하기 위한 자치 조직이 됨 응집력 있고 느슨하게 결합된 서비스 (이미 비지니스에서 구분된)  문제점\n 비지니스 기준으로 서비스를 분리하기 위해서 비지니스에 대한 이해 필요  하위 도메인으로 분해 분해방식  DDD( Domain Driven Design) 하위 도메인에 해당하는 서비스를 정의. 도메인은 여러 하위 도메인으로 구성됨  core- 비지니스를 위한 핵심 차별화 요소이자 애플리케이션의 가장 중요한 부분 supporting - 비지니스와 관련은 있지만 차별화 요소 아님, 사내에서 개발하거나 외주줘도 됨 generic - 비지니스와 관련이 없는 기능으로 가능하다면 상용 서비스들을 활용  온라인 상점의 하위도메인   이점\n 하위 도메인이 이미 안정적이기 때문에 안정적인 아키텍처가 됨 개발팀은 기술적인 기능보다는 비지니스 가치를 제공하기 위한 자치 조직이 됨 응집력 있고 느슨하게 결합된 서비스 ( 이미 비지니스에서 구분된)  문제점\n 하위 도메인과 서비스를 식별하려면 비지니스에 대한 이해 필요  데이터 일관성 유지하면서 쿼리 수행방법( 데이터 베이스 관리영역)  서비스당 데이터베이스를 가지는것을 원칙으로 한다. 공유 데이터는 안티 패턴임  마무리 마이크로 서비스는 알면 알수록 정해진 답이 없고, 상황에 맞게 마이크로 서비스를 시도하는 목적에 최적화 하기 위한 아키텍처로 그때그때 달라져야 된다는 생각이 듭니다. 그러기 위해선 마이크로 서비스에 활용할 수 있는 기술 요소들에 대한 정확한 이해가 가장 중요하겠지요.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/docker-in-docker/",
        "title": "Docker in docker",
        "tags": ["docker in docker", "docker", "dind"],
        "description": "",
        "content": " 지난 한주 동안 애플리케이션을 Kubernetes 배포하는 CI/CD 파이프라인 구성했습니다. K8S CI/CD 파이프라인 구성은 차후에 포스팅하기로 하고, K8S용 파이프라인 구성에 사용된 Docker in docker 기술을 소개하겠습니다.\nDinD(Docker In Docker)기술은 컨테이너 내부에 Docker를 사용하는 기술을 말하며 일반적으로 K8S CI/CD 구성, Docker 엔진 개발 등에 사용됩니다.\nDinD 원리 DinD 원리를 이해하기 위해서는 Docker의 명령어 실행 구조를 이해해야 합니다.\nDocker를 설치/실행하면 Docker daemon과 Docker CLI가 설치/실행됩니다. 콘솔 화면에서 Docker 명령어를 입력할 경우 Docker CLI가 명령어를 받아서 Socket을 통해 Docker daemon에 전달합니다. Docker 설치 경로 중 /var/run/docker.sock 파일을 볼 수가 있는데, 이 파일이 Docker daemon에게 명령을 내릴수 있는 인터페이스입니다. Docker CLI가 /var/run/docker.sock를 통해 daemon에 명령어를 전달한다는 것은, 외부의 Docker CLI도 저파일에 접근 할수 있다면 해당 Docker에 명령을 내릴 수 있는 것 입니다.\n결국 /var/run/docker.sock 파일만 공유를 하면 Local에 설치된 Docker를 컨테이너 내부에서 사용할 수 있고, 또한 컨터에너 내부에 설치된 Docker를 다른 컨테이너 내부에서 사용이 가능해집니다.\n사용해보기 로컬에 설치된 Docker를 컨테이너에서 사용하기 위해 아래 명령어를 통해 수행합니다.\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock docker docker ps\n-v 옵션을 통해 로컬 Docker의 /var/run/docker.sock를 컨테이너의 /var/run/docker.sock로 공유합니다. 사용된 이미지는 Docker가 설치된 이미지로 원래는 컨테이너 내부에서 자신의 Docker를 사용하도록 만들어진 이미지. 단 위의 명령어는 /var/run/docker.sock를 local것으로 교체하여 local Docker에 명령어 전달합니다. 이미지에 대한 자세한 내용은 Docker image에서 참고하세요.\n결과 로컬 PC의 Docker ps 결과물과 동일한 내용이 출력됩니다.\n$ docker run --rm -v /var/run/docker.sock:/var/run/docker.sock docker docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8614e57a7099 docker \u0026#34;docker-entrypoint.s…\u0026#34; 1 second ago Up Less than a second clever_morse"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/zuul-swagger-integration/",
        "title": "Zuul Swagger 연동",
        "tags": ["swagger", "zuul", "srping cloud"],
        "description": "",
        "content": " MSA(Micro Service Architecture) 활용하여 팀 홈페이지를 제작하는 과정에서 User BFF에서 백엔드 서비스를 호출하기 위해 Zuul을 사용하였습니다. 여러 가지 서비스가 Zuul을 통해 User BFF에 노출되고 있어, Zuul을 통해 서비스되는 모든 API를 하나의 Swagger page로 노출하고 싶었습니다. 다행히도 Swagger에서 Zuul 연계하는 기능이 있어 이를 소개합니다.\nMSA에 대한 자세한 설명은 차후에 포스팅하기로 하고, 이해를 돕기 위해 간단한 홈페이지 아키텍처 첨부합니다.\ngraph LR; A[Client] --|Request| B(User BFF) B --|Request| C{Zuul} C --|Request| D[Mail service] C --|Request| E[User service]  사용법 1. Swagger2 라이브러리 추가 Zuul 프로젝트에 swagger2라이브러리를 추가합니다.\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springfox-swagger2\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springfox-swagger-ui\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 2. Swagger2 설정 아래 소스와 같이 SwaggerResourcesProvider를 상속받아 SwaagerConfiguration을 생성 후 Zuul에 등록된 서비스들의 API DOC URL을 입력합니다.\nimport java.util.ArrayList; import java.util.List; import org.springframework.boot.autoconfigure.EnableAutoConfiguration; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Primary; import org.springframework.stereotype.Component; import springfox.documentation.swagger.web.SwaggerResource; import springfox.documentation.swagger.web.SwaggerResourcesProvider; import springfox.documentation.swagger.web.UiConfiguration; import springfox.documentation.swagger2.annotations.EnableSwagger2; @Component @Primary @EnableAutoConfiguration @EnableSwagger2 public class SwaggerConfiguration implements SwaggerResourcesProvider { @Bean public UiConfiguration uiConfig() { return new UiConfiguration(\u0026#34;validatorUrl\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;alpha\u0026#34;, \u0026#34;schema\u0026#34;, UiConfiguration.Constants.DEFAULT_SUBMIT_METHODS, false, true, 60000L); } @Override public List\u0026lt;SwaggerResource\u0026gt; get() { List\u0026lt;SwaggerResource\u0026gt; resources = new ArrayList\u0026lt;\u0026gt;(); resources.add(swaggerResource(\u0026#34;dtlabs-registration-service\u0026#34;, \u0026#34;{SERVICE_API_DOC_URL}\u0026#34;, \u0026#34;2.0\u0026#34;)); resources.add(swaggerResource(\u0026#34;dtlabs-data-mgmt-service\u0026#34;, \u0026#34;{SERVICE_API_DOC_URL}\u0026#34;, \u0026#34;2.0\u0026#34;)); resources.add(swaggerResource(\u0026#34;dtlabs-user-oauth-service\u0026#34;, \u0026#34;{SERVICE_API_DOC_URL}\u0026#34;, \u0026#34;2.0\u0026#34;)); return resources; } private SwaggerResource swaggerResource(String name, String location, String version) { SwaggerResource swaggerResource = new SwaggerResource(); swaggerResource.setName(name); swaggerResource.setLocation(location); swaggerResource.setSwaggerVersion(version); return swaggerResource; } }"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/ubiquitous-language-in-msa/",
        "title": "MSA 에서 유비쿼터스 언어(보편 언어)의 중요성",
        "tags": ["ubiquitous language", "bounded context", "ms 분리 워크샵", "domain driven design"],
        "description": "",
        "content": " Health-Care Application의 MS 분리 워크샵을 진행한 후 자문해봤다.\n 보상/혜택  Reward가 사용자에게 주어지면 그게 사용자 입장에서는? Item인가 Reward인가? 운동을 안해서 주는 경고도 Reward인가? 내가 운동을 많이 했다고 선물이 아니라 칭찬메시지가 오는 것도 Reward인가? 친구가 나를 역전했다고 알려주는 것이 Reward인가?  챌린지/목표/이벤트  챌린지와 목표의 차이는? 챌린지가 이벤트와 다른 것인가? 마케팅 기획이 이벤트인가? 챌린지를 집단으로 한다면 챌린지인가 이벤트인가?  포인트  포인트가 챌린지의 보상인가? 포인트가 사용자의 경험치인가? 운동량과 포인트는 누구에게나 동일한 비율로 적재되는가?  리워드 지갑/혜택 저장  리워드를 지갑에 넣는 순간 Item 아닌가?  활동/액티비티  걸음 등 운동만 활동인가? 회원 가입도 활동으로 보는가? 쿠폰 사용도 활동으로 보는가? 친구 맺기 등 소셜 기능은 활동인가? 활동은 정보 수집의 대상인가?   문제점 투성이다. 같은 의미로 내뱉는 말들이 제각각 다르고, 똑같은 말로 들리는데 그 의도는 다른 것이란다.\n이 이름들로 API 를 구현하고 DB Table 을 정의한다면, 짬뽕 시스템이 될 것만 같다.\n이를 타파할 방법은 \u0026hellip; Ubiquitous Language(보편 언어) 다.\nWhat ? Ubiquitous Language(보편 언어) 는 도메인 전문가, 아키텍트, 개발자 등 프로젝트 구성원 모두에게 공유된 언어를 뜻한다.\n표준 비즈니스 용어나 도메인 전문가들의 은어가 아니다. 커버 영역은 전 세계 공통, 전사 조직 등에서 소화하는 것이 아니라 프로젝트를 수행하는 조직 내부에서만 유효하다.\n유비쿼터스 언어는 다양하게 사용된다.\n 구성원 간 의사소통 설계 모델링, UML 작성 등 설계를 바탕으로 구현시의 코드의 클래스명, 메소드명 정의  따라서, 유비쿼터스 언어(보편 언어) 가 도메인의 의도를 정확히 반영하고 핵심 개념이 잘 표현될 수 있도록 정의해야 한다.\nWhy ? 1. 커뮤니케이션의 오류는 프로젝트 진행의 병목 지점이 된다. 구성원 간 각자의 언어를 사용하는 경우, 의사소통에 비용이 든다. 상대방이 내뱉은 단어를 내가 사용하는 단어로 번역해서 이해해야 하고, 회의 내용을 공유할 때 각자 해석이 분분한 지점에 첨언을 달아야 한다.\n혹은 전달받은 정의서대로 구현을 마치고 나면, 그 결과가 아니라고 할 때가 있다. 정의서를 펼쳐놓고 얼굴 맞대로 얘기하면 그제야 어디서 어긋 낫는지 파악된다. 재작업의 연속이다.\n2. MSA를 도입한 조직에서 언어는 업무 범위와 책임을 정의한다. Bounded Context는 Ubiquitous Language(보편 언어) 로 정의된 모델을 기반으로 경계를 만든다. MSA에서 각각의 마이크로 서비스는 Bounded Context와 대응된다. 그리고 마이크로 서비스는 담당 조직의 규모, 기술 스택, 책임 등에 매우 연관이 깊다.\n즉, Ubiquitous Language(보편 언어) 가 조직에 영향을 끼친다.\nHow ? 용어 사전을 만든다. Wiki 등 다양한 형태로 도메인 전문가, 아키텍트, 개발자 등 프로젝트 구성원이 모여 용어 사전을 만든다. 워크샵의 형태가 적당하다.(MS 분리 워크샵 등)\n약어나 코드 값 등 직관적으로 이해가 되지 않는 내용은 줄이고 Full Name을 사용한다.\n특정 형식 없이 그림 등의 형태로 관리한다.\n만들어진 용어 사전은 모두가 편히 접할 수 있는 곳에 위치한다.\nUbiquitous Language(보편 언어) 를 사용한다. 도메인 모델링, UML 다이어그램 작성시 Ubiquitous Language(보편 언어) 를 사용한다.\n구성원 간 커뮤니케이션에 이를 활용해 텍스트 뿐만 아니라 소리내어 사용한다.\n커뮤니케이션, 문서만 아니라 코드에도 Ubiquitous Language(보편 언어) 를 녹여서 구현한다.\nUbiquitous Language(보편 언어) 를 지속적으로 발견하고 발전시킨다. 프로젝트 진행 중에 구성원 간에 용어에 대한 의견을 개진하고 지속적으로 갱신한다.\n용어를 처음 정한 형태로 계속 사용하지 않아도 된다. 이를 문서, 코드 등에도 갱신하고 최종적으로 모델링과 코드에 잘 담겨 있어야 한다.\n이를 위해 짧은 주기로 반복 개발/검증하는 에자일 개발 프로세스가 적합해 보인다.\n주기적으로 Ubiquitous Language(보편 언어) 를 갱신, 모델링 UML 다이어그램에 반영하고 코드에 녹여낸다. 그 후 피드백하는 과정을 반복한다.\nConclusion 프로젝트에서 언어의 중요성은 아무리 강조해도 모자라다.\n특히 MSA 를 적용한 개발팀은 언어 정의에 따라 업무 범위와 책임이 구별되는 만큼 중요하게 생각해야 한다.\n모두가 함께 쓰는 공통 언어 + 중의적 의미가 제거된 명확한 언어 + 번역이 필요없는 언어를 추구하자.\n 추가로, Ubiquitous Language(보편 언어) 는 Domain Driven Design 을 언급할 때 함께 설명된다.\nDomain Driven Design 도 같이 알아볼 필요가 있다.\n "
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/kubernetes_deploy_script/",
        "title": "K8S App deploy script",
        "tags": ["kubernetes", "docker", "deploy"],
        "description": "",
        "content": " 최근 Container 기술이 각광을 받고 있습니다. 그래서 저도 Kubernets 교육을 수강하고 있는데, Kubernets에 배포 한번 하기 참 힘드네요. Project build, Docker image build, Docker push, K8s deploy 총 4개의 과정을 거쳐야 애플리케이션 배포가 끝이 납니다. 도저히 이 과정을 참을 수 없어서 배포 스크립트를 만들었습니다. 사용방법은 아래 설명하였으니, 잘 활용하시기 바랍니다.\n사용법 1. deploy.sh 저장 페이지 하단 deploy.sh 파일을 개발중인 프로젝트 루트에 저장합니다.\n2. 설정 deploy.sh 파일의 상단 설정을 입력합니다.\n#!/bin/sh  #작성중인 Project 종류 #maven | node PROJECT_TYPE=\u0026#34;maven\u0026#34; #Maven build 과정 중 테스트 수행 여부 MAVEN_TEST_SKIP=true; NODE_NPM_INSTALL=false; #기존에 생성된 Docker Image, Kubernetes deploy, service 삭제여부 #기존에 생성된 Kubernetes deploy, service는 kubectl apply로 덮어쓰기 불가, 삭제 후 재생성 필요 DELETE_PREVIOUS_DOCKER_IMAGE_AND_KUBERNETES_DEPLOYMENT=true #Docker Repository 정보 #Dockerhub registry: docker.io #Harbor registry: harbor1.ghama.io #Docker hub는 DOCKER_REPOSITORY_PROJECT와 DOCKER_REPOSITORY_USER 동일 DOCKER_REPOSITORY_URL=\u0026#34;\u0026#34; DOCKER_REPOSITORY_PROJECT=\u0026#34;\u0026#34; DOCKER_REPOSITORY_USER=\u0026#34;\u0026#34; DOCKER_REPOSITORY_PASSWORD=\u0026#34;\u0026#34; DOCKER_IMAGE_NAME=\u0026#34;\u0026#34; #Kuberetes deployment, service/Dockerfile 경로 #파일이 존재하는 폴더까지의 경로 KUBERNETES_DEPLOYMENT_PATH=\u0026#34;./k8s/\u0026#34; DOCKER_FILE_PATH=\u0026#34;./\u0026#34; #Kuberetes Cluster 정보 #ICP의 경우 포탈에서 Configur client 코드를 복사하여 showlog \u0026#34;Login kubernetes\u0026#34; 밑에 덮어쓰기 KUBERNETES_CLUSTER_NAME=\u0026#34;\u0026#34; KUBERNETES_CLUSTER_URL=\u0026#34;\u0026#34; KUBERNETES_CLUSTER_USERNAME=\u0026#34;\u0026#34; KUBERNETES_CLUSTER_TOKEN=\u0026#34;\u0026#34; KUBERNETES_CLUSTER_NAMESPACE=\u0026#34;\u0026#34; KUBERNETES_CONTEXT_NAME=\u0026#34;\u0026#34; 3. deploy.sh 실행 deploy.sh를 실행합니다. 단, Windows docker toolbox를 사용할 경우 docker terminal에서 실행해야합니다.\n$ ./deploy.sh deploy.sh 코드 #!/bin/sh  #작성중인 Project 종류 #maven | node PROJECT_TYPE=\u0026#34;maven\u0026#34; #Maven build 과정 중 테스트 수행 여부 MAVEN_TEST_SKIP=true; NODE_NPM_INSTALL=false; #기존에 생성된 Docker Image, Kubernetes deploy, service 삭제여부 #기존에 생성된 Kubernetes deploy, service는 kubectl apply로 덮어쓰기 불가, 삭제 후 재생성 필요 DELETE_PREVIOUS_DOCKER_IMAGE_AND_KUBERNETES_DEPLOYMENT=true #Docker Repository 정보 #Dockerhub registry: docker.io #Harbor registry: harbor1.ghama.io #Docker hub는 DOCKER_REPOSITORY_PROJECT와 DOCKER_REPOSITORY_USER 동일 DOCKER_REPOSITORY_URL=\u0026#34;\u0026#34; DOCKER_REPOSITORY_PROJECT=\u0026#34;\u0026#34; DOCKER_REPOSITORY_USER=\u0026#34;\u0026#34; DOCKER_REPOSITORY_PASSWORD=\u0026#34;\u0026#34; DOCKER_IMAGE_NAME=\u0026#34;\u0026#34; #Kuberetes deployment, service/Dockerfile 경로 #파일이 존재하는 폴더까지의 경로 KUBERNETES_DEPLOYMENT_PATH=\u0026#34;./k8s/\u0026#34; DOCKER_FILE_PATH=\u0026#34;./\u0026#34; #Kuberetes Cluster 정보 #ICP의 경우 포탈에서 Configur client 코드를 복사하여 showlog \u0026#34;Login kubernetes\u0026#34; 밑에 덮어쓰기 KUBERNETES_CLUSTER_NAME=\u0026#34;\u0026#34; KUBERNETES_CLUSTER_URL=\u0026#34;\u0026#34; KUBERNETES_CLUSTER_USERNAME=\u0026#34;\u0026#34; KUBERNETES_CLUSTER_TOKEN=\u0026#34;\u0026#34; KUBERNETES_CLUSTER_NAMESPACE=\u0026#34;\u0026#34; KUBERNETES_CONTEXT_NAME=\u0026#34;\u0026#34; function showlog() { echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;--------------------------- $1---------------------------\u0026#34; } showlog \u0026#34;deploy.sh\u0026#34; showlog \u0026#34;Packaging\u0026#34; if [ \u0026#34;${PROJECT_TYPE}\u0026#34; == \u0026#34;maven\u0026#34; ]; then showlog \u0026#34;Maven packaging\u0026#34; if ./mvnw clean install -Dmaven.test.skip=${MAVEN_TEST_SKIP}; then echo \u0026#34;\u0026#34; else exit 1 fi else showlog \u0026#34;Node packaging\u0026#34; if ${NODE_NPM_INSTALL}; then npm install fi fi if ${DELETE_PREVIOUS_DOCKER_IMAGE_AND_KUBERNETES_DEPLOYMENT}; then showlog \u0026#34;Delete docker image, kubernetes deployment\u0026#34; kubectl delete -f ${KUBERNETES_DEPLOYMENT_PATH} docker rmi ${DOCKER_REPOSITORY_URL}/${DOCKER_REPOSITORY_PROJECT}/${DOCKER_IMAGE_NAME} docker rmi ${DOCKER_REPOSITORY_PROJECT}/${DOCKER_IMAGE_NAME} fi showlog \u0026#34;Build/Tag docker image\u0026#34; docker build ${DOCKER_FILE_PATH} -t ${DOCKER_REPOSITORY_URL}/${DOCKER_REPOSITORY_PROJECT}/${DOCKER_IMAGE_NAME} showlog \u0026#34;Login docker repository\u0026#34; docker login ${DOCKER_REPOSITORY_URL} -u ${DOCKER_REPOSITORY_USER} -p ${DOCKER_REPOSITORY_PASSWORD} showlog \u0026#34;Push Docker image\u0026#34; docker push ${DOCKER_REPOSITORY_URL}/${DOCKER_REPOSITORY_PROJECT}/${DOCKER_IMAGE_NAME} showlog \u0026#34;Login kubernetes\u0026#34; kubectl config set-cluster ${KUBERNETES_CLUSTER_NAME} --server=${KUBERNETES_CLUSTER_URL} --insecure-skip-tls-verify=true kubectl config set-context ${KUBERNETES_CONTEXT_NAME} --cluster=${KUBERNETES_CLUSTER_NAME} kubectl config set-credentials ${KUBERNETES_CLUSTER_USERNAME} --token=${KUBERNETES_CLUSTER_TOKEN} kubectl config set-context ${KUBERNETES_CONTEXT_NAME} --user=${KUBERNETES_CLUSTER_USERNAME} --namespace=${KUBERNETES_CLUSTER_NAMESPACE} kubectl config use-context ${KUBERNETES_CONTEXT_NAME} showlog \u0026#34;Create kuberetes deployment\u0026#34; kubectl apply -f ${KUBERNETES_DEPLOYMENT_PATH} showlog \u0026#34;Pod infomation\u0026#34; kubectl get pod showlog \u0026#34;Service infomation\u0026#34; kubectl get service"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/kubernetes/hpa/",
        "title": "[Kubernetes 활용(3/8)] HPA(오토스케일링)",
        "tags": ["kubernetes", "container", "container orchestration", "hpa", "horizontal pod autoscaler", "autoscale", "scale out"],
        "description": "",
        "content": " 지난 챕터에서는 이미 배포되어 있는 애플리케이션을 무중단으로 업데이트하는 방법에 대해 보았습니다. 이번에는 이어서 애플리케이션을 자동으로 Scale-out 할수 있는 Horizontal Pod Autoscaler 기능을 적용해보도록 하겠습니다.\nHorizontal Pod Autoscaler 란 Horizontal Pod Autoscaler는 지정된 CPU 사용률을 기반으로 Replication Controller, Deployment 또는 Replica Set의 Pod 수를 자동으로 조정합니다. Kubernetes에서는 CPU 자원에 대한 사용량을 다음과 같은 식으로 계산하여 Pod을 자동 Scale-out 할 수 있습니다.\n[CPU example] TargetNumOfPods = ceil(sum(CurrentPodsCPUUtilization) / Target) 주기적으로 Pod의 자원 사용을 체크하고, 특정 시간의 여유를 두고 downscale/upscale이 이루어지는데, 이는 kube-controller-manager가 담당합니다. 아래와 같은 설정은 HPA 개별적으로 적용할 수 있는 부분은 아니고 kube-controller-manager에 적용이 된다면 클러스터 환경에 전체적으로 적용이 됩니다. Minikube의 경우 kube-controller-manager를 위한 설정 파일이 존재합니다. (/etc/kubernetes/manifests/kube-controller-manager.yaml)\nkube-controller-manager는 Kubernetes 내 daemon 중 하나이고, default로 설정된 사항은 다음과 같습니다.\n \u0026ndash;horizontal-pod-autoscaler-downscale-delay : 마지막 다운스케일 후 n시간 경과 후 다음 다운스케일 가능 (디폴트 5분)\n \u0026ndash;horizontal-pod-autoscaler-upscale-delay : 마지막 업스케일 후 m시간 경과 후 다음 업스케일 가능 (디폴트 3분)\n \u0026ndash;horizontal-pod-autoscaler-sync-period : 필요한 Pod 수 계산을 위한 시간 간격 설정 가능 (디폴트 30초)\n  Horizontal Pod Autoscaler 적용하기 Horizontal Pod Autoscaler는 Kubernetes에서 축약어로 HPA 또는 hpa라고 부르기도 합니다. HPA object 생성을 위한 yaml 파일을 통해 HPA를 적용 할 수 있습니다.\nResources Requests 설정 및 애플리케이션 배포 HPA가 부하 발생에 따른 필요한 Pod의 수를 계산하기 위해 기준이 되는 Resources Requests 사용량을 설정해야 합니다. 최대 자원 사용량도 Deployment object yaml에 작성할 수 있습니다.\n Deployment를 통한 Resources Requests 설정 Deployment yaml 파일을 작성하고, containers 설정 부분에 resources.requests.cpu 부분에 CPU 자원을 200m(milli-cores) 또는 0.2로 요청합니다.\ngs-spring-boot-docker-deployment.yaml\napiVersion: apps/v1beta2 # for versions before 1.8.0 use apps/v1beta1 kind: Deployment metadata: name: gs-spring-boot-docker-deployment labels: app: gs-spring-boot-docker spec: replicas: 1 minReadySeconds: 10 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 25% selector: matchLabels: app: gs-spring-boot-docker template: metadata: labels: app: gs-spring-boot-docker spec: containers: - name: gs-spring-boot-docker image: dtlabs/gs-spring-boot-docker:1.0 imagePullPolicy: Always ports: - containerPort: 8080 resources: requests: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;200m\u0026#34; limits: memory: \u0026#34;1Gi\u0026#34; cpu: \u0026#34;500m\u0026#34;# kubectl apply 명령어로 Deployment object 설정 변경을 적용 합니다. $ kubectl apply -f gs-spring-boot-docker-deployment.yaml deployment \u0026#34;gs-spring-boot-docker-deployment\u0026#34; configured   Service 생성\ngs-spring-boot-docker-service.yaml\napiVersion: v1 kind: Service metadata: name: gs-spring-boot-docker-service spec: ports: - name: http port: 8081 targetPort: 8080 selector: app: gs-spring-boot-docker type: NodePort# Service 생성 # kubectl apply 명령어에서 -f 옵션을 통해 파일명이 gs-spring-boot-docker-service.yaml 임을 인자로 전달합니다. $ kubectl apply -f ./gs-spring-boot-docker-service.yaml service \u0026#34;gs-spring-boot-docker-service\u0026#34; created# Service object 조회를 통해 NodePort를 알아냅니다. # 8081:30993/TCP 에서 30993이 외부로 노출된 NodePort 입니다. $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gs-spring-boot-docker-service NodePort 10.99.197.147 \u0026lt;none\u0026gt; 8081:30993/TCP 12m kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 14d HorizontalPodAutoscaler 생성  [애플리케이션명]-hpa.yaml 파일 생성  테스트하기 위한 애플리케이션명은 \u0026lsquo;gs-spring-boot-docker\u0026rsquo; 입니다. 여기에 suffix로 \u0026lsquo;-hpa\u0026rsquo;를 추가 합니다. suffix는 선택사항으로 필수는 아닙니다.\n이 문서에서는 \u0026lsquo;gs-spring-boot-docker\u0026rsquo; 이름의 애플리케이션에 대한 \u0026lsquo;hpa\u0026rsquo; object 생성을 위한 yaml이라는 의미로 gs-spring-boot-docker-hpa.yaml 파일을 생성합니다.\ngs-spring-boot-docker-hpa.yaml\napiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: gs-spring-boot-docker-hpa spec: maxReplicas: 10 minReplicas: 1 scaleTargetRef: apiVersion: apps/v1beta2 kind: Deployment name: gs-spring-boot-docker-deployment targetCPUUtilizationPercentage: 50 line1 apiVersion API Server에서 관리되는 API 버전을 나타냅니다. 사용자가 입력한 apiVersion에 맞는 API를 사용하게 됩니다. Kubernetes API는 실험 단계의 API를 \u0026lsquo;beta\u0026rsquo; 형태로 지원하고, 지속 업데이트 하고 있습니다. 따라서 Kubernetes API 공식문서를 통해 현재 사용자의 Kubernetes 버전 별 호환 및 사용 가능한 API를 확인 후 사용해야 합니다. 현재 HPA의 경우 stable 버전(autoscaling/v1)에서는 CPU 리소스에 대해서만 오토스케일링을 지원하며, beta버전(autoscaling/v2beta1)에서 Memory 또는 사용자의 custom metrics 등의 리소스를 추가적으로 지원하고 있습니다.\nline2 kind 현재 yaml이 어떤 object를 생성하기 위함인지 kind에 설정합니다. kind: HorizontalPodAutoscaler 설정을 통해 현재 yaml로 HorizontalPodAutoscaler object를 생성하게 됩니다.\nline3 metadata HorizontalPodAutoscaler object 자신의 고유 정보를 입력합니다.\nline4 metadata.name HorizontalPodAutoscaler object에 대한 Unique-key를 입력합니다. 이 name 값을 통해 여러 object 중 해당 name을 갖는 object를 조회할 수 있습니다.\nline5 spec HorizontalPodAutoscaler object가 수행하는 내용에 대한 설정 입니다.\nline6 spec.maxReplicas 업스케일 시 생성할 수 있는 Pod의 최대수를 설정합니다.\nline7 spec.minReplicas 다운스케일 시 생성할 수 있는 Pod의 최소수를 설정합니다.\nline8 spec.scaleTargetRef HorizontalPodAutoscaler object가 동작할 대상에 대한 설정 입니다.\nline9 spec.scaleTargetRef.apiVersion HorizontalPodAutoscaler object는 Deployment object 또는 ReplicaController object 등에 맵핑시킬 수 있고, 맵핑한 object의 apiVersion입니다.\nline10 spec.scaleTargetRef.kind HorizontalPodAutoscaler와 맵핑 될 object의 종류를 설정합니다. 해당 예제는 Deployment object가 관리하는 Pod을 대상으로 업스케일/다운스케일 됩니다.\nline11 spec.scaleTargetRef.name HorizontalPodAutoscaler와 맵핑 될 Deployment object의 name을 설정 합니다.\nline12 spec.targetCPUUtilizationPercentage Pod에 설정된 CPU resource에 대한 request 설정 기준대비 CPU 사용 임계치를 퍼센트(%)로 설정하는데, 숫자만 입력 합니다. 해당 임계치를 넘어서면 업스케일이 동작하게 됩니다.\n  kubectl autoscale 명령어를 통한 생성 HPA object는 파일 기반으로 생성하는 것 외에 아래와 같은 명령어를 통해서도 생성이 가능합니다.\n # HorizontalPodAutoscaler 생성 $ kubectl autoscale deployment gs-spring-boot-docker-deployment --cpu-percent=50 --min=1 --max=10 deployment \u0026#34;gs-spring-boot-docker-deployment\u0026#34; autoscaled  그 밖의 HorizontalPodAutoscaler object yaml 파일 상세 작성 방법은 HorizontalPodAutoscaler API 공식문서 를 참고 바랍니다.\n HorizontalPodAutoscaler 생성 확인\n\u0026lsquo;0% / 50%\u0026lsquo;과 같이 사용량이 정상 표시 된 경우, Horizontal Pod Autoscaling의 준비가 완료된 것 입니다.\n# HorizontalPodAutoscaler 생성 $ kubectl apply -f ./gs-spring-boot-docker-hpa.yaml horizontalpodautoscaler \u0026#34;gs-spring-boot-docker-hpa\u0026#34; created # horizontalpodautoscaler 생성 확인 # \u0026#39;kubectl get horizontalpodautoscaler\u0026#39;와 \u0026#39;kubectl get hpa\u0026#39;는 동일한 의미 입니다. $ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE gs-spring-boot-docker-hpa Deployment/gs-spring-boot-docker-deployment 0% / 50% 1 10 1 36m 현재 사용량이 \u0026lt;unknown\u0026gt;인 경우가 발생할 수 있는데, 아래의 경우를 고려하여 사용량이 표시되도록 할 수 있습니다.\n 일정 시간이 지나면, 사용량이 표시될 수 있습니다. CPU resource에 대한 request 설정을 아직 하지 않았을 경우, \u0026lt;unknown\u0026gt;으로 표시 됩니다. CPU resource에 대한 request 설정을 하였고, 일정 시간이 지나도 \u0026lt;unknown\u0026gt;으로 표시된 경우, 부하를 발생시키면, 적당한 시간이 지나면 CPU 사용량이 표시됩니다.  $ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE gs-spring-boot-docker-hpa Deployment/gs-spring-boot-docker-deployment \u0026lt;unknown\u0026gt; / 50% 1 10 1 36m Horizontal Pod Autoscaler 테스트  사용량 모니터링 시작  kubectl get hpa 명령어에서 옵션으로 \u0026lsquo;-w \u0026lsquo;을 사용하게 되면, 실시간으로 갱신되는 정보를 지속 조회할 수 있습니다. 지속적인 모니터링을 위해 별개의 커맨드 창에서 수행하시기 바랍니다.\n$ kubectl get hpa -w NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE gs-spring-boot-docker-hpa Deployment/gs-spring-boot-docker-deployment 27% / 50% 1 10 1 2d   부하테스트\nURL 요청 테스트를 위해 wget 커맨드를 통해 접속 테스트의 반복 요청을 합니다.\nURL은 Node의 IP와 Service가 NodePort 방식을 통해 외부로 노출된 Port를 사용합니다.\n$ while true; do wget -q -O- http://169.56.109.58:30993; done Hello Docker World Hello Docker World Hello Docker World Hello Docker World Hello Docker World ...  사용량에 따른 Pod Autoscaling 확인  CPU 사용량이 27%에서 106%으로 설정한 기준값인 50%를 넘게 되면서 Pod REPLICAS가 1 -\u0026gt; 3 으로 증가되고, 부하가 분산되었습니다.\n시간이 지남에 따라 CPU는 24%로 안정되는 것 또한 확인할 수 있습니다.\n$ kubectl get hpa -w NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE gs-spring-boot-docker-hpa Deployment/gs-spring-boot-docker-deployment 27% / 50% 1 10 1 2d gs-spring-boot-docker-hpa Deployment/gs-spring-boot-docker-deployment 27% / 50% 1 10 1 2d gs-spring-boot-docker-hpa Deployment/gs-spring-boot-docker-deployment 106% / 50% 1 10 1 2d gs-spring-boot-docker-hpa Deployment/gs-spring-boot-docker-deployment 106% / 50% 1 10 3 2 gs-spring-boot-docker-hpa Deployment/gs-spring-boot-docker-deployment 35% / 50% 1 10 3 2d gs-spring-boot-docker-hpa Deployment/gs-spring-boot-docker-deployment 35% / 50% 1 10 3 2d gs-spring-boot-docker-hpa Deployment/gs-spring-boot-docker-deployment 24% / 50% 1 10 3 2d gs-spring-boot-docker-hpa Deployment/gs-spring-boot-docker-deployment 24% / 50% 1 10 3 2d  "
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/kubernetes/rolling-update/",
        "title": "[Kubernetes 활용(2/8)] Rolling Update(무중단 배포)",
        "tags": ["kubernetes", "container", "container orchestration", "rolling update"],
        "description": "",
        "content": " 지난 챕터에서는 Kubernetes 환경에서 애플리케이션을 배포하고 접속하는 방법을 알아보았습니다. 그렇다면 이미 배포되어 있는 애플리케이션을 업데이트할 때 중단 없이 처리할 수 있을까요?? 네. 가능합니다! Kubernetes에서는 중단 없이 애플리케이션을 배포할 수 있도록 Rolling Update라는 기능을 지원하고 있습니다. 지난 챕터에서 간단히 나오긴 했지만 다시 한번 자세히 알아봅시다.\nRolling Update 란? 서비스 중단 없이 애플리케이션을 업데이트 하기 위해서, Kubernetes에서는 rolling update라는 기능을 지원합니다. 이 기능을 통해서 전체 Pod을 일시에 중단/업데이트 하는 것이 아니라, 한번에 n개씩 Pod을 순차적으로 업데이트할 수 있습니다. 이를 통해 서비스 중단 현상 없이 애플리케이션 버전 업데이트 및 롤백을 할 수 있습니다.\nkubectl rolling-update 커맨드의 사용은 Replication Controllers를 이용해 애플리케이션을 배포한 경우에만 사용 합니다. 최신 버전의 Kubernetes에서는 Deployment를 이용한 애플리케이션 배포를 권장 합니다.\nRolling Update 하기 애플리케이션 배포 우선 Rolling Update 테스트를 위한 애플리케이션 버전 1.0을 아래와 같이 배포 합니다. (Deployment, Service 생성 필요)\n아래의 설정 내용 중, Image 설정이 \u0026lsquo;dtlabs/gs-spring-boot-docker:1.0\u0026rsquo; 임을 통해 Image 버전(1.0)을 확인 합니다.\n Deployment 생성\ngs-spring-boot-docker-deployment.yaml\napiVersion: apps/v1beta1 # for versions before 1.8.0 use apps/v1beta1 kind: Deployment metadata: name: gs-spring-boot-docker-deployment labels: app: gs-spring-boot-docker spec: replicas: 3 minReadySeconds: 10 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 selector: matchLabels: app: gs-spring-boot-docker template: metadata: labels: app: gs-spring-boot-docker spec: containers: - name: gs-spring-boot-docker image: dtlabs/gs-spring-boot-docker:1.0 imagePullPolicy: Always ports: - containerPort: 8080 line8 spec.replicas Pod의 복제 수를 나타냅니다. rolling update 테스트를 위해 초기 설정을 Pod 3개 복제로 설정하였습니다.\nline9 spec.minReadySeconds Pod이 Ready 단계가 된 후, Available 단계가 될 때 까지의 시간 차이를 의미합니다. 테스트 결과, minReadySeconds를 설정하지 않으면, Ready에서 곧 바로 Available이 되고, 몇 초간 순단 현상이 있음이 확인되었습니다. 반면, minReadySeconds를 통해 10초 정도 여유 시간을 주었을 때, 순단이 최소화 되는 것을 확인 하였습니다. Pod의 컨테이너가 초기화 되는 시간을 고려하여 적절한 시간 minReadySeconds를 설정하는 것을 권장합니다.\nline10 spec.strategy RollingUpdate에 대한 상세 설정을 합니다.\nline11 spec.strategy.type \u0026ldquo;Recreate\u0026rdquo; or \u0026ldquo;RollingUpdate\u0026rdquo;를 설정 가능 합니다. 기본값은 \u0026ldquo;RollingUpdate\u0026rdquo; 입니다. Recreate의 경우 Pod가 삭제된 후 재생성 됩니다.\nline12 spec.strategy.rollingUpdate spec.strategy.type에서 \u0026ldquo;RollingUpdate\u0026rdquo;를 설정한 경우, RollingUpdate에 대한 상세 설정을 합니다.\nline13 spec.strategy.rollingUpdate.maxSurge rolling update 중 정해진 Pod 수 이상으로 만들 수 있는 Pod의 최대 개수입니다. 기본값은 25% 입니다.\nline14 spec.strategy.rollingUpdate.maxUnavailable rolling update 중 unavailable 상태인 Pod의 최대 개수를 설정 합니다. rollgin update 중 사용할 수 없는 Pod의 최대 개수입니다. 값은 0보다 큰 정수를 통해 Pod의 절대 개수 설정이 가능하고, \u0026ldquo;25%\u0026ldquo;와 같이 percentage 표현 또한 가능 합니다. maxUnavailable에서 percentage 계산은 rounding down(내림) 방식이며 기본값은 25% 입니다. maxSurge와 maxUnavailable 값이 동시에 0이 될 수는 없습니다.\n replica: 3인 경우, 25%는 0.75개 이지만, 최소값이 1이기 때문에 maxUnavailable은 1개로 계산 됩니다.\n replica: 9인 경우, 25%는 2.25개 이지만, rounding down(내림) 하여 maxUnavailable은 2개로 계산 됩니다.\n  # Deployment 생성을 통해 애플리케이션을 배포 합니다. # kubectl create 명령어에서 -f 옵션을 통해 파일명이 gs-spring-boot-docker-deployment.yaml 임을 인자로 전달합니다. $ kubectl create --save-config -f ./gs-spring-boot-docker-deployment.yaml deployment \u0026#34;gs-spring-boot-docker-deployment\u0026#34; created Service 생성\ngs-spring-boot-docker-service.yaml\napiVersion: v1 kind: Service metadata: name: gs-spring-boot-docker-service spec: ports: - name: http port: 8081 targetPort: 8080 selector: app: gs-spring-boot-docker type: NodePort# Service 생성 # kubectl create 명령어에서 -f 옵션을 통해 파일명이 gs-spring-boot-docker-service.yaml 임을 인자로 전달합니다. $ kubectl create --save-config -f ./gs-spring-boot-docker-service.yaml service \u0026#34;gs-spring-boot-docker-service\u0026#34; created  Deployment 및 Service 생성 관련 상세 방법은 바로 이전 챕터를 확인하시기 바랍니다.\nRolling Update 및 확인 Deployment를 이용한 애플리케이션 배포 방식에서는 rolling update를 위해 kubectl set image 명령어를 사용합니다.\n Rolling Update 진행 모니터링\nkubectl get pod 명령어에서 옵션으로 \u0026lsquo;-w \u0026lsquo;을 사용하게 되면, 실시간으로 갱신되는 정보를 지속 조회할 수 있습니다. 별개의 커맨드창을 사용하여 버전 업데이트 수행 전부터 실시간 갱신 정보의 모니터링을 시작합니다. Deployment를 통해 \u0026lsquo;replica: 3\u0026rsquo; 을 설정 하였기 때문에 3개의 Pod이 Running 되는 것을 확인할 수 있습니다.\n$ kubectl get pod -w NAME READY STATUS RESTARTS AGE gs-spring-boot-docker-deployment-2374595156-47dz4 1/1 Running 0 1m gs-spring-boot-docker-deployment-2374595156-5fvft 1/1 Running 0 59s gs-spring-boot-docker-deployment-2374595156-zb0nt 1/1 Running 0 1m  kubectl set image 명령어를 통한 Image 2.0 버전 업데이트 수행  $ kubectl set image deployment/gs-spring-boot-docker-deployment gs-spring-boot-docker=dtlabs/gs-spring-boot-docker:2.0 deployment \u0026#34;gs-spring-boot-docker-deployment\u0026#34; image updated Rolling Update 확인\n$ kubectl get pod -w NAME READY STATUS RESTARTS AGE gs-spring-boot-docker-deployment-2374595156-5fvft 1/1 Running 0 59s \u0026lt;- Old Pod #1 gs-spring-boot-docker-deployment-2374595156-47dz4 1/1 Running 0 1m \u0026lt;- Old Pod #2 gs-spring-boot-docker-deployment-2374595156-zb0nt 1/1 Running 0 1m \u0026lt;- Old Pod #3 gs-spring-boot-docker-deployment-313620219-m0hgq 0/1 Pending 0 0s \u0026lt;- New Pod #1 생성 시작 gs-spring-boot-docker-deployment-313620219-m0hgq 0/1 Pending 0 0s gs-spring-boot-docker-deployment-313620219-m0hgq 0/1 ContainerCreating 0 0s gs-spring-boot-docker-deployment-313620219-m0hgq 1/1 Running 0 1s \u0026lt;- New Pod #1 생성 완료 gs-spring-boot-docker-deployment-2374595156-5fvft 1/1 Terminating 0 1m \u0026lt;- Old Pod #1 Terminating 시작 gs-spring-boot-docker-deployment-313620219-78q8s 0/1 Pending 0 0s \u0026lt;- New Pod #2 생성 시작 gs-spring-boot-docker-deployment-313620219-78q8s 0/1 Pending 0 0s gs-spring-boot-docker-deployment-313620219-78q8s 0/1 ContainerCreating 0 0s gs-spring-boot-docker-deployment-313620219-78q8s 1/1 Running 0 0s \u0026lt;- New Pod #2 생성 완료 gs-spring-boot-docker-deployment-2374595156-5fvft 0/1 Terminating 0 1m \u0026lt;- Old Pod #1 Terminating 완료 gs-spring-boot-docker-deployment-2374595156-47dz4 1/1 Terminating 0 1m \u0026lt;- Old Pod #2 Terminating 시작 gs-spring-boot-docker-deployment-313620219-q3js4 0/1 Pending 0 0s \u0026lt;- New Pod #3 생성 시작 gs-spring-boot-docker-deployment-313620219-q3js4 0/1 Pending 0 0s gs-spring-boot-docker-deployment-313620219-q3js4 0/1 ContainerCreating 0 0s gs-spring-boot-docker-deployment-313620219-q3js4 1/1 Running 0 0s \u0026lt;- New Pod #3 생성 완료 gs-spring-boot-docker-deployment-2374595156-5fvft 0/1 Terminating 0 1m \u0026lt;- Old Pod #1 Terminating 완료 gs-spring-boot-docker-deployment-2374595156-47dz4 1/1 Terminating 0 1m \u0026lt;- Old Pod #2Terminating 완료 gs-spring-boot-docker-deployment-2374595156-zb0nt 0/1 Terminating 0 1m \u0026lt;- Old Pod #3 Terminating 시작 gs-spring-boot-docker-deployment-2374595156-zb0nt 0/1 Terminating 0 1m \u0026lt;- Old Pod #3 Terminating 완료  "
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/12-factor/",
        "title": "12-Factor",
        "tags": ["cloud", "12 factors"],
        "description": "",
        "content": " 최근 소프트웨어를 Cloud 환경에 배포하여 서비스 형태로 제공하는 SaaS(Software As A Service)가 보편화되고 있습니다. 저희 팀에서도 작년부터 레거시 시스템을 Cloud로 전환하기 위한 컨설팅을 준비하고 있는데요, 그 과정에서 스터디한 12-Factor를 알아보도록 하겠습니다. 12-Factor는 Heroku 플랫폼을 통해 방대한 앱의 개발, 운영, 확장 등을 관찰한 많은 사람들이 고안해낸 SaaS 개발 방법론 입니다. 12-Factor를 준수할 경우 아래 SaaS의 특징을 갖출 수 있습니다. 즉 12-Factor는 애플리케이션이 Cloud 환경에서 올바르게 게 동작하기 위해서 지켜야 하는 12가지 규칙을 말합니다.\nSaaS의 특징\n 설정 자동화를 위한 절차(declarative)를 체계화하여 새로운 개발자가 프로젝트에 참여하는데 드는 시간과 비용을 최소화합니다 OS에 따라 달라지는 부분을 명확히 하고, 실행 환경 사이의 이식성을 극대화합니다 최근 등장한 클라우드 플랫폼 배포에 적합하고, 서버와 시스템의 관리가 필요 없게 됩니다 개발 환경과 운영 환경의 차이를 최소화하고 민첩성을 극대화하기 위해 지속적인 배포가 가능합니다 툴, 아키텍처, 개발 방식을 크게 바꾸지 않고 확장(scale up) 할 수 있습니다  12-Factor는 이름에서 보이듯 12가지 항목으로 구성되어있습니다, 이어지는 내용에서는 12가지 항목을 설명하겠습니다. 보다 자세한 내용은 12facot.net 문서를 참고하세요.\n1. CodeBase 애플리케이션의 1개의 코드 베이스(Git, SVN)를 통해 관리되어야 하며, 동일한 코드로 운영/개발에 배포하여야 한다.\n 애플리케이션은 1개의 코드 베이스를 가진다 애플리케이션은 1개의 코드 베이스를 통해 운영/개발용으로 배포한다 CodeBase 항목은 이어지는 타 항목을 준수기 위해 기본적으로 준수해야 하는 항목이다 CodeBase 항목은 SVN, Git과 같은 코드 관리 시스템 사용으로 준수할 수 있다   중요도: Non-negotiable\n 2. Dependencies 애플리케이션의 모든 종속성을 명시적으로 선언하여 사용한다. 애플리케이션이 필요로 하는 라이브러리를 dependency manifest 파일에 (Gemfile, POM 등) 명시적으로 선언하여 사용한다. SaaS는 상황에 따라 다양한 환경(window, mac, linux)에 배포될 수 있다. Gemfile, pom 등을 사용하여 다양한 환경에서도 SaaS가 정상 동작할 수 있음을 보장할 수 있다. 예를 들어 curl 등을 사용하여 lib를 사용할 경우 os에 따라 오동작 할 수 있다.\n Dependencies 항목 준수 방법  Dependencies 항목은 Gemfile, POM 등을 사용하여 준수할 수 있다. 필요한 모든 라이브러리와 버전을 리스트 업하고, 배포할 시 빌드 명령어를 실행. mvn build 또는 build install(Ruby) 등    중요도: High\n Spring boot의 경우 embedded runtime, external runtime에 따라 dependency 를 명시적으로 선언할 수 있다.\n 배포방식 : jar or war dependency : spring-boot-starter-tomcat을 사용할 것인지 여부 결정 sample code : http://www.slideshare.net/SpringCentral/12-factor-cloud-native-apps-for-spring-developers 19~20 page  3. Config 모든 설정 정보는 코드로부터 분리된 공간에 저장되어야 하고, 런타임에서 코드에 의해 읽혀야 한다. SaaS는 동일한 코드를 여러 환경(운영/개발)에 배포한다. 이를 위해 환경마다 달리 사용되어야 하는 정보를 분리한다.\n 분리되어야 할 정보  데이터베이스나 다른 백업 서비스를 처리하는 리소스 외부 리소스(S3, Twitter 등)의 인증 정보 각 배포마다 달라지는 값(cononical hostname..) dev,test,stage,prod의 배포 단계마다 다를 수 있는 어떤 값들  설정정보를 저장하면 안되는 곳  code properties file build : one build, many deploy니까 app server(jndi database 같은 정보)  Config 항목 준수 방법  배포 환경(개발/운영)용 설정파일을 작성 Spring Cloud Config 사용    중요도: Medium\n 4. Backing services 백엔드 서비스를 연결된 리소스로 취급한다. SaaS의 리소스는 자유롭게 배포에 연결되거나 분리할 수 있고, 코드 수정 없이 전환이 가능해야 한다. 예를 들어 DB를 MySQL에서 Amazon RDS로 전환할 때 코드 수정 없이 가능해야 한다.\n 백엔드 서비스  네트워크을 통해 이용하는 모든 서비스 DB, Cache, SMTP, Messaging/Queueing system  준수 방법  Config에 백엔드 서비스의 URL이나 Locator를 저장하고, 코드에서는 설정을 읽어서 사용 Factor3. Config 기능 사용  참고  Spring sample https://github.com/joshlong/12factor-backing-services http://www.slideshare.net/SpringCentral/12-factor-cloud-native-apps-for-spring-developers 30~32 page    중요도: High\n 5. Build, Release, Run 코드 베이스는 build \u0026gt; release \u0026gt; run의 단계를 거쳐 배포로 변환되며, 각 단계는 엄격하게 분리되어야 한다.\n 준수 방법  빌드 단계는 개발자, 배포 단계는 배포툴, 실행 단계는 프로세스 매니저에 의해 시작  참고  Factor5-Design,Build,Release,Run(12 page): http://www.slideshare.net/SpringCentral/12-factor-cloud-native-apps-for-spring-developers    중요도: Conceptual\n 6. Process 실행 환경에서 앱은 하나 이상의 프로세스로 실행되며, 각 프로세스는 stateless로 아무것도 공유하지 않아야 한다. SaaS는 여러 개의 인스터스로 배포될 수 있다. 각 인스턴스는 메모리 파일 등을 공유할 수 없으며, 인스턴스가 재실행 될 때 local file, session과 같은 상태 정보는 모두 초기화된다.\n 준수 방법  메모리/파일을 사용할 경우 단일 트랜잭션 내에서 읽고, 쓰고 등의 모든 작업을 처리. 세션 상태 데이터의 경우 Memcached 또는 Redis와 같은 데이터 저장소에 저장    중요도: High\n 7. Port Binding 배포된 SaaS 애플리케이션을 타 애플리케이션에서 접근할 수 있도록 포트 바인딩을 통해 서비스를 공개한다. 앱도 백엔드 서비스처럼 URL을 제공하고, 라우팅 레이어가 외부에 공개된 호스트 명의로 들어온 요청을 포트에 바인딩 된 웹 프로세스에 전달한다. Factor4. Backing services의 확장으로, 포트 바인딩에 의해 공개되는 서비스는 HTTP뿐만 아니라 ejabberd나 Redis 같은 모든 종류의 서버 소프트웨어가 해당된다.\n 준수 방법  보통 dependency에 웹서버 라이브러리를 추가해서 구현  참고  Spring Cloud Netflix http://cloud.spring.io/spring-cloud-netflix/ https://spring.io/blog/2015/01/20/microservice-registration-and-discovery-with-spring-cloud-and-netflix-s-eureka    중요도: Medium\n 8. Concurrency 앱은 수평으로 확장할 수 있어야 하고, Factor6. Processes에 의해 동시성을 높일 수 있다.\n 준수 방법  모든 일을 처리하는 하나의 프로세스 대신 기능별로 분리된 프로세스 실행(micro service) 프로세스가 데몬형태가 아니어야 함 OS 프로세스 관리자/분산 프로세스 매니저/Foreman 같은 툴에 의존해서 output stream을 관리하고, 충돌이 발생한 프로세스에 대응, 재시작과 종료를 처리해야 함    중요도: Low\n 9. Disposability 프로세스는 shut down 신호를 받았을 때 graceful shut down 해야 한다. SaaS는 요청에 의해 Sacle up/down이 빈번히 발생한다. Disposability를 준수함으로써 이러한 사용에 안정성을 얻을 수 있다. 예를 들어 Sacle down 시점에 graceful shut down 이 아니라면 db lock 등으로 인해 타 프로세스에 영향을 주게 된다.\n 중요도: Mediu\n 10. dev/prod parity development, staging, production 환경을 최대한 비슷하게 유지한다. SaaS 애플리케이션은 개발 환경과 production 환경의 차이를 작게 유지하여 지속적인 배포가 가능하도록 디자인되어야 한다.\n 준수 방법  시간의 차이를 최소화: 개발자가 작성한 코드는 몇 시간 또는 몇 분 후에 배포되어야 함 담당자의 차이를 최소화: 코드를 작성한 개발자들이 배포와 production에서의 모니터링에 깊게 관여함 툴의 차이를 최소화: 개발과 production 환경을 최대한 비슷하게 유지    중요도: Medium\n 11. Logs 로그를 이벤트 스트림으로 취급하여 로그를 로컬에 저장하지 않는다. SaaS는 언제든지 인스턴스가 생성/삭제될 수 있다. 이때 로컬에 저장된 로그는 초기화되기 때문에 로그는 스트림으로 취급하여 별도의 저장소에 보관해야 한다.\n 준수 방법  스트림을 버퍼링 없이 stdout, stderr 로 출력함 별도의 로그 저장소를 사용    중요도: Low\n 12. Admin Process admin/maintenance 작업을 일회성 프로세스로 실행해야 한다.\n 일회성 프로세스  데이터베이스 마이그레이션 일회성 스크립트 실행  준수 방법  관리/유지보수 작업은 release와 함께 실행 release와 동일한 환경에서 실행하고, 같은 코드 베이스와 config를 사용 admin 코드는 동기화 문제를 피하기 위해 애플리케이션 코드와 함께 배포    중요도: High\n 참고자료  The Twenve-Factor App  http://12factor.net/12-Factor  Apps in Plain English  http://www.clearlytech.com/2014/01/04/12-factor-apps-plain-english/  12 Factor(Cloud Native) Apps for Spring Developer  http://www.slideshare.net/SpringCentral/12-factor-cloud-native-apps-for-spring-developers  Building and running 12 factor microservies on docker  https://www.packtpub.com/books/content/building-and-running-12-factor-microservices-docker-part-1 https://www.packtpub.com/books/content/building-and-running-12-factor-microservices-docker-part-2  The 12 Factor PHP App  http://slashnode.com/the-12-factor-php-app-part-1/ http://slashnode.com/the-12-factor-php-app-part-2/   "
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/kubernetes/getting-start/",
        "title": "[Kubernetes 활용(1/8)] 시작하기",
        "tags": ["kubernetes", "container", "container orchestration"],
        "description": "",
        "content": " 언젠가부터 클라우드 열풍이 불어 닥치고 있습니다. 기술의 변화를 보면 IaaS보다는 PaaS나 SaaS를 선호하고, VM에서 직접 컨트롤 하기 보다는 컨테이너, 서버리스 형태의 기술들이 뜨고 있습니다. 저도 그러한 이유로 작년부터 조금씩 회사에서 Kubernetes 스터디를 하고 있네요. 이번 첫번째 챕터에서는 Kubernetes를 처음 접하는 사용자를 위해 Kubernetes 환경에서 애플리케이션을 배포/접속하고 관리하는 기본적인 방법에 대해 보도록 하겠습니다.\n시작하기 전에 애플리케이션 배포를 진행하기 전에 이해가 필요한 기본 개념입니다.\nKubernetes  Kubernetes는 2014년 Google이 시작한 프로젝트로, 애플리케이션 컨테이너의 배포, 스케일링, 오퍼레이팅을 자동화 해주는 오픈 소스 플랫폼입니다.\n Cluster  Kubernetes 내 추상 개념으로, 컨테이너 애플리케이션을 배포, 스케일링, 오퍼레이팅 할 수 있는 단위 환경이 곧 Cluster 입니다. Cluster 는 Master와 Node로 구성되어 있습니다.\n Master\n Cluster 전체를 관리하는 주체 노드의 글로벌 이벤트를 감지하고 응답하는 등 의사결정을 수행 1대 이상으로 구성할 수 있고 일반적인 운영 환경에서는 단일 마스터가 아닌 이중화 또는 삼중화 형태로 구성할 수 있음 Kube API Server, Controller Manager, Scheduler, etcd 등 여러 모듈로 구성  Node\n VM 또는 실제 머신을 의미 kubelet이라는 에이전트를 통해 마스터와 통신 실제 컨테이너인 Pod가 생성되는 곳    Pod object  Kubernetes 에서 관리되는 가장 작은 단위입니다.\n Containerized app이 배포되는 컴포넌트 Strongly coupled 한 관계로 Life-cycle이 일치하는 경우는 복수개의 컨테이너로 구성 MSA에서는 보통 1개 Container당 1개의 App Scaling, Replication 의 단위   Deployment object  애플리케이션의 배포/삭제, scale out 의 역할을 합니다.Deployment를 생성하면 Deployment가 Pod과 ReplicaSets를 함께 생성합니다. Pod에 containerized 애플리케이션들이 포함되고, Pod이 생성되면서 애플리케이션이 배포되는 원리 입니다. ReplicaSets는 replica 수를 지속 모니터링하고, 유지시켜줍니다. 만약 Pod이 삭제되어 replica 수와 맞지 않게 되면 ReplicaSets가 동작하여 지정된 replica 수가 되도록 Pod을 생성합니다.\n Service object  Pod의 논리적 집합과 액세스 정책을 정의하는 추상화된 개념입니다. Service object는 software service (예를들면, mysql)에 대해 이름이 부여된 추상 개념입니다. Service object는 클러스터 내부에서 접근 가능한 port와 외부에서 접근 가능한 nodePort를 가집니다. 이 포트를 통해 요청이 왔을 경우, Service object에 설정된 selector를 이용하여 요청이 전달될 Pod을 찾는데, Service object의 selector 값에 해당하는 label을 가진 Pod 그룹을 찾고 load balancing (기본은 random) 설정에 따라 특정 Pod에 요청이 전달 됩니다. 같은 Cluster 내에서 Pod이 어떤 Node 내 생성되었는지와 상관 없이 Service는 selector/label 방식으로 Pod을 찾을 수 있습니다. Pod은 생성/삭제되기 쉽습니다. 만약 Pod이 죽게 되면, Replica Sets가 동적으로 Pod을 생성해냅니다. Pod은 Node 내부에서 고유한 IP를 가지는데, Pod이 쉽게 생성/삭제 되는 특성상, 이 IP를 통해 접속하기에 무리가 있습니다. Kubernetes에서는 이에 대한 안정적인 endpoint를 제공하기 위해 Service object를 생성 및 활용 합니다.\nKubernetes의 Service Object는 서비스 디스커버리와 로드밸런싱 기능을 제공하기 때문에 마이크로서비스 아키텍처의 구성 요소 중 하나인 Spring Netflix Eureka를 대신할 수도 있습니다. \n 애플리케이션 준비 Kubernetes에서 애플리케이션 배포를 위해 Docker Image를 사용합니다. 여기서는 제가 Docker Hub에 미리 Push 해놓은 샘플 Image를 활용해 봅시다.\n Image Name : dtlabs/gs-spring-boot-docker:1.0 Spring Boot 공식 가이드에서 제공하는 GitHub 샘플소스를 통해 빌드된 Image 입니다. Docker를 이용하여 애플리케이션을 Build 및 Docker Hub에 Push 하는 방법은 Docker 시작하기 가이드를 참고 하시기 바랍니다.   애플리케이션 배포 Kubernetes에서 애플리케이션 배포를 위해 Deployment object를 사용합니다. 그리고, 배포된 애플리케이션 서비스 디스커버리 및 접속을 위해 Service object를 사용합니다. 먼저 Deployment object를 생성해볼까요?\nDeployment yaml 파일 생성 및 작성  [애플리케이션명]-deployment.yaml 파일 생성\n테스트하기 위한 애플리케이션명은 \u0026lsquo;gs-spring-boot-docker\u0026rsquo; 입니다. 여기에 suffix로 \u0026lsquo;-deployment\u0026rsquo;를 추가 하였습니다. suffix는 선택사항으로 필수는 아닙니다. \u0026lsquo;gs-spring-boot-docker\u0026rsquo; 이름의 애플리케이션에 대한 \u0026lsquo;deployment\u0026rsquo; object 생성을 위한 yaml이라는 의미로 gs-spring-boot-docker-deployment.yaml 파일을 생성하였습니다.\nyaml 작성 방법을 참고하여 gs-spring-boot-docker-deployment.yaml 파일에 아래의 내용을 작성 합니다.\napiVersion: apps/v1beta2 # for versions before 1.8.0 use apps/v1beta1 kind: Deployment metadata: name: gs-spring-boot-docker-deployment labels: app: gs-spring-boot-docker spec: replicas: 1 selector: matchLabels: app: gs-spring-boot-docker template: metadata: labels: app: gs-spring-boot-docker spec: containers: - name: gs-spring-boot-docker image: dtlabs/gs-spring-boot-docker:1.0 ports: - containerPort: 8080 imagePullPolicy: Always resources: requests: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;200m\u0026#34; limits: memory: \u0026#34;1Gi\u0026#34; cpu: \u0026#34;500m\u0026#34; line1 apiVersion API Server에서 관리되는 API 버전을 나타냅니다. 사용자가 입력한 apiVersion에 맞는 API를 사용하게 됩니다. Kubernetes API는 실험 단계의 API를 \u0026lsquo;beta\u0026rsquo; 형태로 지원하고, 지속 업데이트 하고 있습니다. 따라서 Kubernetes API 공식가이드를 통해 현재 사용자의 Kubernetes 버전 별 호환 및 사용 가능한 API를 확인 후 사용해야 합니다.\nline2 kind 현재 yaml이 어떤 object를 생성하기 위함인지 kind에 설정합니다. kind: Deployment 설정을 통해 현재 yaml로 Deployment object를 생성하게 됩니다.\nline3 metadata Deployment object 자신의 고유 정보를 입력합니다.\nline4 metadata.name Deployment object에 대한 Unique-key를 입력합니다. 이 name 값을 통해 여러 object 중 해당 name을 갖는 object를 조회할 수 있습니다.\nline5 metadata.labels Deployment object에 대한 label을 설정하는데, 복수개의 label 설정이 가능 합니다. object들의 그룹화가 가능하고, 같은 label을 가진 object들을 같은 그룹으로 식별 됩니다.\nline7 spec Deployment object가 수행하는 내용에 대한 설정 입니다.\nline8 spec.replicas Deployment object가 ReplicaSets object를 통해 복제해야 할 Pod의 개수를 설정합니다.\nline10 spec.selector.matchLabels Deployment object가 관리해야할 Pod이 어떤 것인지 찾기 위해 selector 정보로 Pod의 label 정보를 비교하고 매칭되는 label을 갖는 Pod들만 관리 대상으로 생각합니다.\nline11 spec.template Deployment object가 생성할 Pod 관련 설정 입니다.\nline12 spec.template.metadata.labels Pod의 label을 설정하는데, 복수개의 label 설정이 가능 합니다. object들의 그룹화가 가능하고, 같은 label을 가진 object들을 같은 그룹으로 식별 됩니다. 다른 애플리케이션을 위한 Pod과 label이 겹치지 않도록 유의 합니다.\nline16 spec.template.spec Deployment object가 생성할 Pod에 대한 설정 입니다.\nline17 spec.template.spec.containers Deployment object가 생성할 Pod이 관리하는 container들의 설정입니다.\nline18 spec.template.spec.containers.name container 이름 입니다.\nline19 spec.template.spec.containers.image container image name 입니다. docker에서의 image name 및 tag를 입력하는 방식과 같은 방식으로 입력합니다.\nline20 spec.template.spec.containers.ports containerPort를 배열로 복수개 설정 합니다.\nline21 spec.template.spec.containers.ports.containerPort contianer가 사용하는 port에 대한 설정 입니다. 8080 포트를 사용한다고 명시되어 있습니다.\nline22 spec.template.spec.containers.imagePullPolicy \u0026ldquo;Always\u0026rdquo;와 \u0026ldquo;IfNotPresent\u0026rdquo;의 설정이 가능 합니다. \u0026ldquo;Always\u0026rdquo;인 경우, Remote Registry로 부터 Image를 항상 Download하고, \u0026ldquo;IfNotPresent\u0026rdquo;는 우선적으로 캐싱된 Image가 있으면 해당 Image를 사용하고, 없는 경우에만 Remote Registry로 부터 Download를 시도 합니다.\nDeployment object yaml 파일 상세 작성 방법은 Deployment 공식가이드를 참고 바랍니다.\nline24 spec.template.spec.containers.resources.requests 컨테이너가 요청할 최소한의 리소스에 대한 설정입니다. Spring Boot 애플리케이션의 경우는 메모리 값을 256M 이상으로 설정해야 합니다.\nline27 spec.template.spec.containers.resources.limits 컨테이너가 최대한으로 사용할 리소스에 대한 설정입니다. 애플리케이션에 따라 적절한 CPU와 메모리 값으로 설정해주어야 합니다. CPU를 너무 낮게 설정하면 애플리케이션이 기동되는데 많은 시간이 걸릴 수 있습니다.\n특정 노드에 Pod 배포하기 지정된 노드에 Pod를 배포하고 싶은 경우에 참고합니다.\n#노드 조회를 통해 노드명 확인 $ kubectl get node NAME STATUS ROLES AGE VERSION 10.178.158.149 NotReady 15d v1.8.3+icp 10.178.158.172 Ready 15d v1.8.3+icp 10.178.158.177 Ready 15d v1.8.3+icp 10.178.158.181 Ready 15d v1.8.3+icp 10.178.158.186 Ready 15d v1.8.3+icp #kubectl label nodes = 명령어를 통해 노드에 label 추가 $ kubectl label nodes 10.178.158.181 hostname=10.178.158.181 node \u0026#34;10.178.158.181\u0026#34; labeled #노드의 라벨 조회 $ kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS 10.178.158.149 NotReady 15d v1.8.3+icp beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,gpu/nvidia=NA,kubernetes.io/hostname=10.178.158.149 10.178.158.172 Ready 15d v1.8.3+icp beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,gpu/nvidia=NA,kubernetes.io/hostname=10.178.158.172,management=true,role=master 10.178.158.177 Ready 15d v1.8.3+icp beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,gpu/nvidia=NA,kubernetes.io/hostname=10.178.158.177,proxy=true 10.178.158.181 Ready 15d v1.8.3+icp beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,gpu/nvidia=NA,hostname=10.178.158.181,kubernetes.io/hostname=10.178.158.181,test=test 10.178.158.186 Ready 15d v1.8.3+icp beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,gpu/nvidia=NA,kubernetes.io/hostname=10.178.158.186  #Deployment 생성 파일에 아래 nodeSelector 필드를 추가 #nodeSelector를 통해 Pod가 특정 노드에서만 실행하도록 제한. #\u0026#39;hostname=10.178.158.181\u0026#39; 라는 label을 가진 노드를 지정하여 pod를 배포 apiVersion: apps/v1beta2 # for versions before 1.8.0 use apps/v1beta1 kind: Deployment metadata: name: gs-spring-boot-docker-deployment labels: app: gs-spring-boot-docker spec: replicas: 1 selector: matchLabels: app: gs-spring-boot-docker template: metadata: labels: app: gs-spring-boot-docker spec: containers: - name: gs-spring-boot-docker image: dtlabs/gs-spring-boot-docker:1.0 ports: - containerPort: 8080 imagePullPolicy: Always nodeSelector: hostname: 10.178.158.181 Deployment object 생성  Deployment object 생성을 위해 아래의 command를 수행합니다.\n# gs-spring-boot-docker-deployment.yaml 파일 위치 확인 $ ls gs-spring-boot-docker-deployment.yaml # kubectl apply 명령어에서 -f 옵션을 통해 파일명이 gs-spring-boot-docker-deployment.yaml 임을 인자로 전달합니다. # --record 옵션을 통해 yaml 설정 이력을 기록할 수 있습니다. $ kubectl apply --record -f ./gs-spring-boot-docker-deployment.yaml deployment \u0026#34;gs-spring-boot-docker-deployment\u0026#34; created \u0026lsquo;kube apply\u0026rsquo; VS. \u0026lsquo;kubectl create \u0026ndash;save-config\u0026rsquo;\nKubernetes에서 object 생성을 위한 기본 명령어는 \u0026lsquo;kubectl create\u0026rsquo; 입니다.\n하지만, \u0026lsquo;kubectl create\u0026rsquo; 동일한 동작을 하고, 더 간결한 방식인 \u0026lsquo;kube apply\u0026rsquo;를 사용합니다.\n그 이유는,\n \u0026lsquo;kubectl create -f yaml파일명\u0026rsquo;을 통해 object를 생성시, 생성 시점의 설정에 대한 버전 기록을 위해 \u0026lsquo;\u0026ndash;save-config\u0026rsquo; flag를 사용할 수 있습니다. 기록은 설정 내 \u0026lsquo;annotation\u0026rsquo; 부분에 쓰여집니다. 생성시 \u0026lsquo;\u0026ndash;save-config\u0026rsquo;를 추가하고, 이후에 설정 변경이 발생할 경우, \u0026lsquo;과거-현재-신규\u0026rsquo; 버전에 대한 three-way diff 방식의 비교를 합니다. \u0026rsquo;\u0026ndash;save-config\u0026rsquo; 없이 생성시, (생성 시점의 과거 버전 기록이 제외된) \u0026lsquo;현재-신규\u0026rsquo; 버전에 대한 two-way diff 방식의 비교를 합니다. two-way diff 방식에서는, 특정 설정을 삭제하는 변경 적용 시, 과거 버전의 기록이 없기 때문에, 삭제될 부분을 인식하지 못하는 제약이 있기 때문입니다. 따라서, \u0026lsquo;kubectl create\u0026rsquo; 동일한 동작을 하고, 더 간결한 방식인 \u0026lsquo;kube apply\u0026rsquo;를 사용합니다.  상세 설명은 여기의 kubectl apply 공식가이드를 참고 바랍니다.\n Deployment object 생성을 확인 합니다.  # Deployments 조회 (kubectl get deployments 명령어 또는 kubectl get deploy 명령어는 같은 의미이므로 선택적으로 사용 가능) $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE gs-spring-boot-docker-deployment 1 1 1 1 23m Deployment state\n NAME - 클러스터 내 Deployment의 이름이 리스트업 됩니다. DESIRED - 개발자가 작성한 Deployment yaml 파일 내 정의한 replicas의 수를 나타냅니다. CURRENT - 현재 Running 중인 replicas의 수를 나타냅니다. UP-TO-DATE - desired state를 만족하기 위해 현재 수행되어지고 있는 replicas의 수를 나타냅니다. AVAILABLE - 현재 User가 사용 가능한 replicas의 수를 나타냅니다. AGE - 애플리케이션이 Running 상태가 된 이후 부터의 시간을 나타냅니다.    Pod, ReplicaSets object 생성 또한 확인 가능 합니다.\n# ReplicaSets 조회 (kubectl get replicasets 명령어 또는 kubectl get rs 명령어는 같은 의미이므로 선택적으로 사용 가능) $ kubectl get rs NAME DESIRED CURRENT READY AGE gs-spring-boot-docker-deployment-56fb494f67 1 1 1 32m # Pods 조회 (kubectl get pods 명령어 또는 kubectl get pod 명령어 또는 kubectl get po 명령어는 같은 의미이므로 선택적으로 사용 가능) $ kubectl get po NAME READY STATUS RESTARTS AGE gs-spring-boot-docker-deployment-56fb494f67-g2lwr 1/1 Running 0 33m 이어서 Service object 도 만들어 봅시다.\nService object yaml 파일 생성 및 작성  [애플리케이션명]-service.yaml 파일 생성\n  테스트하기 위한 애플리케이션명은 \u0026lsquo;gs-spring-boot-docker\u0026rsquo; 입니다. 여기에 suffix로 \u0026lsquo;-service\u0026rsquo;를 추가 하였습니다. suffix는 선택사항으로 필수는 아닙니다. \u0026lsquo;gs-spring-boot-docker\u0026rsquo; 이름의 애플리케이션에 대한 \u0026lsquo;service\u0026rsquo; object 생성을 위한 yaml이라는 의미로 gs-spring-boot-docker-service.yaml 파일을 생성하였습니다.\n yaml 작성 방법을 참고하여 gs-spring-boot-docker-service.yaml 파일에 아래의 내용을 작성 합니다.   apiVersion: v1 kind: Service metadata: name: gs-spring-boot-docker-service spec: ports: - name: \u0026#34;8080\u0026#34; port: 8081 targetPort: 8080 selector: app: gs-spring-boot-docker type: NodePort line1 apiVersion API Server에서 관리되는 API 버전을 나타냅니다. 사용자가 입력한 apiVersion에 맞는 API를 사용하게 됩니다. Kubernetes API는 실험 단계의 API를 \u0026lsquo;beta\u0026rsquo; 형태로 지원하고, 지속 업데이트 하고 있습니다. 따라서 Kubernetes API 공식가이드를 통해 현재 사용자의 Kubernetes 버전 별 호환 및 사용 가능한 API를 확인 후 사용해야 합니다.\nline2 kind 현재 yaml이 어떤 object를 생성하기 위함인지 kind에 설정합니다. kind: Service 설정을 통해 현재 yaml로 Service object를 생성하게 됩니다.\nline3 metadata Service object 자신의 고유 정보를 입력합니다.\nline4 metadata.name Service object에 대한 Unique-key를 입력합니다. 이 name 값을 통해 여러 object 중 해당 name을 갖는 object를 조회할 수 있습니다.\nline5 spec Service object가 수행하는 내용에 대한 설정 입니다.\nline6 spec.ports Service object 에 설정할 포트 정보 입니다.\nspec.ports.name Service object 에 설정할 포트 중 특정 포트 정보에 대한 명칭 입니다.\nspec.ports.port Cluster 내부에서 사용될 Service object의 포트 입니다.\nspec.ports.targetPort Service object로 들어온 요청을 전달할 target이되는 Pod이 노출하고 있는 포트 입니다. Deployment의 spec.template.spec.containers.ports.containerPort에 전달 됩니다. 기본적으로 targetPort는 port 필드와 동일한 값으로 설정됩니다.\nspec.selector Service object가 요청을 전달할 Pod을 찾기위한 검색어 입니다. Pod의 label이 app: gs-spring-boot-docker 인 Pod을 찾아 요청을 전달 하게 됩니다. 찾은 Pod이 여러개인 경우 load balancing 정책에 따라 하나의 Pod을 선택하게 됩니다.\nspec.type Service object를 노출하기 위한 방식을 설정 합니다. 가능한 type으로는 ClusterIP, NodePort, LoadBalancer가 있습니다.\n ClusterIP - Service object의 cluster-internal IP만 노출 하고, Cluster 내부에서만 접근 가능 합니다. NodePort - 각 Node에서 static 포트를 노출합니다. 클러스터 외부에서 :의 형태로 request가 가능 합니다. 앱을 배포후 테스트할 때 선택하기 좋은 type입니다. LoadBalancer - 특정 cloud provider(GCE/AWS)의 load balancer에게 Service object를 노출시키는 방법 입니다.   Service object 생성  Service object 생성을 위해 아래의 command를 수행합니다.\n# gs-spring-boot-docker-service.yaml 파일 위치 확인 $ ls gs-spring-boot-docker-deployment.yaml gs-spring-boot-docker-service.yaml # kubectl apply 명령어에서 -f 옵션을 통해 파일명이 gs-spring-boot-docker-service.yaml 임을 인자로 전달합니다. $ kubectl apply -f ./gs-spring-boot-docker-service.yaml service \u0026#34;gs-spring-boot-docker-service\u0026#34; created  Service object 생성을 확인 합니다.  # Service 조회 (kubectl get service 명령어 또는 kubectl get svc 명령어는 같은 의미이므로 선택적으로 사용 가능) # 8081이 Service에 접근하기 위한 Cluster 내부 포트, 30993이 외부 노출 포트 입니다. 외부포트는 직접 설정하지 않고 자동 부여된 경우 30000-32767 사이에서 random 부여 됩니다. $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gs-spring-boot-docker-service NodePort 10.99.197.147 \u0026lt;none\u0026gt; 8081:30993/TCP 11s  애플리케이션 접속  접속을 위한 IP / Port 확인\n앞서, Service object의 NodePort설정으로 : 의 형태로 접속 테스트가 가능합니다.\n아래의 명령어를 통해 NodeIP와 NodePort 정보를 조회합니다. (개인별 테스트 환경에 따라 다른값의 IP, Port가 조회될 수 있습니다)\n아래의 내용 중 \u0026lsquo;Node: poc.k8s-worker01.cloudz.co.kr/169.56.109.58\u0026rsquo; 이 부분에서 \u0026lsquo;169.56.109.58\u0026lsquo;의 IP가 테스트에서 이용할 NodeIP 입니다. 이 방법으로 확인이 어렵다면 Kubernetes 대쉬보드에서 확인해 보거나 Minikube의 경우에는 minikube가 설치된 VM 머신의 IP를 활용하시면 됩니다.\n# pod name 알아내기 $ kubectl get pod NAME READY STATUS RESTARTS AGE gs-spring-boot-docker-deployment-56fb494f67-g2lwr 1/1 Running 0 3h # pod name을 이용하여, kubectl describe $ kubectl describe pod gs-spring-boot-docker-deployment-56fb494f67-g2lwr Name: gs-spring-boot-docker-deployment-56fb494f67-g2lwr Namespace: default Node: poc.k8s-worker01.cloudz.co.kr/169.56.109.58 ... (생략) 아래의 내용 중 \u0026lsquo;8081:30993/TCP\u0026rsquo;에서 \u0026lsquo;30993\u0026rsquo; 부분이 외부로 노출된 NodePort 입니다.\n# Service object 조회를 통해 NodePort를 알아냅니다. # 8081:30993/TCP 에서 30993이 외부로 노출된 NodePort 입니다. $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gs-spring-boot-docker-service NodePort 10.99.197.147 \u0026lt;none\u0026gt; 8081:30993/TCP 12m kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 14d 브라우저 접속 테스트\n위에서 알아낸 169.56.109.58:30993 을 브라우저로 접속하여 정상 접속 됨을 확인 합니다.\n  애플리케이션 변경 Image 버전 Update Kubernetes에서 소스 수정 및 버전을 업데이트 하는 방법입니다. Docker Hub에 미리 Push 된 샘플 Image를 사용합니다.\n Image Name : dtlabs/gs-spring-boot-docker:2.0 Spring Boot 공식 가이드에서 제공하는 샘플소스를 수정하여 빌드된 Image 입니다. 업데이트 버전을 tagging 하여 Docker build 후, Docker Hub에 Push 하였습니다. Docker를 이용하여 애플리케이션을 Build 및 Docker Hub에 Push 하는 방법은 Docker 시작하기 가이드를 참고 하시기 바랍니다.    kubectl set image 명령어를 통한 Image 버전 업데이트 수행\n# deployment/gs-spring-boot-docker-deployment 설정을 통해, deployment object 중 name이 gs-spring-boot-docker-deployment인 것의 Pod Image를 업데이트 하겠다는 의미 입니다. # gs-spring-boot-docker=dtlabs/gs-spring-boot-docker:2.0 설정을 통해, Image의 name이 gs-spring-boot-docker인 Image를 dtlabs/gs-spring-boot-docker:2.0 라는 Docker Hub/Registry Image로 업데이트 하겠다는 의미 입니다. # kubectl set image 명령어 수행 시, --record 를 command에 추가 하였다면, kubectl rollout history 수행 결과에서 CHANGE-CAUSE 값으로 업데이트 시 수행한 명령어가 기록되어 보여집니다. $ kubectl set image deployment/gs-spring-boot-docker-deployment gs-spring-boot-docker=dtlabs/gs-spring-boot-docker:2.0 --record deployment \u0026#34;gs-spring-boot-docker-deployment\u0026#34; image updated kubectl set image 명령어를 통해 단일 Pod을 업데이트 하는 경우, 기존의 Pod을 Terminating 후, 신규 Pod을 Running 상태로 변경하는 동안 일정 시간의 서비스 불가 상태를 야기 할 수 있습니다. 이를 방지하기 위해 Pod을 2개 이상 유지하고, rolling update 기능을 통해 Pod의 순차적 업데이트를 수행하는 것이 좋습니다.\n 버전 업데이트 확인  Kubernetes에서는 Image의 버전 업데이트 및 배포 시, 과거 버전의 롤백을 위해, 각 버전 별 Revision를 보존합니다. 그리고 Revision 별 Pod의 상태는 Revision 별 생성되는 Replica Sets이 저장하고 있습니다. 그렇기 때문에, 만약 관리자가 업데이트 후 롤백을 시도 할 경우, 보존되고 있는 Revision 중 롤백 버전에 해당하는 Revision의 Replica Sets이 롤백 버전의 Pod을 복구 하게 됩니다.\n# 기존 running 상태의 Pod 및 새로 ContainerCreating 중인 Pod 확인 # ContainerCreating 단계는 조회 시점에 따라 skip될 수 있습니다 # Pod의 Unique NAME이 서로 다른 것을 확인할 수 있고, 신규 버전의 Pod이 ContainerCreating 중에도 애플리케이션 이전 버전에 여전히 접속 가능 합니다. $ kubectl get pod NAME READY STATUS RESTARTS AGE gs-spring-boot-docker-deployment-56fb494f67-g2lwr 1/1 Running 0 4h gs-spring-boot-docker-deployment-7fbf88754d-6grl7 0/1 ContainerCreating 0 1m # Replica Sets 또한 Revision 보존을 위해 이전 버전의 Replica Sets이 그대로 유지되는 모습을 확인할 수 있습니다. # Pod이 ContainerCreating 중에는 새로 생성된 Replica Sets의 READY 상태가 \u0026#39;0\u0026#39;으로 업데이트 진행중임을 확인할 수 있습니다. $ kubectl get rs NAME DESIRED CURRENT READY AGE gs-spring-boot-docker-deployment-56fb494f67 1 1 1 4h gs-spring-boot-docker-deployment-7fbf88754d 1 1 0 1m ################################ # 일정 시간 이후, ContainerCreating(Image 다운로드) 및 업데이트가 완료된 후 ################################ # 업데이트가 완료 되면, 아래와 같이 신규 생성된 Pod이 Running $ kubectl get pod NAME READY STATUS RESTARTS AGE gs-spring-boot-docker-deployment-56fb494f67-g2lwr 1/1 Terminating 0 4h gs-spring-boot-docker-deployment-7fbf88754d-6grl7 1/1 Running 0 9m # 업데이트가 완료 되면, 아래와 같이 신규 생성된 Replica Sets이 READY 상태가 됩니다. $ kubectl get rs NAME DESIRED CURRENT READY AGE gs-spring-boot-docker-deployment-56fb494f67 0 0 0 4h gs-spring-boot-docker-deployment-7fbf88754d 1 1 1 9m 브라우저 접속 및 버전 2.0으로 업데이트된 것을 확인합니다.\n 버전 변경 history 확인\nkubectl rollout history 명령어를 통해 업데이트 history를 확인할 수 있습니다.\nkubectl set image 명령어 수행 시, \u0026ndash;record 옵션을 사용했다면, kubectl rollout history 수행 결과에서 CHANGE-CAUSE 값으로 업데이트 시 수행한 명령어 기록을 확인할 수 있습니다.\n# deployment/gs-spring-boot-docker-deployment의 의미는, deployment의 이름이 gs-spring-boot-docker-deployment 인 것을 조회하겠다는 뜻 입니다. $ kubectl rollout history deployment/gs-spring-boot-docker-deployment deployments \u0026#34;gs-spring-boot-docker-deployment\u0026#34; REVISION CHANGE-CAUSE 1 kubectl create --filename=gs-spring-boot-docker-deployment.yaml --record=true 2 kubectl set image deployment/gs-spring-boot-docker-deployment gs-spring-boot-docker=dtlabs/gs-spring-boot-docker:2.0 # --revision=2 를 추가하면, 특정 revision 업데이트 정보를 확인할 수 있습니다. # revision #2에서 Image: dtlabs/gs-spring-boot-docker:2.0를 통해 정상적으로 Image 업데이트가 이루어진 것을 확인 할 수 있습니다. $ kubectl rollout history deployment/gs-spring-boot-docker-deployment --revision=2 deployments \u0026#34;gs-spring-boot-docker-deployment\u0026#34; with revision #2 Pod Template: Labels: app=gs-spring-boot-docker pod-template-hash=3969443108 Containers: gs-spring-boot-docker: Image: dtlabs/gs-spring-boot-docker:2.0 Port: 80/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Image 버전 Rollback  undo 방식\n버전 변경 바로 직전의 상태로 Rollback을 원할 경우에 사용 가능한 방법 입니다.  $ kubectl rollout undo deployment/gs-spring-boot-docker-deployment deployment \u0026#34;gs-spring-boot-docker-deployment\u0026#34; rolled back # 기존 Running Pod이 Terminating 되고, 신규 생성된 Pod이 Running 되었습니다. $ kubectl get pod NAME READY STATUS RESTARTS AGE gs-spring-boot-docker-deployment-56fb494f67-g2lwr 1/1 Terminating 0 5h gs-spring-boot-docker-deployment-7fbf88754d-6grl7 1/1 Terminating 0 1h gs-spring-boot-docker-deployment-56fb494f67-rfvnv 1/1 Running 0 5s # Revision #1에 해당하는 Replica Sets를 유지키시고 있었기 때문에, 새로 Replica Sets을 생성하지 않고 Rollback이 수행 됨을 알 수 있습니다. $ kubectl get rs NAME DESIRED CURRENT READY AGE gs-spring-boot-docker-deployment-56fb494f67 1 1 1 5h gs-spring-boot-docker-deployment-7fbf88754d 0 0 0 1h 브라우저에서 변경된 버전에 접속하여 Rollback을 확인 합니다. \u0026lsquo;2.0\u0026rsquo; 버전에서 아래와 같이 기존 버전(1.0)으로 변경된 것을 확인할 수 있습니다.\n   \u0026ndash;to-revision 방식\n특정 Revision을 명시적으로 선택하여 버전을 변경할 경우 사용합니다.\n\u0026ndash;to-revision 방식을 테스트 하기 위해, 버전 1.0을 2.0이 되도록, Revision #2로 다시 변경 해보도록 하겠습니다.\n$ kubectl rollout undo deployment/gs-spring-boot-docker-deployment --to-revision=2 deployment \u0026#34;gs-spring-boot-docker-deployment\u0026#34; rolled back # 기존 Running Pod이 Terminating 되고, 신규 생성된 Pod이 Running 되었습니다. $ kubectl get pod NAME READY STATUS RESTARTS AGE gs-spring-boot-docker-deployment-56fb494f67-g2lwr 1/1 Terminating 0 5h gs-spring-boot-docker-deployment-56fb494f67-rfvnv 1/1 Terminating 0 6m gs-spring-boot-docker-deployment-7fbf88754d-6grl7 1/1 Terminating 0 1h gs-spring-boot-docker-deployment-7fbf88754d-7zx7s 1/1 Running 0 6s # Revision #2에 해당하는 Replica Sets를 유지키시고 있었기 때문에, 새로 Replica Sets을 생성하지 않고 Rollback이 수행 됨을 알 수 있습니다. $ kubectl get rs NAME DESIRED CURRENT READY AGE gs-spring-boot-docker-deployment-56fb494f67 0 0 0 5h gs-spring-boot-docker-deployment-7fbf88754d 1 1 1 1h  브라우저에서 변경된 버전에 접속하여 Rollback을 확인 합니다.\n\u0026lsquo;1.0\u0026rsquo; 버전에서 아래와 같이 \u0026lsquo;2.0\u0026rsquo; 버전으로 변경된 것을 확인할 수 있습니다.\nyaml 설정 변경 Kubernetes object 생성 시, 사용자가 작성한 yaml 파일의 설정 이외에, Kubernetes에서 자동 부여하는 default 설정이 있습니다. 아래의 명령어를 통해 default 설정이 추가된 전체 설정을 확인할 수 있습니다.\n$ kubectl get deploy gs-spring-boot-docker-deployment -o yaml  이렇게 적용된 설정을 변경할 때, 다음의 명령어들을 사용할 수 있는데, 그 차이점을 명확히 이해하고 사용해야 합니다.\n kubectl apply\n 사용자가 작성한 yaml에 의해 설정된 사항만 설정 변경이 적용 됩니다 (overwrite 방식) 이 명령어는 \u0026lsquo;kubectl create \u0026ndash;save-config\u0026rsquo; 와 같은 동작을 합니다. (object 생성 전, \u0026lsquo;kubectl apply -f 파일명\u0026rsquo;의 명령어 수행을 통해 object 생성이 가능 합니다.)\n# gs-spring-boot-docker-deployment.yaml 파일 수정 후 아래의 명령어 수행 $ kubectl apply -f gs-spring-boot-docker-deployment.yaml  kubectl edit\n 전체 설정(사용자가 작성한 yaml의 설정과 자동 부여되는 default 설정)이 적용 됩니다 (overwrite 방식) Editor 도구 (VIM or 사용자 Editor)에서 전체 설정이 보여지고, 수정 및 저장 시 설정이 적용 됩니다. 전체 설정에 대한 yaml 파일에 대한 형상관리의 어려움 때문에, 자동 부여되는 default 설정 변경시 특히 주의해야 합니다.\n# deployment/gs-spring-boot-docker-deployment 를 통해, deployment object 중 name이 gs-spring-boot-docker-deployment 인 설정을 Editor로 open 합니다. $ kubectl edit deployment/gs-spring-boot-docker-deployment # Editor 변경 및 저장 시, 설정이 적용 됩니다.  kubectl patch\n 전체 설정(사용자가 작성한 yaml의 설정과 자동 부여되는 default 설정)에 추가 설정 일부를 패치 yaml 파일을 통해 삽입할 수 있습니다.\npatch-file.yaml\nspec: template: spec: containers: - name: patch-demo-ctr-2 image: redis# 현재 경로에 patch-file.yaml을 작성한 후, # deployment 중 name이 gs-spring-boot-docker-deployment인 object에 대해 patch-file.yaml 설정을 추가삽입 하겠다는 의미 입니다. $ kubectl patch deployment gs-spring-boot-docker-deployment --patch \u0026#34;$(cat patch-file.yaml)\u0026#34; Windows OS에서는 아래의 \u0026ldquo;$(cat patch-file.yaml)\u0026rdquo; 명령어가 동작하지 않기 때문에, 아래의 동일한 동작을 하는 다른 방법을 사용 합니다.\n\u0026gt; kubectl patch deployment gs-spring-boot-docker-deployment --patch \u0026#34;{\\\u0026#34;spec\\\u0026#34;:{\\\u0026#34;template\\\u0026#34;:{\\\u0026#34;spec\\\u0026#34;:{\\\u0026#34;containers\\\u0026#34;:[{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;patch-demo-ctr-2\\\u0026#34;,\\\u0026#34;image\\\u0026#34;:\\\u0026#34;redis\\\u0026#34;}]}}}}  kubectl replace \u0026ndash;force\n 경우에 따라, Deployment 설정은 정상인데, broken Pod이 발생하여, Pod 자체를 재생성(Pod의 설정은 그대로 유지하며)할 경우가 발생할 수 있습니다. 이 때, \u0026lsquo;kubectl replace \u0026ndash;force\u0026rsquo;를 사용 하는데, 이 경우 서비스가 중단되는 \u0026lsquo;Disruptive updates\u0026rsquo;가 발생하게 됨을 유의하여 사용 합니다.\n############################ # 사용자가 작성한 yaml의 설정을 이용하여 replace 할 경우, 자동 부여되는 default 설정 부분은 일부 변경됨(uid 같이 K8S에서 부여한 식별코드 등) $ kubectl replace --force -f gs-spring-boot-docker-deployment.yaml ############################ # 전체 설정(사용자가 작성한 yaml의 설정과 자동 부여되는 default 설정)을 이용하여 replace 할 경우 (= 설정을 그대로 유지한 상태로 Pod만 재생성) $ kubectl get deployment gs-spring-boot-docker-deployment -o yaml | kubectl replace --force -f - Deployment 설정 중, pod template에 해당하는 설정(.spec.template)을 변경하면, Rollout이 발생하고, Kubernetes에서 Revision을 관리 하게 됩니다. 새로운 Revision이 생성되면, 새로운 Replica Sets가 생성되고, 그에 따라 새로운 Pod이 생성 및 Running 된 후, 기존 Revision의 Pod은 Terminating 됩니다.\n   디버깅 kubectl describe Kubernetes object의 상세 설명을 보기 위해 사용합니다.\n 기본 형식\nkubectl describe (-f FILENAME | TYPE [NAME_PREFIX | -l label] | TYPE/NAME) [options]\n 상세 설명 보기\nPod이 어떤 Node에 생성되었는지, Node명과 NodeIP를 알 수 있습니다.(아래의 예시에서 Node: poc.k8s-worker01.cloudz.co.kr/169.56.109.58) 그리고 Pod container에 사용된 Image의 tag를 통해 버전을 확인할 수 있습니다.\n# kubectl get pod을 이용하여, 로그를 확인할 Pod의 Name을 알아냅니다. $ kubectl get pod NAME READY STATUS RESTARTS AGE gs-spring-boot-docker-deployment-56fb494f67-g2lwr 1/1 Terminating 0 20h gs-spring-boot-docker-deployment-56fb494f67-rfvnv 1/1 Terminating 0 14h gs-spring-boot-docker-deployment-7fbf88754d-6grl7 1/1 Terminating 0 15h gs-spring-boot-docker-deployment-7fbf88754d-7zx7s 1/1 Running 0 14h # describe 대상 object가 pod이고, 그 Name이 gs-spring-boot-docker-deployment-7fbf88754d-7zx7s 인 것에 대한 설명을 가져옵니다. $ kubectl describe pod gs-spring-boot-docker-deployment-7fbf88754d-7zx7s Name: gs-spring-boot-docker-deployment-7fbf88754d-7zx7s Namespace: default Node: poc.k8s-worker01.cloudz.co.kr/169.56.109.58 Start Time: Thu, 07 Dec 2017 19:11:35 +0900 Labels: app=gs-spring-boot-docker pod-template-hash=3969443108 Annotations: kubernetes.io/created-by={\u0026#34;kind\u0026#34;:\u0026#34;SerializedReference\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;reference\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;ReplicaSet\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;gs-spring-boot-docker-deployment-7fbf88754d\u0026#34;,\u0026#34;uid\u0026#34;:\u0026#34;a0cd4243-db2... Status: Running IP: 192.168.176.145 Created By: ReplicaSet/gs-spring-boot-docker-deployment-7fbf88754d Controlled By: ReplicaSet/gs-spring-boot-docker-deployment-7fbf88754d Containers: gs-spring-boot-docker: Container ID: docker://785fe1ea6cce6d705f2c7a5f3ec64619988923abb2e82e21575344daf9d8df34 Image: dtlabs/gs-spring-boot-docker:2.0 Image ID: docker-pullable://dtlabs/gs-spring-boot-docker@sha256:2dc3ce60f8dcf25d41a8cae795b38d0c230493a29b70778a9da02efc9fe52d5e Port: 8080/TCP State: Running Started: Thu, 07 Dec 2017 19:11:41 +0900 Ready: True Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-rshst (ro) Conditions: Type Status Initialized True Ready True PodScheduled True Volumes: default-token-rshst: Type: Secret (a volume populated by a Secret) SecretName: default-token-rshst Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.alpha.kubernetes.io/notReady:NoExecute for 300s node.alpha.kubernetes.io/unreachable:NoExecute for 300s Events: \u0026lt;none\u0026gt; kubectl logs Kubernetes에서 애플리케이션 디버깅을 위해 log를 확인할 수 있습니다. -f 옵션을 추가하여, streamed log를 확인할 수도 있습니다.\n 기본 형식  kubectl logs [-f] [-p] (POD | TYPE/NAME) [-c CONTAINER] [options]\n log 보기  # kubectl get pod을 이용하여, 로그를 확인할 Pod의 Name을 알아냅니다. $ kubectl get pod NAME READY STATUS RESTARTS AGE gs-spring-boot-docker-deployment-56fb494f67-g2lwr 1/1 Terminating 0 20h gs-spring-boot-docker-deployment-56fb494f67-rfvnv 1/1 Terminating 0 14h gs-spring-boot-docker-deployment-7fbf88754d-6grl7 1/1 Terminating 0 15h gs-spring-boot-docker-deployment-7fbf88754d-7zx7s 1/1 Running 0 14h # kubectl logs 명령어 뒤에 Pod Name을 입력합니다. $ kubectl logs gs-spring-boot-docker-deployment-7fbf88754d-7zx7s . ____ _ __ _ _ /\\\\ / ___\u0026#39;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | \u0026#39;_ | \u0026#39;_| | \u0026#39;_ \\/ _` | \\ \\ \\ \\  \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) \u0026#39; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v1.5.9.RELEASE) 2017-12-07 10:11:44.558 INFO 1 --- [ main] hello.Application : Starting Application v0.1.0 on gs-spring-boot-docker-deployment-7fbf88754d-7zx7s with PID 1 (/app.jar started by root in /) 2017-12-07 10:11:44.608 INFO 1 --- [ main] hello.Application : No active profile set, falling back to default profiles: default ... 2017-12-07 10:11:50.845 INFO 1 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http) 2017-12-07 10:11:50.851 INFO 1 --- [ main] hello.Application : Started Application in 7.797 seconds (JVM running for 9.041) # -f 옵션을 추가하여, streamed log를 확인할 수도 있습니다. $ kubectl logs gs-spring-boot-docker-deployment-7fbf88754d-7zx7s -f  kubectl exec 동작하고 있는 Container 내부 shell 접속이 가능 합니다. -c 옵션으로 특정 container를 지정하여 접속 가능 합니다. 현재 Pod 내 단일 container로 -c 옵션은 주지 않았습니다.\n 기본 형식\nkubectl exec POD [-c CONTAINER] \u0026ndash; COMMAND [args\u0026hellip;] [options]\n shell 접속\n# kubectl get pod을 이용하여, 로그를 확인할 Pod의 Name을 알아냅니다. $ kubectl get pod NAME READY STATUS RESTARTS AGE gs-spring-boot-docker-deployment-56fb494f67-g2lwr 1/1 Terminating 0 20h gs-spring-boot-docker-deployment-56fb494f67-rfvnv 1/1 Terminating 0 14h gs-spring-boot-docker-deployment-7fbf88754d-6grl7 1/1 Terminating 0 15h gs-spring-boot-docker-deployment-7fbf88754d-7zx7s 1/1 Running 0 14h # kubectl exec 명령어와 -it 옵션을 사용하고, command로 /bin/sh을 실행시키면 shell 입력창이 보이게 됩니다. $ kubectl exec -it gs-spring-boot-docker-deployment-7fbf88754d-7zx7s -- /bin/sh / # ls app.jar dev home media proc run srv tmp var bin etc lib mnt root sbin sys usr # 현재 hostname이 Pod Name인 것을 확인할 수 있습니다. / # hostname gs-spring-boot-docker-deployment-7fbf88754d-7zx7s # shell 접속 종료 시, \u0026#39;exit\u0026#39; 입력 또는 Ctrl + p, Ctrl + q 입력 애플리케이션 삭제 kubectl delete Kubernetes의 모든 object (Deployment, Service, Pod, Replica Sets, \u0026hellip;)를 삭제하기 위해서 공통 적으로 kubectl delete 명령어를 사용합니다. 배포한 애플리케이션을 삭제하기 위해서는, Pod을 delete하는 것이 아니라 Deployment를 delete하는 방식 입니다. Pod을 delete하게 되면 Replica Sets이 설정된 replica 의 수에 따라 Pod을 복원하려 하기 때문입니다. Deployment를 delete하게 되면 관련된 Replica Sets, Pod, Deployment가 모두 삭제 됩니다.\n 기본 형식  kubectl delete ([-f FILENAME] | TYPE [(NAME | -l label | \u0026ndash;all)]) [options]\n 애플리케이션 삭제    kubectl delete OBJECT_TYPE OBJECT_NAME 방식 : OBJECT_TYPE에 대한 OBJECT_NAME을 통해 특정 이름의 object를 단건 씩 삭제가 가능 합니다.  kubectl delete -f FILE_NAME 방식 : yaml 파일 내 정의된 object에 대한 삭제가 가능 합니다. 특정 디렉토리 내 모든 Kubernetes yaml, json 파일을 대상으로 delete할 수도 있습니다.\n# kubectl get deploy(= kubectl get deployment)를 이용하여, 로그를 확인할 Pod의 Name을 알아냅니다. $ kubectl get deploy NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE gs-spring-boot-docker-deployment 1 1 1 1 20h #################################### # kubectl delete OBJECT_TYPE OBJECT_NAME 방식 $ kubectl delete deployment gs-spring-boot-docker-deployment deployment \u0026#34;gs-spring-boot-docker-deployment\u0026#34; deleted #################################### # kubectl delete -f FILE_NAME 방식 # 현재 명령어가 수행되는 디렉토리 상의 모든 Kubernetes yaml, json 파일 확인 $ ls gs-spring-boot-docker-deployment.yaml gs-spring-boot-docker-service.yaml # -f 를 통해 파일을 이용 하여 delete를 수행합니다. # 하나의 파일을 이용한 delete는, \u0026#39;kubectl delete -f xxx.yaml\u0026#39;과 같이 사용 합니다. # path로 \u0026#39;.\u0026#39; 을 주어 현재 경로에 있는 모든 Kubernetes yaml, json 파일을 대상으로 하도록 합니다. 단, 의도치 않은 파일까지 삭제되지 않도록 주의해야 합니다. $ kubectl delete -f . deployment \u0026#34;gs-spring-boot-docker-deployment\u0026#34; deleted service \u0026#34;gs-spring-boot-docker-service\u0026#34; deleted # Deployment를 delete하면 관련된 Pod, Replica Sets 또한 모두 삭제 됩니다. $ kubectl get pod No resources found. 여기까지 Kubernetes 환경에서 애플리케이션을 배포하고 접속, 관리하는 방법에 대해 알아보았습니다. 다음에는 DB나 Redis 등과 같은 Backing 서비스의 연동 방법에 대해 실습해보도록 합시다.\n   "
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/command-tool/",
        "title": "CF CLI 사용하기",
        "tags": ["cloud foundry", "cf", "cf cli", "paas"],
        "description": "",
        "content": " 오픈소스 CF CLI에서 자주 사용하는 CF CLI 명령어를 사용하는 방법을 정리해보았습니다.\nCF CLI에서 사용 가능한 모든 명령어의 사용법은 CF 공식 홈페이지 문서를 참고하길 바랍니다.\n CF CF CLI 사용 방법을 알아보기에 앞서 CF가 무엇인지 간단히 살펴보겠습니다. CF는 Cloud Foundry의 약자로, 2011년 VMWare가 업계 최초로 만든 오픈소스 PaaS 플랫폼입니다. 지금은 Cloud Foundry 재단에서 관리하고 있습니다.\n오픈소스 CF 제품 Cloud Foundry의 오픈소스 Cloud Foundry 기술을 기반으로 만든 제품은 다음과 같습니다.\n PCF (Pivotal Cloud Foundry)  Pivotal에서 오픈소스 Cloud Foundry로 만든 상용 PaaS  Bluemix  IBM에서 오픈 소스 Cloud Foundry로 만든 상용 PaaS   CF CLI GO 언어로 개발된 Cloud Foundry의 Command Line Interface입니다. 사용자가 CF CLI를 설치하면 명령 프롬프트를 통해서 CF기반의 PaaS 환경에 자신이 만든 애플리케이션을 배포하거나 서비스 생성하여 바인등하는 등의 다양한 기능을 사용할 수 있습니다.\nCF CLI 설치 방법 CLI Installer를 다운로드하여 간단히 설치할 수 있습니다.\n CF CLI 설치 가이드 페이지 에 접속합니다. Installer 문단에서 컴퓨터 OS 정보에 맞는 파일을 다운로드합니다.\n 다운로드한 파일을 실행하여 CF CLI를 설치합니다. 명령어 프롬프트에 cf를 실행하여 정상 설치 여부를 확인합니다. 다음과 같은 정보가 표시되면 정상적으로 설치된 것을 확인할 수 있습니다.  $ cf cf 버전 6.32.0+0191c33d9.2017-09-26, Cloud Foundry command line tool Usage: cf [global options] command [arguments...] [command options] Before getting started: config login,l target,t help,h logout,lo Application lifecycle: apps,a run-task,rt events push,p logs set-env,se start,st ssh create-app-manifest stop,sp app restart,rs env,e restage,rg scale Services integration: marketplace,m create-user-provided-service,cups services,s update-user-provided-service,uups create-service,cs create-service-key,csk update-service delete-service-key,dsk delete-service,ds service-keys,sk service service-key bind-service,bs bind-route-service,brs unbind-service,us unbind-route-service,urs Route and domain management: routes,r delete-route create-domain domains map-route create-route unmap-route ...  Tip. 컴퓨터가 32bit인지 64bit인지 확인 방법\n1. 명령 프롬프트에서 systeminfo명령어를 입력합니다.\n2. 다음과 같이 시스템 종류 정보가 표시됩니다.\n- x86-based PC -\u0026gt; 32bit\n- x64-based PC -\u0026gt; 64bit\n Getting Started cf help CF CLI 명령어 도움말입니다. CF CLI에서 사용 가능한 모든 명령어에 대한 도움말을 표시합니다.\n사용법 cf help cf help [명령어] 옵션  옵션 없이 사용할 시, 사용 가능한 모든 명령어를 표시합니다. [명령어] 정보를 알고 싶은 특정 명령어  예시 $ cf help cf 버전 6.32.0+0191c33d9.2017-09-26, Cloud Foundry command line tool Usage: cf [global options] command [arguments...] [command options] Before getting started: config login,l target,t help,h logout,lo Application lifecycle: apps,a run-task,rt events push,p logs set-env,se start,st ssh create-app-manifest stop,sp app restart,rs env,e restage,rg scale Services integration: marketplace,m create-user-provided-service,cups services,s update-user-provided-service,uups create-service,cs create-service-key,csk update-service delete-service-key,dsk delete-service,ds service-keys,sk service service-key bind-service,bs bind-route-service,brs unbind-service,us unbind-route-service,urs Route and domain management: routes,r delete-route create-domain domains map-route create-route unmap-route ...$ cf help apps 이름: apps - 대상 영역에 모든 앱 나열 사용법: cf apps 별명: a SEE ALSO: events, logs, map-route, push, restart, scale, start, stop cf api CF CLI의 대상(API End Point)을 설정합니다. 설정된 API Endpoint를 대상으로 이후 모든 작업이 진행됩니다.\n사용법 cf api [URL] [URL] 접속하려는 PaaS의 API End Point\n옵션  --skip-ssl-validation 비보안 Endpoint를 사용합니다. --unset 설정되어 있는 EndPoint를 삭제합니다.  예시 $ cf api http://api.paas.sk.com API 엔드포인트를 http://api.paas.sk.com(으)로 설정 중... Warning: Insecure http API endpoint detected: secure https API endpoints are recommended 확인 api endpoint: http://api.paas.sk.com api version: 2.69.0$ cf api cf api http://api.paas.sk.com --skip-ssl-validation cf login PaaS에 로그인합니다.\n사용법 cf login 옵션  옵션 없이 사용하면 로그인에 필요한 정보를 입력할 수 있는 필드가 생성됩니다. -a [api end point] 접속하려는 PaaS API End point 정보 -u [username] 접속하려는 PaaS 아이디 -p [password] 접속하려는 PaaS 비밀번호 -o [org] 접속하려는 Org 정보 -s [space] 접속하려는 Space 정보 --skip-ssl-validation 비보안 End point 사용하기 위한 옵션  예시 $ cf login API 엔드포인트: http://api.paas.sk.com 경고: 비보안 http API 엔드포인트 발견: 보안 https API 엔드포인트를 사용하는 것이 좋습니다. Email\u0026gt; sya@sk.com Password\u0026gt; 인증 중... 확인 조직 선택(또는 Enter를 눌러 건너뜀): 1. cloudlab 2. dtlab Org\u0026gt; ...$ cf login -a https://api.dev.ghama.io cf logout PaaS를 로그아웃합니다.\n사용법 cf logout 예시 $ cf logout 로그아웃 중... 확인 cf target 현재 접속되어 있는 API End Point , Org, Space 정보 표시 및 Org, Space를 변경합니다.\n사용법 cf target 옵션  옵션 없이 실행하면 현재 접속되어 있는 API EndPoint , Org, Space 정보가 표시됩니다. -o [org] 변경 접속하려는 Org 정보 -s [space] 변경 접속하려는 Space 정보  예시 $ cf target api endpoint: http://api... api version: 2.69.0 user: sya@sk.com org: dtlab space: prod$ cf target -o cloudlab -s dev api endpoint: http://api... api version: 2.69.0 user: sya@sk.com org: cloudlab space: dev Apps cf apps 현재 접속되어 있는 Org와 Space의 모든 Application 목록을 보여줍니다.\n사용법 cf apps 예시 $ cf apps sya@sk.com(으)로 dtlab 조직/prod 영역의 앱 가져오는 중... 확인 이름 요청된 상태 인스턴스 메모리 디스크 URL dtlabs-admin-bff-service started 1/1 256M 256M dtlabs-admin-bff-service.paas.sk.com dtlabs-apigateway-service started 1/1 1G 1G dtlabs-apigateway-service.paas.sk.com cf push PaaS에 애플리케이션을 배포합니다.\n사용법 cf push cf push -f [manifest.yml file path] 옵션  [manifest.yml file path] manifest yml 경로 manifest.yml을 이용하지 않고 옵션을 통해서 애플리케이션을 배포하시려면 옵션의 자세한 내용은 CF Push 가이드를 참고하세요. 옵션을 이용하지 않고 manifest.yml을 통한 배포를 추천합니다. 명령 프롬프트에서 프로젝트 경로로 이동 후, cf push 명령어를 실행하거나,옵션으로 manifest.yml 경로를 지정해서 애플리케이션을 배포합니다. manifest.yml 파일이 개발과 운영환경에서 다른 정보를 가진다면 manifest-dev.yml, manifest-prod.yml 등으로 설정 분리 후, manifest 파일을 명시해서 배포합니다. manifest.yml 설정 정보는 CF 공식가이드 문서의 manifest.yml 가이드 문서를 참고하세요.  예시  옵션 없이 사용하는 경우  $ ls log\tmvnw\tpom.xml\ttarget manifest.yml\tmvnw.cmd\tsrc $ cf push Manifest 파일 /Users/seoyoungahn/git/spring-boot-tutorial/cloud-movie/manifest.yml 사용 sya@sk.com(으)로 dtlab 조직/dev 영역에서 cloud-movie 앱 업데이트 중... 확인 cloud-movie 업로드 중... 업로드 중인 앱 파일 원본 위치: /var/folders/mc/0_v3hb1j2j71t_g53qg8qjg40000gn/T/unzipped-app239365053 443.5K, 105 파일 업로드 Done uploading 확인 sya@sk.com(으)로 dtlab 조직/dev 영역에서 cloud-movie 앱 시작 중... Downloading java_buildpack... Downloaded java_buildpack Creating container Successfully created container Downloading app package... Downloaded app package (16.6M) Staging... -----\u0026gt; Java Buildpack Version: v3.11 | https://github.com/cloudfoundry/java-buildpack.git#eba4df6 -----\u0026gt; Downloading Open Jdk JRE 1.8.0_111 from https://java-buildpack.cloudfoundry.org/openjdk/trusty/x86_64/openjdk-1.8.0_111.tar.gz (10.6s) Expanding Open Jdk JRE to .java-buildpack/open_jdk_jre (1.3s) -----\u0026gt; Downloading Open JDK Like Memory Calculator 2.0.2_RELEASE from https://java-buildpack.cloudfoundry.org/memory-calculator/trusty/x86_64/memory-calculator-2.0.2_RELEASE.tar.gz (0.0s) Memory Settings: -Xmx681574K -XX:MaxMetaspaceSize=104857K -XX:MetaspaceSize=104857K -Xms681574K -Xss349K -----\u0026gt; Downloading Spring Auto Reconfiguration 1.12.0_RELEASE from https://java-buildpack.cloudfoundry.org/auto-reconfiguration/auto-reconfiguration-1.12.0_RELEASE.jar (0.0s) Exit status 0 Staging complete Uploading droplet, build artifacts cache... Uploading droplet... Uploading build artifacts cache... Uploaded build artifacts cache (44.9M) Uploaded droplet (61.8M) Uploading complete Destroying container Successfully destroyed container 0 / 1 인스턴스 실행 중, 1 시작 중 1 / 1 인스턴스 실행 중 앱 시작됨 확인 `CALCULATED_MEMORY=$($PWD/.java-buildpack/open_jdk_jre/bin/java-buildpack-memory-calculator-2.0.2_RELEASE -memorySizes=metaspace:64m..,stack:228k.. -memoryWeights=heap:65,metaspace:10,native:15,stack:10 -memoryInitials=heap:100%,metaspace:100% -stackThreads=300 -totMemory=$MEMORY_LIMIT) \u0026amp;\u0026amp; JAVA_OPTS=\u0026#34;-Djava.io.tmpdir=$TMPDIR-XX:OnOutOfMemoryError=$PWD/.java-buildpack/open_jdk_jre/bin/killjava.sh $CALCULATED_MEMORY\u0026#34; \u0026amp;\u0026amp; SERVER_PORT=$PORT eval exec $PWD/.java-buildpack/open_jdk_jre/bin/java $JAVA_OPTS -cp $PWD/. org.springframework.boot.loader.JarLauncher` 명령을 사용하여 cloud-movie 앱이 시작되었습니다. sya@sk.com(으)로 dtlab 조직/dev 영역에서 cloud-movie 앱의 상태 표시 중... 확인 요청된 상태: started 인스턴스: 1/1 사용법: 1G x 1 인스턴스 URL: cloud-movie-leathern-estrin.paas.sk.com 마지막으로 업로드함: Fri Feb 23 01:02:09 UTC 2018 스택: cflinuxfs2 빌드팩: java_buildpack 상태 이후 CPU 메모리 디스크 세부사항 #0 실행 중 2018-02-23 10:07:53 AM 0.0% 0 / 1G 0 / 1G  옵션 사용 시  $ cf push -f C:/Users/cf-sample-app-nodejs-master/manifest-dev.yml cf delete 애플리케이션을 삭제합니다.\n사용법 cf delete [app name] [app name] 삭제할 애플리케이션명\n옵션  -f 재확인 없이 애플리케이션 삭제합니다. -r 관련된 모든 라우트 정보에 해당하는 애플리케이션 삭제합니다.  예시 $ cf delete cna-sample$ cf delete cna-sample -r cf logs 애플리케이션 로그를 출력합니다.\n사용법 cf logs [app name] [app name] 로그를 출력할 애플리케이션명\n예시 $ cf logs cna-sample Retrieving logs for app cna-sample in org dtlab / space prod as sya@sk.com... cf start 애플리케이션 실행을 시작합니다.\n사용법 cf start [app name] [app name] 시작시킬 애플리케이션명\n예시 $ cf start cna-sample aiting for app to start... 이름: cna-sample 요청된 상태: started 인스턴스: 1/1 사용법: 512M x 1 instances routes: cna-sample.paas.sk.com 마지막으로 업로드함: Mon 12 Feb 18:56:13 KST 2018 스택: cflinuxfs2 빌드팩: java_buildpack start command: CALCULATED_MEMORY=$($PWD/.java-buildpack/open_jdk_jre/bin/java-buildpack-memory-calculator-2.0.2_RELEASE -memorySizes=metaspace:64m..,stack:228k.. -memoryWeights=heap:65,metaspace:10,native:15,stack:10 -memoryInitials=heap:100%,metaspace:100% -stackThreads=300 -totMemory=$MEMORY_LIMIT) \u0026amp;\u0026amp; JAVA_OPTS=\u0026#34;-Djava.io.tmpdir=$TMPDIR-XX:OnOutOfMemoryError=$PWD/.java-buildpack/open_jdk_jre/bin/killjava.sh $CALCULATED_MEMORY\u0026#34; \u0026amp;\u0026amp; SERVER_PORT=$PORT eval exec $PWD/.java-buildpack/open_jdk_jre/bin/java $JAVA_OPTS -cp $PWD/. org.springframework.boot.loader.JarLauncher 상태 이후 CPU 메모리 디스크 세부사항 #0 실행 중 2018-02-22T09:50:13Z 0.0% 318.1M of 512M 143.8M of 1G  cf stop 애플리케이션을 중지시킵니다.\n사용법 cf stop [app name] [app name] 중지할 애플리케이션명\n예시 $ cf stop cna-sample sya@sk.com(으)로 dtlab 조직/dev 영역에서 cna-sample앱 중지 중... 확인 cf restart 애플리케이션을 재실행합니다.\n사용법 cf restart [app name] [app name] 재시작할 애플리케이션명\n예시 $ cf restart cna-sample Restarting app cna-sample in org dtlab / space dev as sya@sk.com... Stopping app... Waiting for app to start... 이름: cna-sample 요청된 상태: started 인스턴스: 1/1 사용법: 512M x 1 instances routes: cna-sample.paas.sk.com 마지막으로 업로드함: Mon 12 Feb 18:56:13 KST 2018 스택: cflinuxfs2 빌드팩: java_buildpack start command: CALCULATED_MEMORY=$($PWD/.java-buildpack/open_jdk_jre/bin/java-buildpack-memory-calculator-2.0.2_RELEASE -memorySizes=metaspace:64m..,stack:228k.. -memoryWeights=heap:65,metaspace:10,native:15,stack:10 -memoryInitials=heap:100%,metaspace:100% -stackThreads=300 -totMemory=$MEMORY_LIMIT) \u0026amp;\u0026amp; JAVA_OPTS=\u0026#34;-Djava.io.tmpdir=$TMPDIR-XX:OnOutOfMemoryError=$PWD/.java-buildpack/open_jdk_jre/bin/killjava.sh $CALCULATED_MEMORY\u0026#34; \u0026amp;\u0026amp; SERVER_PORT=$PORT eval exec $PWD/.java-buildpack/open_jdk_jre/bin/java $JAVA_OPTS -cp $PWD/. org.springframework.boot.loader.JarLauncher 상태 이후 CPU 메모리 디스크 세부사항 #0 실행 중 2018-02-22T09:53:32Z 0.0% 163.1M of 512M 143.8M of 1G  Services cf services 현재 접속되어 있는 Org과 Space에서 사용 가능한 모든 Service의 정보를 표시합니다.\n사용법 cf services 예시 $ cf services sya@sk.com(으)로 dtlab 조직/dev 영역의 서비스를 가져오는 중... 확인 이름 서비스 플랜 바인딩된 앱 마지막 조작 amqp-service RabbitMQ standard create 성공 cloud-movie-apigateway 사용자 제공 cloud-movie-config-server 사용자 제공 cloud-movie-discovery 사용자 제공 cloud-movie-redis Redis shared-vm cf marketplace 마켓플레이스에 등록되어 애플리케이션에서 사용 가능한 서비스의 정보를 보여줍니다.\n사용법 cf marketplace 옵션 -s [service name] [service name]에 해당하는 서비스의 자세한 정책을 보여줍니다.\n예시 $ cf marketplace sya@sk.com(으)로 dtlab 조직/dev 영역에서 서비스를 가져오는 중... 확인 서비스 플랜 설명 App-Autoscaler-beta autoscaler-free-plan (Beta Version) Automatically increase or decrease the number of application instances based on a policy you define. CF-AutoScaler free Automatically increase or decrease the number of application instances based on a policy you define. MariaDB Mysql-Plan1-5con, Mysql-Plan2-100con* A simple mysql implementation Mongo-DB default-plan A simple mongo implementation Mongo-DB-Dev default* A simple MongoDB service broker implementation-Test Object-Storage object-storage-1GB, object-storage-100GB* A simple object-storage implementation RabbitMQ standard RabbitMQ is a robust and scalable high-performance multi-protocol messaging broker. Redis shared-vm Redis service to provide a key-value store Redis-dev Free Plen* Shared Redis server * 해당 서비스 플랜에 연관된 비용이 있습니다. 서비스 인스턴스를 작성하면 이 비용이 발생합니다. 팁: 주어진 서비스의 개별 플랜에 대한 설명을 보려면 \u0026#39;cf marketplace -s SERVICE\u0026#39;를 사용하십시오. $ cf marketplace -s Redis sya@sk.com(으)로 Redis 서비스의 서비스 플랜 정보를 가져오는 중... 확인 서비스 플랜 설명 무료 또는 유료 shared-vm This plan provides a single Redis process on a shared VM, which is suitable for development and testing workloads free cf create-service 서비스를 생성합니다.\n사용법 cf create-service [service name] [service plan] [service instance]  애플리케이션에서 marketplace에 등록되어 있는 서비스를 사용하기 위해 생성합니다. 생성하려는 서비스의 정보는 cf marketplace 명령어를 통해 알 수있습니다. [service name] 생성하려는 service name [service plan] 생성하려는 service plan [service instance] 임의 지정  옵션  -c [parameters as json] 해당 서비스에 전달할 parameter 를 JSON 으로 작성합니다. OS 별로 파라미터 전달하는 방식은 CF 공식문서를 참고하세요.  예시 $ cf create-service Redis shared-vm cna-sample-redis-service sya@sk.com(으)로 dtlab 조직/dev 영역에 서비스 인스턴스 cna-sample-redis-service 작성 중... 확인 cf delete-service 서비스 인스턴스를 삭제합니다.\n사용법 cf delete-service [service instance] [service instance] 삭제할 서비스명. 현재 space에 등록되어 있어야 합니다.\n옵션 -f 재확인 없이 애플리케이션을 삭제합니다.\n예시 $ cf delete-service cna-sample-redis-service 서비스 cna-sample-redis-service을(를) 삭제하시겠습니까?\u0026gt; y sya@sk.com(으)로 dtlab 조직/dev 영역에서 cna-sample-redis-service 서비스 삭제 중... 확인$ cf delete-service cna-sample-redis-service -f sya@sk.com(으)로 dtlab 조직/dev 영역에서 cna-sample-redis-service 서비스 삭제 중... 확인 cf bind-service 애플리케이션에 서비스를 연동합니다.\n사용법 cf bind-service [app name] [service instance]  [app name] 대상 애플리케이션명 [service instance] 연동할 service명 cf services 명령어를 통해 해당 space에서 사용할 수 있는 [service instance]를 확인할 수 있습니다.  옵션 -c [parameters as json] 해당 서비스에 전달할 parameter(JSON으로 작성)\nOS 별로 파라미터 전달하는 방식은 CF 공식홈페이지의 bind service 가이드를 참고하세요.\n예시 $ cf bind-service cna-sample cna-sample-redis-service sya@sk.com(으)로 dtlab 조직/dev 영역의 cna-sample 앱에 cna-sample-redis-service 서비스 바인드 중... 확인 팁: 환경 변수 변경사항을 적용하려면 \u0026#39;cf restage cna-sample\u0026#39;을(를) 사용하십시오. cf unbind-service 애플리케이션에 연동한 서비스 연동을 해제합니다.\n사용법 cf unbind-service [app name] [service instance]  [app name] 대상 애플리케이션명 [service instance] 연동을 해제할 서비스명  예시 $ cf unbind-service cna-sample cna-sample-mysql-service sya@sk.com(으)로 dtlab 조직/dev 영역의 cna-sample-redis-service 서비스에서 cna-sample 앱 바인드 해제 중... 확인"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/discovery-duration-error-in-bluemix/",
        "title": "Eureka Server 매핑 정보 삭제 delay 현상 in Bluemix",
        "tags": ["liberty buildpack", "spring cloud", "spring cloud eureka", "bluemix", "buildpack", "java buildpack", "cloud foundry"],
        "description": "Bluemix에서 eureka server의 종료된 앱 정보가 삭제되는데 delay가 발생하는 현상이 발생했다. 어떤 문제일까 ?",
        "content": " What ? 예전에 bluemix에 배포한 어플리케이션이 eureka와 연계했을 때 생각과 다르게 동작하는 것을 발견했습니다.\n해당 현상은 eureka client 어플리케이션이 종료됐을 때, eureka server에서 해당 어플리케이션 정보가 삭제되는데 생각보다 delay가 생기는 것 입니다. (예상 : 5초 내외, but 수 분이상 dashboard 상에서 조회)\n어디서 꼬인 것일까요 ?\nWhy ?  eureka 설정이 잘못됐을 가능성\neureka server 적용된 설정은 아래와 같습니다.\neureka: instance: instance-id: ${vcap.application.instance_id:${spring.application.name}:${spring.application.instance_id:${server.port}}} hostname: ${vcap.application.uris[0]} prefer-ip-address: false non-secure-port: 80 lease-renewal-interval-in-seconds: 5 lease-expiration-duration-in-seconds: 5 client: region: default fetch-registry: false register-with-eureka: false service-url: defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/ eureka.instance.lease-expiration-duration-in-seconds 을 5초로 세팅했습니다.\neureka server에서 eureka client의 health check를 수행하고, 5초를 넘어서면 eureka server에서 해당 eureka client가 unregist 되는 것으로 이해하고 적용했습니다.\n해당 설정은 의도에 맞게 적용된 것일까요 ?\n 어플리케이션 종료시 이상 증상이 발생해서 eureka server에서 감지하지 못할 가능성   Cloud Foundry 기반의 플랫폼에서 어플리케이션의 종료는 어떻게 이루어질까요 ?\n해당 프로세스에 적합하게 어플리케이션이 종료된 것일까요 ?\nHow ? 1. 테스트 준비  java 어플리케이션 eureka client 어플리케이션 (dtlabs-service-admin) eureka server 어플리케이션 (dtlabs-service-discovery) bluemix buildpack liberty-for-java buildpack java buildpack  2. eureka 설정 체크 eureka server 적용된 설정 eureka: instance: instance-id: ${vcap.application.instance_id:${spring.application.name}:${spring.application.instance_id:${server.port}}} hostname: ${vcap.application.uris[0]} prefer-ip-address: false non-secure-port: 80 lease-renewal-interval-in-seconds: 5 lease-expiration-duration-in-seconds: 5 client: region: default fetch-registry: false register-with-eureka: false service-url: defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/  eureka client 적용된 설정 eureka: instance: instance-id: ${vcap.application.instance_id:${spring.application.name}:${spring.application.instance_id:${server.port}}} hostname: ${vcap.application.uris[0]} prefer-ip-address: false non-secure-port: 80 lease-renewal-interval-in-seconds: 5 client: region: default fetch-registry: true register-with-eureka: true registry-fetch-interval-seconds: 5 service-url: defaultZone: ${vcap.services.dtlabs-service-discovery.credentials.uri}/eureka/ eureka.instance.lease-expiration-duration-in-seconds 설정 코드 내 주석 @Data @ConfigurationProperties(\u0026#34;eureka.instance\u0026#34;) public class EurekaInstanceConfigBean implements CloudEurekaInstanceConfig, EnvironmentAware { /** * Indicates the time in seconds that the eureka server waits since it received the * last heartbeat before it can remove this instance from its view and there by * disallowing traffic to this instance. * * Setting this value too long could mean that the traffic could be routed to the * instance even though the instance is not alive. Setting this value too small could * mean, the instance may be taken out of traffic because of temporary network * glitches.This value to be set to atleast higher than the value specified in * leaseRenewalIntervalInSeconds. */ private int leaseExpirationDurationInSeconds = 90; ... } eureka server / eureka client / eureka.instance.lease-expiration-duration-in-seconds 설정을 다 확인해보니, 무언가 이상한게 보입니다.\neureka.instance.lease-expiration-duration-in-seconds 설정을 eureka server에 세팅하고 eureka client에는 세팅을 하지 않았습니다.\neureka 설정의 개념을 살펴보니,\n eureka.instance: eureka service가 자신이 eureka 서버에 등록될 때 사용하는 설정\neureka.client: 다른 eureka service를 찾으려고 할 때 사용하는 설정\n eureka.instance.lease-expiration-duration-in-seconds 는 eureka client 쪽에 설정을 해줘야 의도대로 동작한다.\neureka client에 설정 적용 후 eureka/apps 확인 하지만 eureka client 설정변경과 관계없이, default 값이 90초로 세팅되어 있는데 eureka server dashboard 에서 어플리케이션 정보가 삭제될 때까지 수 분이 걸리는 것으로 봐서 해당 사유는 아닌 듯합니다.\n3. 어플리케이션 종료 체크 buildpack 별 종료 로그 비교  java buildpack\n2017-10-11T10:50:49.32+0900 [API/1] OUT Updated app with guid 1d6ccdc9-730d-459d-a9fe-c097abee52cb ({\u0026#34;state\u0026#34;=\u0026gt;\u0026#34;STOPPED\u0026#34;}) 2017-10-11T10:50:49.33+0900 [CELL/0] OUT Exit status 0 2017-10-11T10:50:49.33+0900 [APP/0] OUT [CONTAINER] org.apache.coyote.http11.Http11NioProtocol INFO Pausing ProtocolHandler [\u0026#34;http-nio-8080\u0026#34;] 2017-10-11T10:50:49.33+0900 [APP/0] OUT [CONTAINER] org.apache.catalina.core.StandardService INFO Stopping service Catalina 2017-10-11T10:50:49.33+0900 [APP/0] OUT 2017-10-11 10:50:49.332 INFO 8 --- [ Thread-7] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@5071d11a: startup date [Wed Oct 11 10:49:51 KST 2017]; parent: org.springframework.context.annotation.AnnotationConfigApplicationContext@74b09b 2017-10-11T10:50:49.33+0900 [APP/0] OUT 2017-10-11 10:50:49.337 INFO 8 --- [ Thread-7] o.s.c.n.e.s.EurekaServiceRegistry : Unregistering application dtlabs-service-activity with eureka with status DOWN 2017-10-11T10:50:49.33+0900 [APP/0] OUT 2017-10-11 10:50:49.337 WARN 8 --- [ Thread-7] com.netflix.discovery.DiscoveryClient : Saw local status change event StatusChangeEvent [timestamp=1507686649337, current=DOWN, previous=UP] 2017-10-11T10:50:49.33+0900 [APP/0] OUT 2017-10-11 10:50:49.338 INFO 8 --- [ Thread-7] com.netflix.discovery.DiscoveryClient : Shutting down DiscoveryClient ... 2017-10-11T10:50:49.33+0900 [APP/0] OUT 2017-10-11 10:50:49.338 INFO 8 --- [nfoReplicator-0] com.netflix.discovery.DiscoveryClient : DiscoveryClient_DTLABS-SERVICE-ACTIVITY/836a081a-b768-4b66-64cc-90ce75991597: registering service... 2017-10-11T10:50:49.33+0900 [APP/0] OUT 2017-10-11 10:50:49.338 INFO 8 --- [ Thread-7] com.netflix.discovery.DiscoveryClient : Unregistering ... 2017-10-11T10:50:49.46+0900 [APP/0] OUT 2017-10-11 10:50:49.457 INFO 8 --- [nfoReplicator-0] com.netflix.discovery.DiscoveryClient : DiscoveryClient_DTLABS-SERVICE-ACTIVITY/836a081a-b768-4b66-64cc-90ce75991597 - registration status: 204 2017-10-11T10:50:49.49+0900 [APP/0] OUT 2017-10-11 10:50:49.487 INFO 8 --- [ Thread-7] com.netflix.discovery.DiscoveryClient : DiscoveryClient_DTLABS-SERVICE-ACTIVITY/836a081a-b768-4b66-64cc-90ce75991597 - deregister status: 200 2017-10-11T10:50:49.94+0900 [APP/0] OUT 2017-10-11 10:50:49.942 INFO 8 --- [ Thread-7] com.netflix.discovery.DiscoveryClient : Completed shut down of DiscoveryClient 2017-10-11T10:50:49.98+0900 [APP/0] OUT 2017-10-11 10:50:49.981 INFO 8 --- [ Thread-7] o.s.c.support.DefaultLifecycleProcessor : Stopping beans in phase 2147483647 2017-10-11T10:50:49.98+0900 [APP/0] OUT 2017-10-11 10:50:49.982 INFO 8 --- [ Thread-7] o.s.c.support.DefaultLifecycleProcessor : Stopping beans in phase 0 2017-10-11T10:50:50.09+0900 [APP/0] OUT 2017-10-11 10:50:50.094 INFO 8 --- [ost-startStop-2] o.a.c.c.C.[Catalina].[localhost].[/] : Closing Spring root WebApplicationContext 2017-10-11T10:50:50.09+0900 [APP/0] OUT 2017-10-11 10:50:50.096 INFO 8 --- [ Thread-7] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans on shutdown 2017-10-11T10:50:50.10+0900 [APP/0] OUT 2017-10-11 10:50:50.101 INFO 8 --- [ Thread-7] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans 2017-10-11T10:50:50.23+0900 [APP/0] OUT 2017-10-11 10:50:50.238 WARN 8 --- [ Thread-7] c.n.c.sources.URLConfigurationSource : No URLs will be polled as dynamic configuration sources. 2017-10-11T10:50:50.24+0900 [APP/0] OUT 2017-10-11 10:50:50.241 INFO 8 --- [ Thread-7] c.n.c.sources.URLConfigurationSource : To enable URLs as dynamic configuration sources, define System property archaius.configurationSource.additionalUrls or make config.properties available on classpath. 2017-10-11T10:50:50.33+0900 [APP/0] OUT 2017-10-11 10:50:50.333 INFO 8 --- [ Thread-7] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit \u0026#39;default\u0026#39; 2017-10-11T10:50:50.40+0900 [APP/0] OUT [CONTAINER] org.apache.catalina.loader.WebappClassLoaderBase WARNING The web application [ROOT] registered the JDBC driver [org.mariadb.jdbc.Driver] but failed to unregister it when the web application was stopped. To prevent a memory leak, the JDBC Driver has been forcibly unregistered. 2017-10-11T10:50:50.50+0900 [APP/0] OUT [CONTAINER] org.apache.coyote.http11.Http11NioProtocol INFO Stopping ProtocolHandler [\u0026#34;http-nio-8080\u0026#34;] 2017-10-11T10:50:50.56+0900 [APP/0] OUT [CONTAINER] org.apache.coyote.http11.Http11NioProtocol INFO Destroying ProtocolHandler [\u0026#34;http-nio-8080\u0026#34;] 2017-10-11T10:50:50.64+0900 [APP/0] OUT Exit status 143 2017-10-11T10:50:50.65+0900 [CELL/0] OUT Destroying container 2017-10-11T10:50:51.27+0900 [CELL/0] OUT Successfully destroyed container  liberty-for-java buildpack bash 2017-10-11T10:55:52.45+0900 [API/0] OUT Updated app with guid 1d6ccdc9-730d-459d-a9fe-c097abee52cb ({\u0026quot;state\u0026quot;=\u0026gt;\u0026quot;STOPPED\u0026quot;}) 2017-10-11T10:55:52.45+0900 [CELL/0] OUT Exit status 0 2017-10-11T10:56:03.46+0900 [CELL/0] OUT Destroying container 2017-10-11T10:56:04.07+0900 [CELL/0] OUT Successfully destroyed container    buildpack 별 log를 확인해보니, 다른 점을 찾을 수 있습니다.\njava buildpack 은 종료시 DispatcherServlet.destory() -\u0026gt; AbstractApplicationContext.close() 가 호출되는데, liberty-for-java buildpack은 여타 동작없이 어플리케이션을 강제종료 시킨 것 처럼 보인다.\nliberty-for-java buildpack 의 강제종료 사유는 무엇일까요?\nCloud Foundry의 어플리케이션 종료 프로세스 (CF-appl-lifecycle)\n 어플리케이션 종료 요청 발생 Cloud Foundry가 SIGTERM 시그널을 어플리케이션 프로세스로 보냄 어플리케이션 프로세스는 10초 내 종료되어야함 10초 내 종료되지 않으면, Cloud Foundry가 SIGKILL 시그널을 보내 어플리케이션을 종료한다.  10초 내로 SIGTERM 으로 종료되지 않으면 SIGKILL로 어플리케이션을 종료시킨다.\n이것 때문에 강제종료 처리가 되는 것일까요 ?\nSIGTERM 이후 10초 내에 어플리케이션이 종료되지 않은 이유는 무엇일까요 ?\nCloud Foundry의 custom command 가이드 체크 (CF custorm command)\n Cloud Foundry위의 어플리케이션을 사용할 때, Cloud Foundry가 보내는 SIGTERM 시그널을 받기 위해서 어플리케이션 프로세스를 exec prefix를 사용해서 start해야 합니다.  Cloud Foundry의 custom command를 적용하기 위해서는 exec 를 prefix로 적용해줘야한다.\nliberty-for-java buildpack은 exec prefix를 적용해 어플리케이션을 구동하고 있을까요 ?\nbuildpack 별 exec prefix 적용 여부  java buildpack\n https://github.com/cloudfoundry/java-buildpack/blob/master/lib/java_buildpack/container/tomcat.rb#L46 https://github.com/cloudfoundry/java-buildpack/commit/df964cdbb70dabe4cceee80a30f03a4206ac01a1#diff-7fac9479a5b3e17b9b3210c9fa06eca9 2015년 11월에 적용  liberty-for-java buildpack\n https://github.com/cloudfoundry/ibm-websphere-liberty-buildpack/blob/master/lib/liberty_buildpack/container/liberty.rb#L123 https://github.com/cloudfoundry/ibm-websphere-liberty-buildpack/commit/8eb1bd0180366661988362d123db72a9b67246e9 2017년 10월 18일에 적용 (어플리케이션 배포 시점은 2017년 10월 11일)\n   예전에 적용된 빌드팩에 exec prefix가 적용되지 않은 것이 문제라면, 빌드팩을 최신 버전(v.3.1.5)으로 변경해서 재배포를 해보겠습니다.\nliberty-for-java buildpack 최신버전으로 변경해서 재배포 API/1\tUpdated app with guid dde4c846-0e96-45f5-b492-704d7a5b0043 ({\u0026#34;state\u0026#34;=\u0026gt;\u0026#34;STOPPED\u0026#34;})\t2018년 2월 19일 06:46:24.264 오후 APP/0\t.app-management/scripts/start: 1: kill: invalid signal number or name: igterm\t2018년 2월 19일 06:46:24.266 오후 CELL/0\tExit status 0\t2018년 2월 19일 06:46:24.268 오후 CELL/0\tSuccessfully destroyed container\t2018년 2월 19일 06:46:35.999 오후 여전히 의도대로 동작하지 않아서 로그를 확인해보니 kill: invalid signal number or name: igterm 이라는 로그가 찍혀 있습니다.\nsigterm의 오타일까요 설마\u0026hellip; 원인을 잘 모르겠네요.\nGG\nConclusion  eureka.instance.lease-expiration-duration-in-seconds 설정은 eureka client에 세팅을 해줘야 합니다. 이 말은 어플리케이션 마다 eureka server에서 unregist 되는 시간을 달리 해줄 수 있다는 의미입니다. 활용할 방법을 고민해볼 만합니다.\n Cloud Foundry의 custom command 사용시 exec prefix를 달아줘야 합니다. Cloud Foundry 기반의 플랫폼을 구축할 때 or buildpack을 개발해서 제공할 때 유의해야 할 부분입니다.\n  liberty-for-java buildpack 은 2017년 10월 exec prefix가 적용된 buildpack을 release 했으나, 원인 모를 곳에서 막히고 말았습니다.\n제가 잘못한 것 인지, buildpack이 잘못한 것인지 시간이 지나고 확인을 해보겠습니다.\n"
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/how-to-use-cf-binding-service-in-local-env/",
        "title": "로컬에서 Spring Cloud Connector 사용하기",
        "tags": ["spring cloud connector", "cloud foundry", "mariadb", "h2", "paas", "spring cloud"],
        "description": "로컬 개발 환경과 PaaS 환경을 분리해서 개발환경을 구성하는 경우의 불편한점을 개선하기 위한 방법을 찾아보자",
        "content": " What? 로컬 개발 환경에서 Spring Cloud Connector를 사용해서 Application에 binding 된 PaaS 서비스를 사용해 봅시다.\nWhy? 로컬 개발 환경과 PaaS 환경(dev, stg, prod 등) 을 분리하여 개발환경을 구성하는 경우 몇가지 불편한 점이 있습니다.\n그 중 하나가 로컬 개발 환경에서 사용하는 서비스와 PaaS 환경에서 사용하는 서비스가 다른 경우입니다.\n예를 들면,\n 로컬 환경에서 H2 DB 를 쓰다가 MariaDB로 배포하는 경우 쿼리가 다릅니다.\nH2:\nDROP TABLE IF EXISTS users CASCADE; CREATE TABLE IF NOT EXISTS users ( id INTEGER, username VARCHAR(100) NOT NULL, age INTEGER NOT NULL, job VARCHAR(100) NOT NULL ); ALTER TABLE users MODIFY id INTEGER NOT NULL AUTO_INCREMENT; MariaDB:\nDROP TABLE IF EXISTS users CASCADE; CREATE TABLE IF NOT EXISTS users ( id INTEGER, username VARCHAR(100) NOT NULL, age INTEGER NOT NULL, job VARCHAR(100) NOT NULL ); ALTER TABLE users MODIFY id INTEGER PRIMARY KEY NOT NULL AUTO_INCREMENT; 다른 점은 \u0026hellip; ? MariaDB는 PRIMARY KEY가 빠지면 에러입니다.\n local 환경에서 초기 데이터를 만들기 번거롭습니다.\n테스트 데이터를 매번 생성 or 초기화 하는 절차가 추가됩니다.\n 어플리케이션 수정 후 PaaS 환경에서 테스트를 할 때마다 매번 배포를 합니다. -\u0026gt; CI/CD 배포 pipeline이 없다면 번거로운 절차입니다.\n 로컬에서는 잘 되는게 PaaS에서는 안됩니다. -\u0026gt; 서비스의 버전이 다른 경우,,,, 로컬에서 아무리 잘 동작해도 서비스하는 환경에서 안되면 말짱 꽝입니다. 버전 맞추는 것도 번거롭습니다.\n  How? Spring Cloud Connector 와 STS/Eclipse 의 Run Configuration(환경변수 주입) 을 활용합시다.\n개발 환경  STS(Spring Tool Suite) Spring Boot 1.5.9 Spring Cloud Connector Spring Cloud Connector Docs Link Cloud Foundry 기반의 OpenPaaS  MariaDB 인스턴스   미리 준비한 것  MariaDB 인스턴스 생성 : js-test-MariaDB 어플리케이션 : js-local-paas-service-conn\n dependency 추가 - Spring Cloud Connector    datasource 설정 Bean 생성  @Configuration @Profile({\u0026#34;dev\u0026#34;}) public class CloudConfiguration extends AbstractCloudConfig { @Value(\u0026#34;${services.datasource.name}\u0026#34;) private String datasourceName; @Value(\u0026#34;${services.datasource.initial-size}\u0026#34;) private int minPoolSize; @Value(\u0026#34;${services.datasource.maximum-pool-size}\u0026#34;) private int maxPoolSize; @Value(\u0026#34;${services.datasource.max-wait-time}\u0026#34;) private int maxWaitTime; /** * configure datasource. * @return dataSource object */ @Bean public DataSource dataSource() { PoolConfig poolConfig = new PoolConfig(minPoolSize, maxPoolSize, maxWaitTime); DataSourceConfig dbConfig = new DataSourceConfig(poolConfig, null); return connectionFactory().dataSource(datasourceName, dbConfig); } }  application-dev.yml - datasource 설정 yaml services: datasource: initial-size: 1 maximum-pool-size: 100 max-wait-time: 3000 name: js-test-mariadb initialize: false  - js-local-paas-service-conn 을 PaaS에 배포한 후 js-test-MariaDB 와 binding 합니다. - 로컬 환경과 PaaS 환경의 데이터가 다른 것을 확인합니다. - 준비 끝! ### 상세 적용 방법 #### MariaDB ssh 연동 PaaS 환경의 서비스 인스턴스는 로컬에서 바로 연동하지 못하고, ssh로 연동합니다. cf ssh -N -L 63306:172.132.14.32:3306 js-local-paas-service-conn #### STS - Run Configuration - Spring Boot - Profiles 설정 어플리케이션의 수행 profile을 dev로 설정합니다. #### STS - Run Configuration - Environment - Environment variables 설정 어플리케이션의 PaaS 환경 변수(VCAP_APPLICATION, VCAP_SERVICES)를 설정합니다. - VCAP_APPLCATION : {} 로 세팅합니다. - VCAP_SERVICES : cf env {어플리케이션명} 으로 조회된 value를 엔터키 없이 복사해서 넣습니다. \u0026gt; cf env로 조회된 mariaDB credential 중 hostname과 port 정보를 ssh 연동한 정보로 수정이 필요합니다. \u0026gt; \u0026gt; ex. \u0026ldquo;hostname\u0026rdquo;: \u0026ldquo;172.132.14.32\u0026rdquo; =\u0026gt; \u0026ldquo;hostname\u0026rdquo;: \u0026ldquo;127.0.0.1\u0026rdquo; #### 로컬에서 PaaS 데이터 확인 성공! ## Conclusion Spring Cloud Connector와 STS/Eclipse의 Run Configuration(환경변수 주입)을 사용해 로컬 개발 환경에서 PaaS의 binding 된 서비스를 사용할 수 있습니다.   "
    },
    {
        "uri": "http://tech.cloudz-labs.io/authors/1000jaeh/",
        "title": "1000jaeh",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/12-factors/",
        "title": "12 factors",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/authors/",
        "title": "Authors",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/categories/",
        "title": "Categories",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/db-%EC%97%B0%EB%8F%99/",
        "title": "DB 연동",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/",
        "title": "DIGITAL LABS",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/ms-%EB%B6%84%EB%A6%AC-%EC%9B%8C%ED%81%AC%EC%83%B5/",
        "title": "MS 분리 워크샵",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/posts/",
        "title": "Posts",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/",
        "title": "Tags",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/architecture/",
        "title": "architecture",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/autoscale/",
        "title": "autoscale",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/authors/bckim0620/",
        "title": "bckim0620",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/authors/blingeeeee/",
        "title": "blingeeeee",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/bluemix/",
        "title": "bluemix",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/bounded-context/",
        "title": "bounded context",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/buildpack/",
        "title": "buildpack",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/caas/",
        "title": "caas",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/cf/",
        "title": "cf",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/cf-cli/",
        "title": "cf cli",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/citizen-developer/",
        "title": "citizen developer",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/cloud/",
        "title": "cloud",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/cloud-application/",
        "title": "cloud application",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/cloud-founcry/",
        "title": "cloud founcry",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/cloud-foundry/",
        "title": "cloud foundry",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/cloud-native-application/",
        "title": "cloud native application",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/cluster/",
        "title": "cluster",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/cncf/",
        "title": "cncf",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/configmap/",
        "title": "configMap",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/container/",
        "title": "container",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/container-orchestration/",
        "title": "container Orchestration",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/container-ochestration/",
        "title": "container ochestration",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/container-orchestration/",
        "title": "container orchestration",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/deploy/",
        "title": "deploy",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/devops/",
        "title": "devops",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/digital-transformation/",
        "title": "digital transformation",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/dind/",
        "title": "dind",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/docker/",
        "title": "docker",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/docker-daemon/",
        "title": "docker daemon",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/docker-in-docker/",
        "title": "docker in docker",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/docker-network/",
        "title": "docker network",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/docker-object/",
        "title": "docker object",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/docker-registry/",
        "title": "docker registry",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/dockerfile/",
        "title": "dockerfile",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/domain-driven-design/",
        "title": "domain driven design",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/elasticsearch/",
        "title": "elasticsearch",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/envoy/",
        "title": "envoy",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/event-driven/",
        "title": "event driven",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/event-store/",
        "title": "event store",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/fluent/",
        "title": "fluent",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/fluentbit/",
        "title": "fluentbit",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/fluentd/",
        "title": "fluentd",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/github-pages/",
        "title": "github pages",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/go/",
        "title": "go",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/grafana/",
        "title": "grafana",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/h2/",
        "title": "h2",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/helm/",
        "title": "helm",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/horizontal-pod-autoscaler/",
        "title": "horizontal pod autoscaler",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/hpa/",
        "title": "hpa",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/hugo/",
        "title": "hugo",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/authors/hunkee1017/",
        "title": "hunkee1017",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/hybrid-application-platform/",
        "title": "hybrid application platform",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/image/",
        "title": "image",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/ingress/",
        "title": "ingress",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/istio/",
        "title": "istio",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/java-buildpack/",
        "title": "java buildpack",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/authors/jihyun/",
        "title": "jihyun",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/authors/jisangyun/",
        "title": "jisangyun",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/authors/junepark/",
        "title": "junepark",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/k8s/",
        "title": "k8s",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/kafka/",
        "title": "kafka",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/kibana/",
        "title": "kibana",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/kubernetes/",
        "title": "kubernetes",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/layer/",
        "title": "layer",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/liberty-buildpack/",
        "title": "liberty buildpack",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/linkerd/",
        "title": "linkerd",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/logging/",
        "title": "logging",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/mariadb/",
        "title": "mariaDB",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/masa/",
        "title": "masa",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/message-queue/",
        "title": "message queue",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/microservice-architecture/",
        "title": "microService architecture",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/microservice/",
        "title": "microservice",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/microservices/",
        "title": "microservices",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/microservices.io/",
        "title": "microservices.io",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/monitoring/",
        "title": "monitoring",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/msa/",
        "title": "msa",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/nginx/",
        "title": "nginx",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/overlay/",
        "title": "overlay",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/paas/",
        "title": "paaS",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/paas/",
        "title": "paas",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/categories/posts/",
        "title": "posts",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/prometheus/",
        "title": "prometheus",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/rabbitmq/",
        "title": "rabbitmq",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/reative/",
        "title": "reative",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/redis/",
        "title": "redis",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/rolling-update/",
        "title": "rolling update",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/runtime-complexity/",
        "title": "runtime complexity",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/scale-out/",
        "title": "scale out",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/secret/",
        "title": "secret",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/serverless/",
        "title": "serverless",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/service/",
        "title": "service",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/service-discovery/",
        "title": "service discovery",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/service-mesh/",
        "title": "service mesh",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/spinnaker/",
        "title": "spinnaker",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/spring-boot/",
        "title": "spring boot",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/spring-cloud/",
        "title": "spring cloud",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/spring-cloud-connector/",
        "title": "spring cloud connector",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/spring-cloud-eureka/",
        "title": "spring cloud eureka",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/spring-cloud-netflix/",
        "title": "spring cloud netflix",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/spring-cloud-stream/",
        "title": "spring cloud stream",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/srping-cloud/",
        "title": "srping cloud",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/static-generator/",
        "title": "static generator",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/swagger/",
        "title": "swagger",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/swarm/",
        "title": "swarm",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/authors/sya/",
        "title": "sya",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/ubiquitous-language/",
        "title": "ubiquitous language",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/user-defined-network/",
        "title": "user defined network",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/virtualization/",
        "title": "virtualization",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/volume/",
        "title": "volume",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/authors/yunsangjun/",
        "title": "yunsangjun",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/zuul/",
        "title": "zuul",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/%EB%A1%9C%EA%B9%85/",
        "title": "로깅",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/%EB%AA%A8%EB%8B%88%ED%84%B0%EB%A7%81/",
        "title": "모니터링",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/%EB%B9%84%EB%8F%99%EA%B8%B0-%ED%86%B5%EC%8B%A0/",
        "title": "비동기 통신",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/%EC%84%9C%EB%B9%84%EC%8A%A4-%EB%A9%94%EC%8B%9C/",
        "title": "서비스 메시",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "http://tech.cloudz-labs.io/tags/%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%97%B0%EB%8F%99/",
        "title": "서비스 연동",
        "tags": [],
        "description": "",
        "content": ""
    }]
